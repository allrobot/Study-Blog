I"(4<h2 id="分类">分类</h2>

<p>先从二分类开始，假设$y\epsilon${0,1}，0和1分别输出no、yes：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/7.png" alt="" /></p>

<p>阈值可以设为0.5，当激活函数的加权输入$\ge$0.5则输出1，小于0.5则输出0</p>

<blockquote>
  <p>这种叫<strong>Logistic回归（逻辑回归）</strong>。</p>
</blockquote>

<p>但增加额外样本后，拟合情况就不那么好了，如图所示：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/8.png" alt="" /></p>

<p>所以不推荐线性回归模型用于分类问题。</p>

<h3 id="sigmoid函数">sigmoid函数</h3>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta^{T}x)
\end{equation*}\]

<p>其中激活函数</p>

\[\begin{equation*}
g(z)=\frac{1}{1+e^{-z}}
\end{equation*}\]

<blockquote>
  <p>Sigmoid和Logistic函数是同义词</p>
</blockquote>

<p>激活函数的z为$\theta^{T}x$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/9.png" alt="" /></p>

<p>z横坐标，$g(z)$纵坐标，左端趋于0，右端趋于1。</p>

<p>假设一个患者的肿瘤，模型输出肿瘤大小x对于患者是有害还是无害的：</p>

\[\begin{equation*}
x=\begin{bmatrix}
x_{0}  \\
x_{1}
\end{bmatrix}
=\begin{bmatrix}
1  \\
\text{肿瘤面积}  \\
\end{bmatrix}
h_{\theta}(x)=0.7
\end{equation*}\]

<p>说明患者的肿瘤输出y=1的概率为0.7，写成数学表达式：$P(y=1|x;\theta)$，$\theta$为概率。
y=1的表达式：$P(y=1|x;\theta)=1-P(y=1|x;\theta)$</p>

<h2 id="决策边界">决策边界</h2>

<p>决策边界(decision boundary)，下图中紫线是决策边界:</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/10.png" alt="" /></p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2})  \\
\end{equation*}\]

<p>输出</p>

\[\begin{equation*}
  \theta^{T}x=-3+x_{1}+x_{2}\ge 0  \\
\end{equation*}\]

<p>所以</p>

\[\begin{equation*}
  x_{1}+x_{2}\ge 3  \\
  x_{1}+x_{2}&lt; 3  \\

\end{equation*}\]

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/11.png" alt="" /></p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4})  \\
\end{equation*}\]

<p>$\theta$初始化为{-1，0，0，1，1}：</p>

\[\begin{equation*}
  \theta=
  \begin{bmatrix}
  -1  \\ 0  \\ 0  \\ 1  \\ 1  \\
  \end{bmatrix}
\end{equation*}\]

<p>如果y=1</p>

\[\begin{equation*}
  -1+x_{1}^{2}+x_{2}^{2}\ge 0  \\
\end{equation*}\]

<p>那么</p>

\[\begin{equation*}
  x_{1}^{2}+x_{2}^{2}\ge 1
\end{equation*}\]

<p>决策边界是取决于参数的，并非通过训练训练集得出的结果，参数不同会得到不同的决策边界。</p>

<p>复杂点的例子：</p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{1}^{2}x_{2}+\theta_{4}x_{1}^{2}x_{2}^{2}+\theta_{4}x_{1}^{3}x_{2}+\ldots)  \\

\end{equation*}\]

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/12.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/13.png" alt="" /></p>

<h2 id="损失函数">损失函数</h2>

<p>给出训练集$(x^{(i)},y^{(i)})$，i维特征向量矩阵，以及激活函数：</p>

<p>$(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\ldots,(x^{(n)},y^{(n)})$</p>

\[\begin{equation*}
  x^{(i)}=
  \begin{bmatrix}
    x_{0}^{(i)}  \\
    x_{1}^{(i)}  \\
    \ldots     \\
    x_{n}^{(i)}  
  \end{bmatrix}
  x_{0}=1,\ 0\ &lt;\ y\ &lt;\ 1  \\
  h_{\theta}(x)=\frac{1}{1+e^{-z}}
\end{equation*}\]

<p>接下来$\theta$目标值如何优化？</p>

<p>原来的线性回归模型的损失函数，把$\frac{1}{2}$移至右边函数里面：</p>

\[\begin{equation*}
  J(\theta_{0},\theta_{1})=\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2  \\
  
  Cost(h_{\theta}(x^{(i)}),y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)}),y^{(i)})^2
\end{equation*}\]

<p>这种损失函数在线性回归模型比较能良好工作，线性回归的非凹函数和凹函数，通常是希望凹函数能很好收敛全局最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/14.png" alt="" /></p>

<p>代价函数可能实际是非凹函数图的，迭代有时经常收敛到局部最优解，为了达到理想中的梯度下降，需要推导新的梯度下降法来收敛全局最小值的。</p>

\[\begin{equation*}
  Cost(h_{\theta}(x),y)=
  \begin{cases}-log(h_{\theta}(x)) &amp; \text{if}\ y=1  \\
  -log(1-h_{\theta}(x)) &amp; \text{if}\ y=0  
  \end{cases}
\end{equation*}\]

<p>当y=1时，纵坐标为$J(\theta)$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/15.png" alt="" /></p>

<p>y值无限趋于1，当$y=1,h_{\theta}(x)=1$时，损失函数输出为0，如果$h_{\theta}(x)$趋于0，那么损失趋于无穷。</p>

<p>如果$h_{\theta}=0$，$P(y=1|x;\theta)=0$，y=1的概率趋于0。假如y=1的话，那么损失函数的值就很大了，用很大的损失来训练这个学习算法，使之下次确保输出1的概率趋于1.</p>

<p>当y=0时，原理是一样的：</p>

<p>$h_{\theta}=1$，$P(y=0|x;\theta)=0$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/16.png" alt="" /></p>

<h2 id="简化损失函数和梯度下降法">简化损失函数和梯度下降法</h2>

\[\begin{equation*}
  J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}Cost(h_{\theta}(x^{(i)},y^{(i)})  \\
  Cost(h_{\theta}(x),y)=
  \begin{cases}-log(h_{\theta}(x)) &amp; if\ y=1  \\
  -log(1-h_{\theta}(x)) &amp; if\ y=0  
  \end{cases}
\end{equation*}\]

<p>为了避免梯度下降法按两种情况写，把两个式子合并:</p>

\[\begin{equation*}
  Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))  \\
\end{equation*}\]

<p>逻辑回归的损失函数，可以这么写的：</p>

\[\begin{equation*}
  
  \begin{split}
  J(\theta)
  &amp; =\frac{1}{m}Cost(h_{\theta}(x^{(i)},y^{(i)})  \\
  &amp; =-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]  \\
  \end{split}
\end{equation*}\]

<p>公式是由统计学的极大似然法决定的，统计学为不同模型快速找参数，给出此方法(原理和证明略)，它的优点是凸函数。</p>

\[\begin{equation*} min_{\theta}J(\theta):  \\
\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta)
\end{equation*}\]

<p>在此式中，$\frac{\partial}{\partial\theta_{j}}J(\theta)$多元微分推导成：</p>

\[\begin{equation*} \frac{\partial}{\partial\theta_{j}}J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\end{equation*}\]

<p>所以导数项：</p>

\[\begin{equation*} \theta_{j}:=\theta_{j}-\alpha \displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\end{equation*}\]

<p>有若干$\theta$参数，更新参数值需要用到这个公式。</p>

<p>线性回归和逻辑回归有什么区别？它们的梯度下降法不是同一个，拟合数据也比线性回归更佳，目前已广泛应用于机器学习领域中。</p>

<h2 id="高级优化算法">高级优化算法</h2>

<p>优化算法，给一个$\theta$，需要代码去计算损失函数和偏导数：</p>

<ul>
  <li>$J(\theta)$</li>
  <li>$\frac{\partial}{\partial\theta_{j}}J(\theta) \qquad (for j=0,1,\ldots,n)</li>
</ul>

<p>然后利用梯度下降法去优化目标值：</p>

\[\begin{equation*} min_{\theta}J(\theta):  \\
\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta)
\end{equation*}\]

<p>除了常规优化算法，还有一些高级优化算法：</p>

<ul>
  <li>批次梯度下降法</li>
  <li>共轭梯度法</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<p>已知的梯度下降法就不介绍了，简单介绍其它三种优化算法的优点：
优点：
	- 不需要手动选择学习率
	- 比常规梯度下降法快多了</p>

<p>缺点：
	- 非常复杂</p>

<p>这些算法给出计算导数项和损失函数的方法，它们有线性搜索算法(智能内循环clever inner-loop)，每次迭代自动尝试不同学习率，所以收敛速度远远比常规梯度下降法快多了。</p>

<p>细节待补充…可能要学几周时间</p>

<h2 id="多分类一对多">多分类：一对多</h2>

<p>二分类  VS  多分类：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/17.png" alt="" /></p>

<p>三种符号来代表三个不同类别的样本，逻辑回归模型能解决二类别问题，将训练集分为正类和负类，这种思想可以用于多分类：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/18.png" alt="" /></p>

<p>把三角形划分正类，其它作为圆形划分到负类，那么就可以训练一个标准的逻辑回归分类器，类推红叉号、蓝正方形可以这么划分。</p>

<p>拟合出3个分类器，根据$h_{\theta}^{(i)}=P(y=i|x;\theta)=0\qquad(i=1,2,3)$来估计出x的概率，$\displaystyle\mathop{max}\limits_{i}h_{\theta_{\theta}^{(i)}(x)}$哪个概率大,分别对应各自的分类。</p>

<p>每个分类器针对一种情况进行训练</p>

<h2 id="过度拟合的问题">过度拟合的问题</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/19.png" alt="" /></p>

<p>第一个图，函数模型的直线不能很好拟合训练集，房价上升处是比较平缓、曲线的，这叫<strong>欠拟合</strong>，这算法具有高偏差特性，因为它和训练集理想的情况有很大的偏差；</p>

<p>第二个图，加入二项式，它能很好拟合训练集；</p>

<p>第三个图，四项式模拟的曲线上下波动，看似很好的拟合训练集(正好通过每一个训练样本），实际并不是预测房价的好模型，这种称为过度拟合，算法具有高方差。</p>

<p>高阶多项式虽然能拟合训练集，但它面临庞大的变量问题，导致训练进程缓慢，再千方百计拟合训练集，只要不能泛化到新的样本就是白搭。</p>

<blockquote>
  <p>泛化：模型应用到新样本的能力</p>
</blockquote>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/20.png" alt="" /></p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/21.png" alt="" /></p>

<p>解决方案：
	1. 减少特征变量$\theta$
		- 手动保留特征变量
		- 模型选择算法
	2. 正则化
		- 保留所有特征，减小参数的量级或参数$\theta_{j}$的大小
		- 训练更多的训练样本，它能很好工作</p>

<h2 id="损失函数正则化">损失函数正则化</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/22.png" alt="" /></p>

<p>高阶多项式，由于过拟合现象导致预测效果不太理想，为了解决应添加正则化参数，$\lambda$参数设为1000，因为数值较大，$\theta_{3}$和$\theta_{4}$的<strong>数值尽可能小</strong>。</p>

\[\begin{equation*} 

\mathop{min}\limits{\theta}\frac{1}{2m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+1000\theta_{3}^3 + 1000\theta_{4}^2

\end{equation*}\]

<p>$\theta_{3}\ \approx\ 0\qquad\theta_{3}\ \approx\ 0$，这样得到更好的模型，增大了两个参数带来的效果。在实际测试中，结果表明<strong>参数越接近0</strong>，得到的函数越<strong>平滑、简单</strong>。</p>

<p>假设预测房价的模型的特征为$x_{1},x_{2},\ldots,x_{100}$，参数为$\theta_{0},\theta_{1},\theta_{2},\ldots,\theta_{100}$</p>

<h3 id="正则化公式">正则化公式</h3>

\[\begin{equation*} 

J(\theta)=\frac{1}{2m}[\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+\lambda\sum\limits_{i=1}^m\theta_{j}^2]

\end{equation*}\]

<p>如果$\lambda$值取得太大，会造成惩罚值变大，导致迭代后的参数几乎接近0，最后结果为欠拟合。</p>

<h2 id="正则化的线性回归">正则化的线性回归</h2>

<p>线性回归的梯度下降法：</p>

\[\begin{equation*} 

\theta_{j}:=\theta_{j}-\alpha\qquad\frac{1}{m}\sum\limits_{i=1}^m \mathop{(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\limits{j=0,1,2,3,\ldots,n)

\end{equation*}\]

<p>常规的梯度下降法是训练所有训练样本后，加起来再除以m进行迭代$\theta$参数。正则化的线性回归，它不迭代$\theta_{0}$参数，因为正则化是计算$j=1,2,3,\ldots,n$惩罚值的，要分开对待两种参数的惩罚值。</p>

\[\begin{equation*} 

\theta_{j}:=\theta_{j}-\alpha\qquad[\frac{1}{m}\sum\limits_{i=1}^m \mathop{(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\limits{j=0,1,2,3,\ldots,n)+frac{\lambda}{m}\theta_{j}]

\end{equation*}\]

<p>上面公式方括号包含的是$\frac{\partial}{\partial \theta_{j}}J(\theta)$的偏导数，证明略，最终得到：</p>

\[\begin{equation*} 

\theta_{j}:=\theta_{j}(1-\alpha\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m \mathop{(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}}\limits{j=0,1,2,3,\ldots,n)

\end{equation*}\]

:ET