<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/Study-Blog/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/Study-Blog/" rel="alternate" type="text/html" hreflang="zh-CN" /><updated>2022-02-23T08:38:40+00:00</updated><id>http://localhost:4000/Study-Blog/feed.xml</id><title type="html">allrobot的云笔记博客</title><subtitle>没啥好说的，本站是云笔记的博客，从Github拉取Blog
</subtitle><author><name>李广琛</name><email>allrobots@163.com</email></author><entry><title type="html">吴恩达机器学习笔记 Day 6</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/19/Andrew-Ng-DL-5.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 6" /><published>2022-02-19T00:00:00+00:00</published><updated>2022-02-19T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/19/Andrew-Ng-DL-5</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/19/Andrew-Ng-DL-5.html"><![CDATA[]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">吴恩达机器学习笔记 Day 5</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/17/Andrew-Ng-DL-5.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 5" /><published>2022-02-17T00:00:00+00:00</published><updated>2022-02-17T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/17/Andrew-Ng-DL-5</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/17/Andrew-Ng-DL-5.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ 
$\displaystyle\underbrace{a_i}_{\text{i从1到n}}$

$\displaystyle\mathop{a_i}\limits_{i\text{从1到}n}$
-->
<h2 id="评估模型">评估模型</h2>

<h3 id="验证模型性能">验证模型性能</h3>

<p>训练集分为三部分：</p>
<ul>
  <li>60%训练集traning set</li>
  <li>20%交叉验证集cross validation set</li>
  <li>20%测试集test set</li>
</ul>

<p>60%训练集训练出来的$\theta$参数，最小化$J(\theta)$，然后计算验证集的测试误差$J(\theta)$(线性回归、逻辑回归的$J(\theta)$定义是不同的)。</p>

<h3 id="选择多项式模型">选择多项式模型</h3>

<ol>
  <li>$h_{\theta}(x)=\theta_{0}+\theta_{1}x$<br /></li>
  <li>$h_{\theta}(x)=\theta_{0}+\theta_{1}x+\theta_{2}x^2$<br /></li>
  <li>$h_{\theta}(x)=\theta_{0}+\theta_{1}x+\ldots+\theta_{3}x^3$<br />
.<br />
.<br />
.<br /></li>
  <li>$h_{\theta}(x)=\theta_{0}+\theta_{1}x+\ldots+\theta_{10}x^{10}$</li>
</ol>

<p>各个不同多项式模型通过训练集训练的$\theta$参数，然后用验证集验出误差，哪个模型的验证集误差较小就选哪个。</p>

<h3 id="判断模型的拟合">判断模型的拟合</h3>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/1.png" alt="" /></p>

<p>偏差（欠拟合）：</p>
<ul>
  <li>$J_{训练}(\theta)$值很高，左边的bias</li>
  <li>$J_{交叉验证}(\theta)\approx J_{训练}(\theta)$</li>
</ul>

<p>方差（过拟合）：</p>
<ul>
  <li>$J_{训练}(\theta)$值较低，右边的variance</li>
  <li>$J_{交叉验证}(\theta)» J_{训练}(\theta)$</li>
</ul>

<blockquote>
  <p>&gt;&gt;数学符号的意思是远远大于此值</p>
</blockquote>

<h3 id="正则化和偏差方差">正则化和偏差/方差</h3>

<p>模型：
$$
\begin{equation*}</p>

<p>h_{\theta}(x)=\theta_{0}+\theta_{1}x+\theta_{2}x^2+\theta_{3}x^3+\theta_{4}x^4  \</p>

<p>J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum\limits_{j=1}^{m}\theta_{j}^2</p>

<p>\end{equation*}
$$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/2.png" alt="" /></p>

<p>$\lambda$值过大，导致迭代后的大部分$\theta$参数$\approx$0，最后模型输出$\approx$0，使之欠拟合；</p>

<p>$\lambda$值过小，导致过拟合现象；</p>

<p>解决方案：</p>
<h4 id="自动选择">自动选择</h4>

<ol>
  <li>$\lambda=0$</li>
  <li>$\lambda=0.01$</li>
  <li>$\lambda=0.02$</li>
  <li>$\lambda=0.04$</li>
  <li>$\lambda=0.08$
.<br />
.<br />
.<br /></li>
  <li>$\lambda=10$</li>
</ol>

<p>用训练集批次$\mathop{min}limits_{\theta}J(\theta)$，然后通过验证集批次计算各自的$J_{cv}(\theta^(i))$平均的误差平方和，接着选择误差值最小的$\lambda$值，做完后观察该模型在测试集的泛化能力表现</p>

<h4 id="手动选择">手动选择</h4>

<p>偏差/方差作为正则化参数$\lambda$，自己手动调整$\lambda$值(0.01~1000)，观察训练误差和验证误差，然后从验证集的曲线低洼处选择最合适$\lambda$，最后查验模型的泛化能力。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/3.png" alt="" /></p>

<h3 id="学习曲线">学习曲线</h3>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/4.png" alt="" /></p>

<p>随着m个样本增加，训练误差随m增大二增大，验证误差经较少的样本m训练后，误差项较大，随m增大而减少，直至贴近训练误差变平。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/6.png" alt="" /></p>

<p>因为模型是直线，不能很好拟合数据，m增加时总误差项增大，到了最后变平。所以样本增加的再多，训练误差不会有什么变化，如左边的紫线和蓝线随着m增大，结果没什么太大变化。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/6.png" alt="" /></p>

<p>总误差项随着样本m增加而增加，但和验证误差的曲线间隔较大，说明方差较高，可以通过增加样本或正则化来解决。</p>

<h3 id="评价学习算法">评价学习算法</h3>

<p>调试一个学习算法:
假如已实现正则线性回归模型来预测房价。然而，用一组新的样本来测试效果，发现它在预测中出现了难以接受的巨大错误。接下来要做的判断：
（先画学习曲线）</p>
<ul>
  <li>获取更多训练样本              解决高方差问题</li>
  <li>减少特征数量                  解决高方差问题</li>
  <li>添加更多特征                  解决高偏差问题</li>
  <li>减少$\lambda$值               解决高偏差问题</li>
  <li>增加$\lambda$值               解决高方差问题</li>
</ul>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-17/7.png" alt="" /></p>

<p>神经网络规模较小、参数少、容易欠拟合，计算量成本较低</p>

<p>神经网络规模较大、参数多、容易过拟合，计算量成本较高</p>

<p>默认一层隐藏层神经网络，添加隐藏层需测试验证集的误差，关于过拟合用正则化解决。</p>

<h2 id="快速开发机器学习">快速开发机器学习</h2>

<p>开局先用简单粗暴的算法训练参数，绘制学习曲线，观察图进行误差分析，根据数据评价指标了解哪些算法是需要或不需要的，以及训练集、参数都有哪些问题等等，从而决定之后的优化方法</p>

<h3 id="误差度量评估算法">误差度量评估算法</h3>

<p>癌症分类示例：
训练逻辑回归模型$h_{\theta}(x)$(癌症y=1，否则y=0)，当发现模型在测试集中有1%的错误。</p>

<p>测试集中有0.5%人有癌症，这错误率就不那么好了</p>

<p>假如模型的错误率降低到0.8，甚至0.5%，模型的质量是否提升？</p>

<p>需要一个标准来判定：</p>

<table>
	<tr style="border: none;">
		<th style="border: none;"></th>
		<th style="border: none;"></th>
		<th style="border: none;">实际</th>
		<th style="border: none;">分组</th>
	</tr>
	<tr style="border: none;">
		<th style="border: none;"></th>
		<th style="border: none;"></th>
		<th style="border: none;">1</th>
		<th style="border: none;">0</th>
	</tr>
	<tr>
		<th style="border: none;">预测</th>
		<th style="border: none;">1</th>
		<th>真阳性</th>
		<th>假阳性</th>
	</tr>
	<tr>
		<th style="border: none;">分类</th>
		<th style="border: none;">0</th>
		<th>假阴性</th>
		<th>真阴性</th>
	</tr>
</table>

<p>精度：
所有病人中预测y=1的，有多少人换癌症？</p>

<p>查准率：在所有预测患有癌症的患者中有多大比率的病人是真正拥有癌症的</p>

\[\begin{equation*} 

查准率=\frac{所有预测正确有阳性的人(真阳性)}{所有预测有阳性的人}=\frac{真阳性}{真阳性+假阳性}

\end{equation*}\]

<p>召回率：在所有患者中真实有癌症的人有多大比率</p>

\[\begin{equation*} 

召回率=\frac{真阳性}{所有患有癌症的人}=\frac{真阳性}{真阳性+假阴性}

\end{equation*}\]

<p>好的模型，查准率和召回率很高</p>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[评估模型 验证模型性能 训练集分为三部分： 60%训练集traning set 20%交叉验证集cross validation set 20%测试集test set 60%训练集训练出来的$\theta$参数，最小化$J(\theta)$，然后计算验证集的测试误差$J(\theta)$(线性回归、逻辑回归的$J(\theta)$定义是不同的)。 选择多项式模型 $h_{\theta}(x)=\theta_{0}+\theta_{1}x$ $h_{\theta}(x)=\theta_{0}+\theta_{1}x+\theta_{2}x^2$ $h_{\theta}(x)=\theta_{0}+\theta_{1}x+\ldots+\theta_{3}x^3$ . . . $h_{\theta}(x)=\theta_{0}+\theta_{1}x+\ldots+\theta_{10}x^{10}$ 各个不同多项式模型通过训练集训练的$\theta$参数，然后用验证集验出误差，哪个模型的验证集误差较小就选哪个。 判断模型的拟合 偏差（欠拟合）： $J_{训练}(\theta)$值很高，左边的bias $J_{交叉验证}(\theta)\approx J_{训练}(\theta)$ 方差（过拟合）： $J_{训练}(\theta)$值较低，右边的variance $J_{交叉验证}(\theta)» J_{训练}(\theta)$ &gt;&gt;数学符号的意思是远远大于此值 正则化和偏差/方差 模型： $$ \begin{equation*} h_{\theta}(x)=\theta_{0}+\theta_{1}x+\theta_{2}x^2+\theta_{3}x^3+\theta_{4}x^4 \ J(\theta)=\frac{1}{2m}\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})^2+\frac{\lambda}{2m}\sum\limits_{j=1}^{m}\theta_{j}^2 \end{equation*} $$ $\lambda$值过大，导致迭代后的大部分$\theta$参数$\approx$0，最后模型输出$\approx$0，使之欠拟合； $\lambda$值过小，导致过拟合现象； 解决方案： 自动选择 $\lambda=0$ $\lambda=0.01$ $\lambda=0.02$ $\lambda=0.04$ $\lambda=0.08$ . . . $\lambda=10$ 用训练集批次$\mathop{min}limits_{\theta}J(\theta)$，然后通过验证集批次计算各自的$J_{cv}(\theta^(i))$平均的误差平方和，接着选择误差值最小的$\lambda$值，做完后观察该模型在测试集的泛化能力表现 手动选择 偏差/方差作为正则化参数$\lambda$，自己手动调整$\lambda$值(0.01~1000)，观察训练误差和验证误差，然后从验证集的曲线低洼处选择最合适$\lambda$，最后查验模型的泛化能力。 学习曲线 随着m个样本增加，训练误差随m增大二增大，验证误差经较少的样本m训练后，误差项较大，随m增大而减少，直至贴近训练误差变平。 因为模型是直线，不能很好拟合数据，m增加时总误差项增大，到了最后变平。所以样本增加的再多，训练误差不会有什么变化，如左边的紫线和蓝线随着m增大，结果没什么太大变化。 总误差项随着样本m增加而增加，但和验证误差的曲线间隔较大，说明方差较高，可以通过增加样本或正则化来解决。 评价学习算法 调试一个学习算法: 假如已实现正则线性回归模型来预测房价。然而，用一组新的样本来测试效果，发现它在预测中出现了难以接受的巨大错误。接下来要做的判断： （先画学习曲线） 获取更多训练样本 解决高方差问题 减少特征数量 解决高方差问题 添加更多特征 解决高偏差问题 减少$\lambda$值 解决高偏差问题 增加$\lambda$值 解决高方差问题 神经网络规模较小、参数少、容易欠拟合，计算量成本较低 神经网络规模较大、参数多、容易过拟合，计算量成本较高 默认一层隐藏层神经网络，添加隐藏层需测试验证集的误差，关于过拟合用正则化解决。 快速开发机器学习 开局先用简单粗暴的算法训练参数，绘制学习曲线，观察图进行误差分析，根据数据评价指标了解哪些算法是需要或不需要的，以及训练集、参数都有哪些问题等等，从而决定之后的优化方法 误差度量评估算法 癌症分类示例： 训练逻辑回归模型$h_{\theta}(x)$(癌症y=1，否则y=0)，当发现模型在测试集中有1%的错误。 测试集中有0.5%人有癌症，这错误率就不那么好了 假如模型的错误率降低到0.8，甚至0.5%，模型的质量是否提升？ 需要一个标准来判定： 实际 分组 1 0 预测 1 真阳性 假阳性 分类 0 假阴性 真阴性 精度： 所有病人中预测y=1的，有多少人换癌症？ 查准率：在所有预测患有癌症的患者中有多大比率的病人是真正拥有癌症的 \[\begin{equation*} 查准率=\frac{所有预测正确有阳性的人(真阳性)}{所有预测有阳性的人}=\frac{真阳性}{真阳性+假阳性} \end{equation*}\] 召回率：在所有患者中真实有癌症的人有多大比率 \[\begin{equation*} 召回率=\frac{真阳性}{所有患有癌症的人}=\frac{真阳性}{真阳性+假阴性} \end{equation*}\] 好的模型，查准率和召回率很高]]></summary></entry><entry><title type="html">吴恩达机器学习笔记 Day 4</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/16/Andrew-Ng-DL-4.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 4" /><published>2022-02-16T00:00:00+00:00</published><updated>2022-02-16T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/16/Andrew-Ng-DL-4</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/16/Andrew-Ng-DL-4.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ 
$\displaystyle\underbrace{a_i}_{\text{i从1到n}}$

$\displaystyle\mathop{a_i}\limits_{i\text{从1到}n}$
-->
<h2 id="非线性模型">非线性模型</h2>

<p>关于住房的分类，假设房子有很多特征，面积、卧室、门、房龄等等100多个参数，预测函数表现为：</p>

\[\begin{equation*} 

g(\theta_{0}+\theta_{1}x_{2}+\theta_{1}x_{2}+\theta_{3}x_{1}x_{2}+\theta_{4}x_{1}^{2}x_{2}\ldots\theta_{100}x_{99}x_{100}^{2}\ldots

\end{equation*}\]

<p>逻辑回归的二阶多项式二次项有多少呢，通过$\frac{n^2}{2}$可知参数大约5000个，三阶多项式约17000个三次项，特征数太大，会使特征空间急剧膨胀，这并非非线性模型的最佳解决方案。</p>

<p>图像识别中，提取车辆把手的像素点亮度矩阵：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/1.png" alt="" /></p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/2.png" alt="" /></p>

<p>车辆图片只有50x50像素点，那么n为2500个特征，每个特征值范围为0~255灰度值，假如图片为彩色RGB，RGB包含红蓝绿三个值，特征共n=7500个。</p>

<p>灰色图片的2500个特征用来建立非线性模型，二阶多项式的参数$\frac{n^2}{2}$约等于300万特征，计算量庞大，不利于计算。</p>

<h2 id="神经网络">神经网络</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/3.png" alt="" /></p>

<p>$a_{i}^{(j)}$=第j层第i个的激活输出</p>

<p>$\theta^{(j)}$=第j层映射到j+1层的参数矩阵</p>

<p>第j层神经网络有$s_{j}$个神经单元，第$j+1$层有$s_{j+1}$神经单元，此时$\theta^{(j)}$参数的维度为$s_{j+1}*(s_{j}+1)$</p>

<p>第一层的神经网络有3个神经单元，第二层有3个神经单元，第一层与第二层的参数$\theta$为3*(3+1)=12个</p>

\[\begin{equation*} 

a_{1}^{(2)}=g(\theta_{10}^{(1)}x_{0}+\theta_{11}^{(1)}x_{1}+\theta_{12}^{(1)}x_{2}+\theta_{13}^{(1)}x_{3})  \\
a_{1}^{(2)}=g(\theta_{20}^{(1)}x_{0}+\theta_{21}^{(1)}x_{1}+\theta_{22}^{(1)}x_{2}+\theta_{23}^{(1)}x_{3})  \\
a_{1}^{(2)}=g(\theta_{30}^{(1)}x_{0}+\theta_{31}^{(1)}x_{1}+\theta_{32}^{(1)}x_{2}+\theta_{33}^{(1)}x_{3})  \\
h_{\theta}(x)=a_{1}^(3)=g(\theta_{10}^{(2)}a_{0}+\theta_{11}^{(2)}a_{1}+\theta_{12}^{(2)}a_{2}+\theta_{13}^{(2)}a_{3})

\end{equation*}\]

<h3 id="一对多">一对多</h3>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/4.png" alt="四层神经网络" /></p>

<p>希望模型y能输出步人时[1 0 0 0]，输出汽车时[0 1 0 0]，输出摩托车时[0 0 1 0]等等。</p>

<h3 id="损失函数">损失函数</h3>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/5.png" alt="" /></p>

<p>L=神经网络的层数</p>

<p>$s_{l}$=第l层的神经网络神经单元数量</p>

<p>二分类，通常y输出0或1来表示，输出层的神经单元只有1个，$h_{\theta}(x)\epsilon R$,$s_{l}$=1，K=1。</p>

<p>多分类，四层神经网络的$h_{\theta}(x)=R^{k}$，$s_{l}=K\ (k\ge 3)$，神经单元至少3个以上。</p>

<p>逻辑回归的损失函数：</p>

\[\begin{equation*}
  J(\theta)=-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_{j}^{2}  \\
\end{equation*}\]

<p>神经网络的损失函数：</p>

\[\begin{equation*}
  h_{\theta}(x)=R^{k}\ (h_{\theta}(x))_{i}=i^{th}output  \\
  J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})]+\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l}+1}(\theta_{ji}^{(l)})^{2}  \\
\end{equation*}\]

<p>m个训练样本，K个输出层的神经单元，L-1层，第i层的神经单元个数，第j层的神经单元总数+1(含偏置项)</p>

<h2 id="反向传播算法">反向传播算法</h2>

<h3 id="梯度下降算法">梯度下降算法</h3>

<p>要求$\mathop{min}\limits_{\theta}J(\theta)$，需要两个条件：<br />
	- $J(\theta)$<br />
	- $\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)$</p>

<p>先计算从第一层的参数到第四层的值，计算过程具体参见<a href="https://www.zybuluo.com/hanbingtao/note/476663">https://www.zybuluo.com/hanbingtao/note/476663</a></p>

<p>第四层的输出层的误差项：$\delta_{j}^{(4)}=a_{j}^{(4)}-y_{j}$</p>

<p>第三层的隐藏层的误差项计算：$(\delta^{(3)}=(\theta^(3))^{T}\delta^{(4)}$.*$g’(z^{(3)})=a^{(3)}$.*$(1-a^{(3)})$</p>

<p>第二层的隐藏层的误差项计算：$(\delta^{(2)}=(\theta^(2))^{T}\delta^{(3)}$.*g’$(z^{(2)})=a^{(2)}.*(1-a^{(2)})$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/6.png" alt="" /></p>

<p>$\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)$的偏导数为$a_{j}^{(l)}\delta{i}^{(l+1)}$</p>

<h3 id="反向传播过程">反向传播过程</h3>

<p>训练集${(x^{(1)},y^{(1)}),\ldots,(x^{(m)},y^{(m)})}$</p>

<p>设$\Delta_{ij}^{(l)}=0\ (\text{for all}\ l,i,j)$，其中的$\Delta$是$\delta$的大写，它设一个全零的误差项矩阵，$\delta$作为累加项在迭代中增加<br />
$\text{For}\ i=1\ to\ m$:<br />
$\qquad$设$a^{(1)}=x^{(i)}$，把第i个训练样本的值输入到第一层；<br />
$\qquad$向前传播计算$a^{(l)}\ \text{for}\ l=2,3,\ldots,L$，从输入层前向计算到输出层；<br />
$\qquad$使用$y^{(i)}$计算$\delta^{(L)}=a^{(L)}-y^{(i)}$，计算输出层的误差项；<br />
$\qquad$计算$\delta^{(L-1)},\delta^(L-2),\ldots,\delta^{(2)}$，逐个从L-1层到第二层的误差项传播计算 (不考虑输入层的误差项)；<br />
$\qquad$$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_{i}^{(l+1)}$ 累加误差项。<br /></p>

<p>$\delta_{ij}$看作一个位于误差项矩阵的坐标，如果$\delta(L)$是矩阵，可以写成：</p>

\[\begin{equation*}
\Delta^{l}:=\Delta^{l}+\delta^{l+1}(a^{(l)})^{T}
\end{equation*}\]

\[\begin{equation*}
  D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}\ \text{if}\ j\ \ne\ 0  \\
  D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}\qquad\qquad \text{if}\ j\ =\ 0  \\
  \frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)}
\end{equation*}\]

<h3 id="损失函数-1">损失函数</h3>

\[\begin{equation*}
  J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})]+\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l}+1}(\theta_{ji}^{(l)})^{2} 
\end{equation*}\]

<p>这种适用于一个输出单元(K=1)的情况，多个输出就要设$\lambda$为0</p>

\[\begin{equation*}
  J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})]  \\
\end{equation*}\]

<p>损失函数$cost(i)=y^{(i)}log (h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-(h_{\theta}(x)))$，可以认为近似于$cost(i)\approx(h_{\theta}(x^{(i)}-y^{(i)})^2$</p>

<p>$\delta_{j}^{(l)}=$第l层的每个$a_{j}^{(l)}$的损失函数值</p>

<p>形式上，$\delta_{j}^{(l)}=\frac{\partial}{\partial z_{j}^{(l)}}cost(i)\qquad (j\ge 0)$，即$cost(i)=y^{(i)}log (h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-(h_{\theta}(x)))$</p>

<h2 id="梯度检验">梯度检验</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-16/7.png" alt="梯度数值的估计" /></p>

<p>梯度下降算法计算蓝色切线的斜率，梯度检验求的是由$\theta+\varepsilon$到$\theta-\varepsilon$的红线的斜率，高$J(\theta+\varepsilon)-J(\theta-\varepsilon)$，宽$J(\theta+2\varepsilon)$，由三角形边长原理求得该点导数的近似值：</p>

\[\begin{equation*}
  \frac{\partial}{\partial\theta^{(l)}}J(\theta)\approx\frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon}
\end{equation*}\]

<p>这里的$\varepsilon$值最好约等于$10^{-4}$左右的值.</p>

<p>特征向量$\theta$</p>

\[\begin{equation*}
\theta\epsilon R^{n}  \\
\theta=\theta_{1},\theta_{2},\theta_{3},\ldots,\theta_{n}  \\
\frac{\partial}{\partial\theta_{1}^{(l)}}J(\theta)\approx\frac{J(\theta_{1}+\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon}  \\
\frac{\partial}{\partial\theta_{2}^{(l)}}J(\theta)\approx\frac{J(\theta_{1},\theta_{2}+\epsilon,\theta_{3},\ldots,\theta_{n})-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon}  \\
\ldots  \\
\ldots  \\
\ldots  \\
\frac{\partial}{\partial\theta_{3}^{(l)}}J(\theta)\approx\frac{J(\theta_{1},\theta_{2},\theta_{3},\ldots,\theta_{n}+\epsilon)-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon}

\end{equation*}\]

<p>然后检查得到的近似值与反向传播的梯度，如果仅有后尾数字稍有不同，说明反向传播算法是没问题的，或高级优化算法的导数是正确的，都能很好优化$J(\theta)$。</p>

<p>实现注意：</p>
<ul>
  <li>实现反向传播算法，需计算梯度(展开$D^{(1)},D^{(2)},D^{(3)}$)</li>
  <li>实现梯度检查，需计算所有参数的近似值</li>
  <li>确保给出的值极为相似</li>
  <li>关闭梯度检查，开始学习</li>
</ul>

<p>注意：在训练分类器之前，要禁用梯度检查代码，否则每次梯度下降迭代（或损失函数的内部循环中）计算梯度变得异常缓慢。</p>

<h2 id="训练神经网络">训练神经网络</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1. 随机初始化权值
2. 执行前向传播计算以便获取$h_{\theta}$和所有的x^{(i)}值
3. 执行代码计算损失函数$J(\theta)$
4. 执行反向传播算法来计算\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$偏导数

  for i=1:m{
      使用训练集进行前向和反向计算
	  获取激活函数$a^{(l)}$和$\delta^{(l)}(l=2,\ldots,L)
	  $\Delta^{l}:=\Delta^{l}+\delta^{l+1}(a^{(l)})^{T}$
	  \ldots
  }
  \ldots
  计算偏导数$\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$
5.  使用梯度检验来比较$\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$，反向传播计算得到的偏导数值和近似值估计互相对比，然后禁用梯度检验代码
6.使用梯度下降或反向传播的高级优化方法，使最小化$J(\theta)$的参数$\theta$
</code></pre></div></div>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[非线性模型 关于住房的分类，假设房子有很多特征，面积、卧室、门、房龄等等100多个参数，预测函数表现为： \[\begin{equation*} g(\theta_{0}+\theta_{1}x_{2}+\theta_{1}x_{2}+\theta_{3}x_{1}x_{2}+\theta_{4}x_{1}^{2}x_{2}\ldots\theta_{100}x_{99}x_{100}^{2}\ldots \end{equation*}\] 逻辑回归的二阶多项式二次项有多少呢，通过$\frac{n^2}{2}$可知参数大约5000个，三阶多项式约17000个三次项，特征数太大，会使特征空间急剧膨胀，这并非非线性模型的最佳解决方案。 图像识别中，提取车辆把手的像素点亮度矩阵： 车辆图片只有50x50像素点，那么n为2500个特征，每个特征值范围为0~255灰度值，假如图片为彩色RGB，RGB包含红蓝绿三个值，特征共n=7500个。 灰色图片的2500个特征用来建立非线性模型，二阶多项式的参数$\frac{n^2}{2}$约等于300万特征，计算量庞大，不利于计算。 神经网络 $a_{i}^{(j)}$=第j层第i个的激活输出 $\theta^{(j)}$=第j层映射到j+1层的参数矩阵 第j层神经网络有$s_{j}$个神经单元，第$j+1$层有$s_{j+1}$神经单元，此时$\theta^{(j)}$参数的维度为$s_{j+1}*(s_{j}+1)$ 第一层的神经网络有3个神经单元，第二层有3个神经单元，第一层与第二层的参数$\theta$为3*(3+1)=12个 \[\begin{equation*} a_{1}^{(2)}=g(\theta_{10}^{(1)}x_{0}+\theta_{11}^{(1)}x_{1}+\theta_{12}^{(1)}x_{2}+\theta_{13}^{(1)}x_{3}) \\ a_{1}^{(2)}=g(\theta_{20}^{(1)}x_{0}+\theta_{21}^{(1)}x_{1}+\theta_{22}^{(1)}x_{2}+\theta_{23}^{(1)}x_{3}) \\ a_{1}^{(2)}=g(\theta_{30}^{(1)}x_{0}+\theta_{31}^{(1)}x_{1}+\theta_{32}^{(1)}x_{2}+\theta_{33}^{(1)}x_{3}) \\ h_{\theta}(x)=a_{1}^(3)=g(\theta_{10}^{(2)}a_{0}+\theta_{11}^{(2)}a_{1}+\theta_{12}^{(2)}a_{2}+\theta_{13}^{(2)}a_{3}) \end{equation*}\] 一对多 希望模型y能输出步人时[1 0 0 0]，输出汽车时[0 1 0 0]，输出摩托车时[0 0 1 0]等等。 损失函数 L=神经网络的层数 $s_{l}$=第l层的神经网络神经单元数量 二分类，通常y输出0或1来表示，输出层的神经单元只有1个，$h_{\theta}(x)\epsilon R$,$s_{l}$=1，K=1。 多分类，四层神经网络的$h_{\theta}(x)=R^{k}$，$s_{l}=K\ (k\ge 3)$，神经单元至少3个以上。 逻辑回归的损失函数： \[\begin{equation*} J(\theta)=-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{2m}\sum\limits_{j=1}^{n}\theta_{j}^{2} \\ \end{equation*}\] 神经网络的损失函数： \[\begin{equation*} h_{\theta}(x)=R^{k}\ (h_{\theta}(x))_{i}=i^{th}output \\ J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})]+\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l}+1}(\theta_{ji}^{(l)})^{2} \\ \end{equation*}\] m个训练样本，K个输出层的神经单元，L-1层，第i层的神经单元个数，第j层的神经单元总数+1(含偏置项) 反向传播算法 梯度下降算法 要求$\mathop{min}\limits_{\theta}J(\theta)$，需要两个条件： - $J(\theta)$ - $\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)$ 先计算从第一层的参数到第四层的值，计算过程具体参见https://www.zybuluo.com/hanbingtao/note/476663 第四层的输出层的误差项：$\delta_{j}^{(4)}=a_{j}^{(4)}-y_{j}$ 第三层的隐藏层的误差项计算：$(\delta^{(3)}=(\theta^(3))^{T}\delta^{(4)}$.*$g’(z^{(3)})=a^{(3)}$.*$(1-a^{(3)})$ 第二层的隐藏层的误差项计算：$(\delta^{(2)}=(\theta^(2))^{T}\delta^{(3)}$.*g’$(z^{(2)})=a^{(2)}.*(1-a^{(2)})$ $\frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)$的偏导数为$a_{j}^{(l)}\delta{i}^{(l+1)}$ 反向传播过程 训练集${(x^{(1)},y^{(1)}),\ldots,(x^{(m)},y^{(m)})}$ 设$\Delta_{ij}^{(l)}=0\ (\text{for all}\ l,i,j)$，其中的$\Delta$是$\delta$的大写，它设一个全零的误差项矩阵，$\delta$作为累加项在迭代中增加 $\text{For}\ i=1\ to\ m$: $\qquad$设$a^{(1)}=x^{(i)}$，把第i个训练样本的值输入到第一层； $\qquad$向前传播计算$a^{(l)}\ \text{for}\ l=2,3,\ldots,L$，从输入层前向计算到输出层； $\qquad$使用$y^{(i)}$计算$\delta^{(L)}=a^{(L)}-y^{(i)}$，计算输出层的误差项； $\qquad$计算$\delta^{(L-1)},\delta^(L-2),\ldots,\delta^{(2)}$，逐个从L-1层到第二层的误差项传播计算 (不考虑输入层的误差项)； $\qquad$$\Delta_{ij}^{(l)}:=\Delta_{ij}^{(l)}+a_{j}^{(l)}\delta_{i}^{(l+1)}$ 累加误差项。 $\delta_{ij}$看作一个位于误差项矩阵的坐标，如果$\delta(L)$是矩阵，可以写成： \[\begin{equation*} \Delta^{l}:=\Delta^{l}+\delta^{l+1}(a^{(l)})^{T} \end{equation*}\] \[\begin{equation*} D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}+\lambda\theta_{ij}^{(l)}\ \text{if}\ j\ \ne\ 0 \\ D_{ij}^{(l)}:=\frac{1}{m}\Delta_{ij}^{(l)}\qquad\qquad \text{if}\ j\ =\ 0 \\ \frac{\partial}{\partial\theta_{ij}^{(l)}}J(\theta)=D_{ij}^{(l)} \end{equation*}\] 损失函数 \[\begin{equation*} J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})]+\frac{\lambda}{2m}\sum\limits_{l=1}^{L-1}\sum\limits_{i=1}^{s_l}\sum\limits_{j=1}^{s_{l}+1}(\theta_{ji}^{(l)})^{2} \end{equation*}\] 这种适用于一个输出单元(K=1)的情况，多个输出就要设$\lambda$为0 \[\begin{equation*} J(\theta)=-\frac{1}{m}[\sum\limits_{i=1}^{m}\sum\limits_{k=1}^{K}y_{k}^{(i)}log (h_{\theta}(x^{(i)}))_{k}+(1-y_{k}^{(i)})log(1-(h_{\theta}(x))_{k})] \\ \end{equation*}\] 损失函数$cost(i)=y^{(i)}log (h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-(h_{\theta}(x)))$，可以认为近似于$cost(i)\approx(h_{\theta}(x^{(i)}-y^{(i)})^2$ $\delta_{j}^{(l)}=$第l层的每个$a_{j}^{(l)}$的损失函数值 形式上，$\delta_{j}^{(l)}=\frac{\partial}{\partial z_{j}^{(l)}}cost(i)\qquad (j\ge 0)$，即$cost(i)=y^{(i)}log (h_{\theta}(x^{(i)}))+(1-y^{(i)})log(1-(h_{\theta}(x)))$ 梯度检验 梯度下降算法计算蓝色切线的斜率，梯度检验求的是由$\theta+\varepsilon$到$\theta-\varepsilon$的红线的斜率，高$J(\theta+\varepsilon)-J(\theta-\varepsilon)$，宽$J(\theta+2\varepsilon)$，由三角形边长原理求得该点导数的近似值： \[\begin{equation*} \frac{\partial}{\partial\theta^{(l)}}J(\theta)\approx\frac{J(\theta+\varepsilon)-J(\theta-\varepsilon)}{2\varepsilon} \end{equation*}\] 这里的$\varepsilon$值最好约等于$10^{-4}$左右的值. 特征向量$\theta$ \[\begin{equation*} \theta\epsilon R^{n} \\ \theta=\theta_{1},\theta_{2},\theta_{3},\ldots,\theta_{n} \\ \frac{\partial}{\partial\theta_{1}^{(l)}}J(\theta)\approx\frac{J(\theta_{1}+\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon} \\ \frac{\partial}{\partial\theta_{2}^{(l)}}J(\theta)\approx\frac{J(\theta_{1},\theta_{2}+\epsilon,\theta_{3},\ldots,\theta_{n})-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon} \\ \ldots \\ \ldots \\ \ldots \\ \frac{\partial}{\partial\theta_{3}^{(l)}}J(\theta)\approx\frac{J(\theta_{1},\theta_{2},\theta_{3},\ldots,\theta_{n}+\epsilon)-J(\theta_{1}-\epsilon,\theta_{2},\theta_{3},\ldots,\theta_{n})}{2\epsilon} \end{equation*}\] 然后检查得到的近似值与反向传播的梯度，如果仅有后尾数字稍有不同，说明反向传播算法是没问题的，或高级优化算法的导数是正确的，都能很好优化$J(\theta)$。 实现注意： 实现反向传播算法，需计算梯度(展开$D^{(1)},D^{(2)},D^{(3)}$) 实现梯度检查，需计算所有参数的近似值 确保给出的值极为相似 关闭梯度检查，开始学习 注意：在训练分类器之前，要禁用梯度检查代码，否则每次梯度下降迭代（或损失函数的内部循环中）计算梯度变得异常缓慢。 训练神经网络 1. 随机初始化权值 2. 执行前向传播计算以便获取$h_{\theta}$和所有的x^{(i)}值 3. 执行代码计算损失函数$J(\theta)$ 4. 执行反向传播算法来计算\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$偏导数 for i=1:m{ 使用训练集进行前向和反向计算 获取激活函数$a^{(l)}$和$\delta^{(l)}(l=2,\ldots,L) $\Delta^{l}:=\Delta^{l}+\delta^{l+1}(a^{(l)})^{T}$ \ldots } \ldots 计算偏导数$\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$ 5. 使用梯度检验来比较$\frac{\partial}{\partial\theta_{jk}^{(l)}}J(\theta)$，反向传播计算得到的偏导数值和近似值估计互相对比，然后禁用梯度检验代码 6.使用梯度下降或反向传播的高级优化方法，使最小化$J(\theta)$的参数$\theta$]]></summary></entry><entry><title type="html">吴恩达机器学习笔记 Day 3</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/14/Andrew-Ng-DL-3.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 3" /><published>2022-02-14T00:00:00+00:00</published><updated>2022-02-14T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/14/Andrew-Ng-DL-3</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/14/Andrew-Ng-DL-3.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ 
$\displaystyle\underbrace{a_i}_{\text{i从1到n}}$

$\displaystyle\mathop{a_i}\limits_{i\text{从1到}n}$
-->
<h2 id="分类">分类</h2>

<p>先从二分类开始，假设$y\epsilon${0,1}，0和1分别输出no、yes：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/7.png" alt="" /></p>

<p>阈值可以设为0.5，当激活函数的加权输入$\ge$0.5则输出1，小于0.5则输出0，如图所示：</p>

<blockquote>
  <p>这种叫<strong>Logistic回归（逻辑回归）</strong>。</p>
</blockquote>

<p>但增加额外样本后，蓝线拟合情况就不那么好了，离红叉号的偏差较高，如图所示：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/8.png" alt="" /></p>

<p>不推荐线性回归模型用于分类问题。</p>

<h2 id="逻辑回归">逻辑回归</h2>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta^{T}x)
\end{equation*}\]

<p>其中sigmoid激活函数</p>

\[\begin{equation*}
g(z)=\frac{1}{1+e^{-z}}
\end{equation*}\]

<blockquote>
  <p>Sigmoid和Logistic函数是同义词</p>
</blockquote>

<p>激活函数的z为$\theta^{T}x$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/9.png" alt="" /></p>

<p>z横坐标，$g(z)$纵坐标，左端趋于0，右端趋于1。</p>

<p>假设一个患者的肿瘤，模型输出肿瘤大小x对于患者是有害还是无害的：</p>

\[\begin{equation*}
x=\begin{bmatrix}
x_{0}  \\
x_{1}
\end{bmatrix}
=\begin{bmatrix}
1  \\
\text{肿瘤面积参数1}  \\
\end{bmatrix}
h_{\theta}(x)=0.7
\end{equation*}\]

<p>说明患者的肿瘤输出y=1的概率为0.7，写成数学表达式：$P(y=1|x;\theta)$，$\theta$为概率。
y=1的表达式：$P(y=1|x;\theta)=1-P(y=1|x;\theta)$</p>

<h3 id="决策边界">决策边界</h3>

<p>决策边界(decision boundary)，下图的紫线是决策边界：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/10.png" alt="" /></p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2})  \\
\end{equation*}\]

<p>输出</p>

\[\begin{equation*}
  \theta^{T}x=-3+x_{1}+x_{2}\ge 0  \\
\end{equation*}\]

<p>所以</p>

\[\begin{equation*}
  x_{1}+x_{2}\ge 3  \\
  x_{1}+x_{2}&lt; 3  \\

\end{equation*}\]

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/11.png" alt="" /></p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4})  \\
\end{equation*}\]

<p>$\theta$初始化为{-1，0，0，1，1}：</p>

\[\begin{equation*}
  \theta=
  \begin{bmatrix}
  -1  \\ 0  \\ 0  \\ 1  \\ 1  \\
  \end{bmatrix}
\end{equation*}\]

<p>如果y=1</p>

\[\begin{equation*}
  -1+x_{1}^{2}+x_{2}^{2}\ge 0  \\
\end{equation*}\]

<p>那么</p>

\[\begin{equation*}
  x_{1}^{2}+x_{2}^{2}\ge 1
\end{equation*}\]

<p>决策边界是取决于参数的，并非通过训练训练集得出的结果，参数不同会得到不同的决策边界。</p>

<p>复杂点的例子：</p>

\[\begin{equation*}
  h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{1}^{2}x_{2}+\theta_{4}x_{1}^{2}x_{2}^{2}+\theta_{4}x_{1}^{3}x_{2}+\ldots)  \\

\end{equation*}\]

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/12.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/13.png" alt="" /></p>

<h3 id="损失函数">损失函数</h3>

<p>给出训练集$(x^{(i)},y^{(i)})$，i维特征向量矩阵，以及激活函数：</p>

<p>$(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\ldots,(x^{(n)},y^{(n)})$</p>

\[\begin{equation*}
  x^{(i)}=
  \begin{bmatrix}
    x_{0}^{(i)}  \\
    x_{1}^{(i)}  \\
    \ldots     \\
    x_{n}^{(i)}  
  \end{bmatrix}
  x_{0}=1,\ 0\ &lt;\ y\ &lt;\ 1  \\
  h_{\theta}(x)=\frac{1}{1+e^{-z}}
\end{equation*}\]

<p>接下来$\theta$目标值如何优化？</p>

<p>原来的线性回归模型的损失函数，把$\frac{1}{2}$移至右边函数里面：</p>

\[\begin{equation*}
  J(\theta_{0},\theta_{1})=\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2  \\
  
  Cost(h_{\theta}(x^{(i)}),y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)}),y^{(i)})^2
\end{equation*}\]

<p>这种损失函数在线性回归模型比较能良好工作，线性回归的非凹函数和凹函数，通常是希望凹函数能很好收敛全局最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/14.png" alt="" /></p>

<p>代价函数可能实际是非凹函数图的，迭代有时经常收敛到局部最优解，为了达到理想中的梯度下降，需要推导新的梯度下降算法来收敛全局最小值的。</p>

\[\begin{equation*}
  Cost(h_{\theta}(x),y)=
  \begin{cases}-log(h_{\theta}(x)) &amp; \text{if}\ y=1  \\
  -log(1-h_{\theta}(x)) &amp; \text{if}\ y=0  
  \end{cases}
\end{equation*}\]

<p>当y=1时，纵坐标为$J(\theta)$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/15.png" alt="" /></p>

<p>y值无限趋于1，当$y=1,h_{\theta}(x)=1$时，损失函数输出为0，如果$h_{\theta}(x)$趋于0，那么损失趋于无穷。</p>

<p>如果$h_{\theta}=0$，$P(y=1|x;\theta)=0$，y=1的概率趋于0。假如y=1的话，那么损失函数的值就很大了，用很大的损失来训练这个学习算法，使之下次确保输出1的概率趋于1.</p>

<p>当y=0时，原理是一样的：</p>

<p>$h_{\theta}=1$，$P(y=0|x;\theta)=0$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/16.png" alt="" /></p>

<h3 id="简化损失函数">简化损失函数</h3>

\[\begin{equation*}
  J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}Cost(h_{\theta}(x^{(i)},y^{(i)})  \\
  Cost(h_{\theta}(x),y)=
  \begin{cases}-log(h_{\theta}(x)) &amp; if\ y=1  \\
  -log(1-h_{\theta}(x)) &amp; if\ y=0  
  \end{cases}
\end{equation*}\]

<p>为了避免梯度下降算法按两种情况写，把两个式子合并:</p>

\[\begin{equation*}
  Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x))  \\
\end{equation*}\]

<p>逻辑回归的损失函数，可以这么写的：</p>

\[\begin{equation*}
  
  \begin{split}
  J(\theta)
  &amp; =\frac{1}{m}Cost(h_{\theta}(x^{(i)},y^{(i)})  \\
  &amp; =-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]  \\
  \end{split}
\end{equation*}\]

<p>公式是由统计学的极大似然法决定的，统计学为不同模型快速找参数，给出此方法(原理和证明略)，它的优点是凸函数。</p>

<h3 id="梯度下降算法">梯度下降算法</h3>

\[\begin{equation*} min_{\theta}J(\theta):  \\
\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta)
\end{equation*}\]

<p>在此式中，$\frac{\partial}{\partial\theta_{j}}J(\theta)$多元微分推导成：</p>

\[\begin{equation*} \frac{\partial}{\partial\theta_{j}}J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\end{equation*}\]

<p>所以导数项：</p>

\[\begin{equation*} \theta_{j}:=\theta_{j}-\alpha \displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\end{equation*}\]

<p>有若干$\theta$参数，更新参数值需要用到这个公式。</p>

<p>线性回归和逻辑回归有什么区别？它们的梯度下降算法不是同一个，拟合数据也比线性回归更佳，目前已广泛应用于机器学习领域中。</p>

<h2 id="高级优化算法">高级优化算法</h2>

<p>优化算法，给一个$\theta$，需要代码去计算损失函数和偏导数：</p>

<ul>
  <li>$J(\theta)$</li>
  <li>$\frac{\partial}{\partial\theta_{j}}J(\theta) \qquad (for\ j=0,1,\ldots,n)$</li>
</ul>

<p>然后利用梯度下降算法去优化目标值：</p>

\[\begin{equation*} min_{\theta}J(\theta):  \\
\theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta)
\end{equation*}\]

<p>除了常规优化算法，还有一些高级优化算法：</p>

<ul>
  <li>批量梯度下降算法</li>
  <li>共轭梯度法</li>
  <li>BFGS</li>
  <li>L-BFGS</li>
</ul>

<p>已知的梯度下降算法就不介绍了，简单介绍其它三种优化算法的优点：
优点：
	- 不需要手动选择学习率
	- 比常规梯度下降算法快多了</p>

<p>缺点：
	- 非常复杂
	- 漫长的学习时长</p>

<p>这些算法给出计算导数项和损失函数的方法，它们有线性搜索算法(智能内循环clever inner-loop)，每次迭代自动尝试不同学习率，所以收敛速度远远比常规梯度下降算法快多了。</p>

<p>细节待补充…可能要学几周时间</p>

<h2 id="多分类一对多">多分类：一对多</h2>

<p>二分类  VS  多分类：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/17.png" alt="" /></p>

<p>三种符号来代表三个不同类别的样本，逻辑回归模型能解决二类别问题，将<strong>训练集分为正类和负类</strong>，这种思想可以用于多分类：</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/18.png" alt="" /></p>

<p>把三角形划分正类，其它作为圆形划分到负类，那么就可以训练一个标准的<strong>逻辑回归分类器</strong>，类推红叉号、蓝正方形<strong>可以这么划分</strong>。</p>

<p>拟合出3个分类器，根据$h_{\theta}^{(i)}=P(y=i|x;\theta)=0\qquad(i=1,2,3)$来估计出x的概率，$\displaystyle\mathop{max}\limits_{i}h_{\theta_{\theta}^{(i)}(x)}$<strong>哪个概率大,分别对应各自的分类</strong>。</p>

<p>每个分类器针对一种情况进行训练</p>

<h2 id="过度拟合的问题">过度拟合的问题</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/19.png" alt="" /></p>

<p>第一个图，函数模型的直线不能很好拟合训练集，房价上升处是比较平缓、曲线的，这叫<strong>欠拟合</strong>，这算法具有<strong>高偏差</strong>，因为它和训练集理想的情况有很大的偏差；</p>

<p>第二个图，加入二项式，它能很好<strong>拟合</strong>训练集；</p>

<p>第三个图，四项式模拟的曲线上下波动，看似很好的拟合训练集(正好通过每一个训练样本），实际并不是预测房价的好模型，这种称为<strong>过度拟合</strong>，算法具有<strong>高方差</strong>。</p>

<p>高阶多项式虽然能拟合训练集，但它面临庞大的变量问题，导致训练进程缓慢，再千方百计拟合训练集，只要不能泛化到新的样本就是白搭。</p>

<blockquote>
  <p>泛化：模型应用到新样本的能力</p>
</blockquote>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/20.png" alt="" /></p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/21.png" alt="" /></p>

<p>解决方案：
	1. 减少特征变量$\theta$
		- 手动保留特征变量
		- 模型选择算法
	2. 正则化
		- 保留所有特征，减小参数的量级或参数$\theta_{j}$的大小
		- 训练更多的训练样本，它能很好工作</p>

<h2 id="损失函数正则化">损失函数正则化</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/22.png" alt="" /></p>

<p>高阶多项式，由于过拟合现象导致预测效果不太理想，为了解决应添加正则化参数，$\lambda$参数设为1000，因为数值较大，$\theta_{3}$和$\theta_{4}$的<strong>数值尽可能小</strong>。</p>

\[\begin{equation*} 

\mathop{min}\limits{\theta}\frac{1}{2m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+1000\ \theta_{3}^3 + 1000\ \theta_{4}^2

\end{equation*}\]

<p>$\theta_{3}\ \approx\ 0\qquad\theta_{3}\ \approx\ 0$，这样取得更好的模型，增大了两个参数带来的效果。在实际测试中，结果表明<strong>参数越接近0</strong>，得到的函数越<strong>平滑、简单</strong>(原理证明略)。</p>

<p>假设预测房价的模型的特征为$x_{1},x_{2},\ldots,x_{100}$，参数为$\theta_{0},\theta_{1},\theta_{2},\ldots,\theta_{100}$</p>

<h3 id="正则化算法">正则化算法</h3>

\[\begin{equation*} 

J(\theta)=\frac{1}{2m}[\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+\lambda\sum\limits_{i=1}^m\theta_{j}^2]

\end{equation*}\]

<p>如果$\lambda$值取得太大，会造成惩罚值变大，导致迭代后的参数几乎接近0，最后结果为欠拟合。</p>

<h2 id="线性回归的正则化">线性回归的正则化</h2>

<h3 id="正则化损失函数">正则化损失函数:</h3>

\[\begin{equation*} 
  J(\theta)=\frac{1}{2m}[\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\lambda\sum\limits_{i=1}^n \theta_{j}^2]
\end{equation*}\]

<h3 id="梯度下降算法-1">梯度下降算法：</h3>

<p>$\theta_{0}$已经算过了：</p>

\[\begin{equation*} 
  \theta_{0}:=\theta_{0}-\alpha\qquad\frac{1}{m}\sum\limits_{i=1}^m \mathop{(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}}
\end{equation*}\]

<p>计算正则化损失函数对$\theta_{j}$，即$\frac{\partial}{\partial \theta_{j}}J(\theta)$偏导数</p>

\[\begin{equation*} 
  \theta_{j}:=\theta_{j}-\alpha[\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}]
\end{equation*}\]

\[\begin{equation*} 
  \theta_{j}:=\theta_{j}(1-\alpha\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}
\end{equation*}\]

<p>训练所有训练样本后，加起来再除以m进行迭代$\theta$参数。正则化的线性回归，它不迭代$\theta_{0}$参数，因为正则化是计算$j=1,2,3,\ldots,n$惩罚值的，要分开计算两种参数的惩罚值。</p>

<p>这里的$\theta_{j}(1-\alpha\frac{1}{m})&lt;1$，学习率很小，m是较大的数值，因此$\alpha\frac{1}{m}$是一个正数，假如该项值=0.99，那么$\theta*0.99$值会变得小一点，也就是在$\theta_{j}^2$的平方范数值变小了。</p>

<h2 id="正规方程的正则化">正规方程的正则化</h2>

<p>设计一个$m*(n+1)$的矩阵X，X的每行代表一个训练样本，y矩阵为训练集的标签：</p>

\[\begin{equation*} 
X=
\begin{bmatrix}
(x^{(1)})^{T}  \\
\ldots  \\
(x^{(m)})^{T}  
\end{bmatrix}
\qquad\qquad\qquad
y=
\begin{bmatrix}
y^{(1)}  \\
\ldots   \\
y^{(m)}  
\end{bmatrix}
\end{equation*}\]

<p>为了最小化损失函数，$\mathop{min}\limits{\theta}J(\theta)$</p>

<h3 id="梯度下降算法-2">梯度下降算法</h3>

<p>$\frac{\partial}{\partial \theta_{j}}J(\theta)$设各个参数为0进行偏导
$$
\begin{equation*} 
\theta=(x^{T}X+\lambda
\begin{bmatrix}
0&amp;0&amp;0&amp;\ldots&amp;0  <br />
0&amp;1&amp;0&amp;\ldots&amp;0  <br />
0&amp;0&amp;1&amp;\ldots&amp;0  <br />
0&amp;0&amp;0&amp;\ldots&amp;0  <br />
0&amp;0&amp;0&amp;\ldots&amp;1
\end{bmatrix}
)^{-1}X^{T}y</p>

<p>\end{equation*}
$$</p>

<p>设n=2的话，上面公式的矩阵是3x3，因此矩阵是(n+1)x(n+1)，按公式去计算可以取得$J(\theta)$最小值。</p>

<h3 id="正规方程不可逆">正规方程不可逆</h3>

<p>当训练样本$m\ \le$特征$n$时，X转置乘以X的矩阵是不可逆的，算法：</p>

\[\begin{equation*} 
\theta=(x^{T}X)^{-1}X^{T}y
\end{equation*}\]

<p>解决方案是减少$\theta_{j}$参数来解决的，正则化考虑到这点，只要正则化参数，严格要求<strong>$\lambda$大于0</strong>，一般可以解决不可逆的问题。</p>

\[\begin{equation*} 
\theta=(x^{T}X+\lambda
\begin{bmatrix}
0&amp;0&amp;0&amp;\ldots&amp;0  \\
0&amp;1&amp;0&amp;\ldots&amp;0  \\
0&amp;0&amp;1&amp;\ldots&amp;0  \\
0&amp;0&amp;0&amp;\ldots&amp;0  \\
0&amp;0&amp;0&amp;\ldots&amp;1
\end{bmatrix}
)^{-1}X^{T}y

\end{equation*}\]

<h2 id="逻辑回归的正则化">逻辑回归的正则化</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/23.png" alt="" /></p>

<p>逻辑回归的函数有众多相关度较低的特征，损失函数为：</p>

\[\begin{equation*} 
-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]

\end{equation*}\]

<p>正则化的逻辑回归损失函数：</p>

\[\begin{equation*} 
-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{m}\theta_{j}
\end{equation*}\]

<p>依旧是针对$\theta_{1},\theta_{2},\ldots,\theta_{n}$参数，关于参数的更新，还是分开对待的。</p>

<h3 id="梯度下降算法-3">梯度下降算法</h3>

\[\begin{equation*} 
  \theta_{j}:=\theta_{j}(1-\alpha\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}  \\
  \theta_{j}:=\theta_{j}(1-\alpha[\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}]  \\
\end{equation*}\]

<p>其中的$\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}$是损失函数</p>

<p>关于更高级的优化算法使用正则化，假设有n个参数的n维矩阵：</p>

\[\begin{equation*} 
\theta=
\begin{bmatrix}
\theta_{0}  \\
\theta_{1}  \\
\theta_{2}  \\
\ldots  \\
\theta_{n}
\end{bmatrix}
\end{equation*}\]

<h3 id="优化算法">优化算法</h3>

<p>先计算所有样本的$J(\theta)$：</p>

\[\begin{equation*} 
-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{m}\theta_{j}
\end{equation*}\]

<p>然后计算$\frac{\partial}{\partial\theta_{0}}J(\theta)$对\theta_{0}$的偏导数：
\(\begin{equation*} 
  \frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{0}^{(i)})
\end{equation*}\)
$\frac{\partial}{\partial\theta_{1}}J(\theta)$对于$\theta_{1}$的导数项：
\(\begin{equation*} 
  [\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{1}^{(i)})]+\frac{\lambda}{m}\theta_{1}
\end{equation*}\)
$\frac{\partial}{\partial\theta_{2}}J(\theta)$对于$\theta_{2}$导数项：
\(\begin{equation*} 
  [\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{2}^{(i)})]+\frac{\lambda}{m}\theta_{2}
\end{equation*}\)
后面</p>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[分类 先从二分类开始，假设$y\epsilon${0,1}，0和1分别输出no、yes： 阈值可以设为0.5，当激活函数的加权输入$\ge$0.5则输出1，小于0.5则输出0，如图所示： 这种叫Logistic回归（逻辑回归）。 但增加额外样本后，蓝线拟合情况就不那么好了，离红叉号的偏差较高，如图所示： 不推荐线性回归模型用于分类问题。 逻辑回归 \[\begin{equation*} h_{\theta}(x)=g(\theta^{T}x) \end{equation*}\] 其中sigmoid激活函数 \[\begin{equation*} g(z)=\frac{1}{1+e^{-z}} \end{equation*}\] Sigmoid和Logistic函数是同义词 激活函数的z为$\theta^{T}x$ z横坐标，$g(z)$纵坐标，左端趋于0，右端趋于1。 假设一个患者的肿瘤，模型输出肿瘤大小x对于患者是有害还是无害的： \[\begin{equation*} x=\begin{bmatrix} x_{0} \\ x_{1} \end{bmatrix} =\begin{bmatrix} 1 \\ \text{肿瘤面积参数1} \\ \end{bmatrix} h_{\theta}(x)=0.7 \end{equation*}\] 说明患者的肿瘤输出y=1的概率为0.7，写成数学表达式：$P(y=1|x;\theta)$，$\theta$为概率。 y=1的表达式：$P(y=1|x;\theta)=1-P(y=1|x;\theta)$ 决策边界 决策边界(decision boundary)，下图的紫线是决策边界： \[\begin{equation*} h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}) \\ \end{equation*}\] 输出 \[\begin{equation*} \theta^{T}x=-3+x_{1}+x_{2}\ge 0 \\ \end{equation*}\] 所以 \[\begin{equation*} x_{1}+x_{2}\ge 3 \\ x_{1}+x_{2}&lt; 3 \\ \end{equation*}\] \[\begin{equation*} h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{4}) \\ \end{equation*}\] $\theta$初始化为{-1，0，0，1，1}： \[\begin{equation*} \theta= \begin{bmatrix} -1 \\ 0 \\ 0 \\ 1 \\ 1 \\ \end{bmatrix} \end{equation*}\] 如果y=1 \[\begin{equation*} -1+x_{1}^{2}+x_{2}^{2}\ge 0 \\ \end{equation*}\] 那么 \[\begin{equation*} x_{1}^{2}+x_{2}^{2}\ge 1 \end{equation*}\] 决策边界是取决于参数的，并非通过训练训练集得出的结果，参数不同会得到不同的决策边界。 复杂点的例子： \[\begin{equation*} h_{\theta}(x)=g(\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}+\theta_{4}x_{1}^{2}x_{2}+\theta_{4}x_{1}^{2}x_{2}^{2}+\theta_{4}x_{1}^{3}x_{2}+\ldots) \\ \end{equation*}\] 损失函数 给出训练集$(x^{(i)},y^{(i)})$，i维特征向量矩阵，以及激活函数： $(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\ldots,(x^{(n)},y^{(n)})$ \[\begin{equation*} x^{(i)}= \begin{bmatrix} x_{0}^{(i)} \\ x_{1}^{(i)} \\ \ldots \\ x_{n}^{(i)} \end{bmatrix} x_{0}=1,\ 0\ &lt;\ y\ &lt;\ 1 \\ h_{\theta}(x)=\frac{1}{1+e^{-z}} \end{equation*}\] 接下来$\theta$目标值如何优化？ 原来的线性回归模型的损失函数，把$\frac{1}{2}$移至右边函数里面： \[\begin{equation*} J(\theta_{0},\theta_{1})=\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^2 \\ Cost(h_{\theta}(x^{(i)}),y^{(i)})=\frac{1}{2}(h_{\theta}(x^{(i)}),y^{(i)})^2 \end{equation*}\] 这种损失函数在线性回归模型比较能良好工作，线性回归的非凹函数和凹函数，通常是希望凹函数能很好收敛全局最优解。 代价函数可能实际是非凹函数图的，迭代有时经常收敛到局部最优解，为了达到理想中的梯度下降，需要推导新的梯度下降算法来收敛全局最小值的。 \[\begin{equation*} Cost(h_{\theta}(x),y)= \begin{cases}-log(h_{\theta}(x)) &amp; \text{if}\ y=1 \\ -log(1-h_{\theta}(x)) &amp; \text{if}\ y=0 \end{cases} \end{equation*}\] 当y=1时，纵坐标为$J(\theta)$ y值无限趋于1，当$y=1,h_{\theta}(x)=1$时，损失函数输出为0，如果$h_{\theta}(x)$趋于0，那么损失趋于无穷。 如果$h_{\theta}=0$，$P(y=1|x;\theta)=0$，y=1的概率趋于0。假如y=1的话，那么损失函数的值就很大了，用很大的损失来训练这个学习算法，使之下次确保输出1的概率趋于1. 当y=0时，原理是一样的： $h_{\theta}=1$，$P(y=0|x;\theta)=0$ 简化损失函数 \[\begin{equation*} J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}Cost(h_{\theta}(x^{(i)},y^{(i)}) \\ Cost(h_{\theta}(x),y)= \begin{cases}-log(h_{\theta}(x)) &amp; if\ y=1 \\ -log(1-h_{\theta}(x)) &amp; if\ y=0 \end{cases} \end{equation*}\] 为了避免梯度下降算法按两种情况写，把两个式子合并: \[\begin{equation*} Cost(h_{\theta}(x),y)=-ylog(h_{\theta}(x))-(1-y)log(1-h_{\theta}(x)) \\ \end{equation*}\] 逻辑回归的损失函数，可以这么写的： \[\begin{equation*} \begin{split} J(\theta) &amp; =\frac{1}{m}Cost(h_{\theta}(x^{(i)},y^{(i)}) \\ &amp; =-\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))] \\ \end{split} \end{equation*}\] 公式是由统计学的极大似然法决定的，统计学为不同模型快速找参数，给出此方法(原理和证明略)，它的优点是凸函数。 梯度下降算法 \[\begin{equation*} min_{\theta}J(\theta): \\ \theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta) \end{equation*}\] 在此式中，$\frac{\partial}{\partial\theta_{j}}J(\theta)$多元微分推导成： \[\begin{equation*} \frac{\partial}{\partial\theta_{j}}J(\theta)=\frac{1}{m}\displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \end{equation*}\] 所以导数项： \[\begin{equation*} \theta_{j}:=\theta_{j}-\alpha \displaystyle\sum\limits_{i=1}^{m}(h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \end{equation*}\] 有若干$\theta$参数，更新参数值需要用到这个公式。 线性回归和逻辑回归有什么区别？它们的梯度下降算法不是同一个，拟合数据也比线性回归更佳，目前已广泛应用于机器学习领域中。 高级优化算法 优化算法，给一个$\theta$，需要代码去计算损失函数和偏导数： $J(\theta)$ $\frac{\partial}{\partial\theta_{j}}J(\theta) \qquad (for\ j=0,1,\ldots,n)$ 然后利用梯度下降算法去优化目标值： \[\begin{equation*} min_{\theta}J(\theta): \\ \theta_{j}:=\theta_{j}-\alpha \frac{\partial}{\partial\theta_{j}}J(\theta) \end{equation*}\] 除了常规优化算法，还有一些高级优化算法： 批量梯度下降算法 共轭梯度法 BFGS L-BFGS 已知的梯度下降算法就不介绍了，简单介绍其它三种优化算法的优点： 优点： - 不需要手动选择学习率 - 比常规梯度下降算法快多了 缺点： - 非常复杂 - 漫长的学习时长 这些算法给出计算导数项和损失函数的方法，它们有线性搜索算法(智能内循环clever inner-loop)，每次迭代自动尝试不同学习率，所以收敛速度远远比常规梯度下降算法快多了。 细节待补充…可能要学几周时间 多分类：一对多 二分类 VS 多分类： 三种符号来代表三个不同类别的样本，逻辑回归模型能解决二类别问题，将训练集分为正类和负类，这种思想可以用于多分类： 把三角形划分正类，其它作为圆形划分到负类，那么就可以训练一个标准的逻辑回归分类器，类推红叉号、蓝正方形可以这么划分。 拟合出3个分类器，根据$h_{\theta}^{(i)}=P(y=i|x;\theta)=0\qquad(i=1,2,3)$来估计出x的概率，$\displaystyle\mathop{max}\limits_{i}h_{\theta_{\theta}^{(i)}(x)}$哪个概率大,分别对应各自的分类。 每个分类器针对一种情况进行训练 过度拟合的问题 第一个图，函数模型的直线不能很好拟合训练集，房价上升处是比较平缓、曲线的，这叫欠拟合，这算法具有高偏差，因为它和训练集理想的情况有很大的偏差； 第二个图，加入二项式，它能很好拟合训练集； 第三个图，四项式模拟的曲线上下波动，看似很好的拟合训练集(正好通过每一个训练样本），实际并不是预测房价的好模型，这种称为过度拟合，算法具有高方差。 高阶多项式虽然能拟合训练集，但它面临庞大的变量问题，导致训练进程缓慢，再千方百计拟合训练集，只要不能泛化到新的样本就是白搭。 泛化：模型应用到新样本的能力 解决方案： 1. 减少特征变量$\theta$ - 手动保留特征变量 - 模型选择算法 2. 正则化 - 保留所有特征，减小参数的量级或参数$\theta_{j}$的大小 - 训练更多的训练样本，它能很好工作 损失函数正则化 高阶多项式，由于过拟合现象导致预测效果不太理想，为了解决应添加正则化参数，$\lambda$参数设为1000，因为数值较大，$\theta_{3}$和$\theta_{4}$的数值尽可能小。 \[\begin{equation*} \mathop{min}\limits{\theta}\frac{1}{2m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+1000\ \theta_{3}^3 + 1000\ \theta_{4}^2 \end{equation*}\] $\theta_{3}\ \approx\ 0\qquad\theta_{3}\ \approx\ 0$，这样取得更好的模型，增大了两个参数带来的效果。在实际测试中，结果表明参数越接近0，得到的函数越平滑、简单(原理证明略)。 假设预测房价的模型的特征为$x_{1},x_{2},\ldots,x_{100}$，参数为$\theta_{0},\theta_{1},\theta_{2},\ldots,\theta_{100}$ 正则化算法 \[\begin{equation*} J(\theta)=\frac{1}{2m}[\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})^{2}+\lambda\sum\limits_{i=1}^m\theta_{j}^2] \end{equation*}\] 如果$\lambda$值取得太大，会造成惩罚值变大，导致迭代后的参数几乎接近0，最后结果为欠拟合。 线性回归的正则化 正则化损失函数: \[\begin{equation*} J(\theta)=\frac{1}{2m}[\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\lambda\sum\limits_{i=1}^n \theta_{j}^2] \end{equation*}\] 梯度下降算法： $\theta_{0}$已经算过了： \[\begin{equation*} \theta_{0}:=\theta_{0}-\alpha\qquad\frac{1}{m}\sum\limits_{i=1}^m \mathop{(h_{\theta}(x^{(i)})-y^{(i)})x_{0}^{(i)}} \end{equation*}\] 计算正则化损失函数对$\theta_{j}$，即$\frac{\partial}{\partial \theta_{j}}J(\theta)$偏导数 \[\begin{equation*} \theta_{j}:=\theta_{j}-\alpha[\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}] \end{equation*}\] \[\begin{equation*} \theta_{j}:=\theta_{j}(1-\alpha\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \end{equation*}\] 训练所有训练样本后，加起来再除以m进行迭代$\theta$参数。正则化的线性回归，它不迭代$\theta_{0}$参数，因为正则化是计算$j=1,2,3,\ldots,n$惩罚值的，要分开计算两种参数的惩罚值。 这里的$\theta_{j}(1-\alpha\frac{1}{m})&lt;1$，学习率很小，m是较大的数值，因此$\alpha\frac{1}{m}$是一个正数，假如该项值=0.99，那么$\theta*0.99$值会变得小一点，也就是在$\theta_{j}^2$的平方范数值变小了。 正规方程的正则化 设计一个$m*(n+1)$的矩阵X，X的每行代表一个训练样本，y矩阵为训练集的标签： \[\begin{equation*} X= \begin{bmatrix} (x^{(1)})^{T} \\ \ldots \\ (x^{(m)})^{T} \end{bmatrix} \qquad\qquad\qquad y= \begin{bmatrix} y^{(1)} \\ \ldots \\ y^{(m)} \end{bmatrix} \end{equation*}\] 为了最小化损失函数，$\mathop{min}\limits{\theta}J(\theta)$ 梯度下降算法 $\frac{\partial}{\partial \theta_{j}}J(\theta)$设各个参数为0进行偏导 $$ \begin{equation*} \theta=(x^{T}X+\lambda \begin{bmatrix} 0&amp;0&amp;0&amp;\ldots&amp;0 0&amp;1&amp;0&amp;\ldots&amp;0 0&amp;0&amp;1&amp;\ldots&amp;0 0&amp;0&amp;0&amp;\ldots&amp;0 0&amp;0&amp;0&amp;\ldots&amp;1 \end{bmatrix} )^{-1}X^{T}y \end{equation*} $$ 设n=2的话，上面公式的矩阵是3x3，因此矩阵是(n+1)x(n+1)，按公式去计算可以取得$J(\theta)$最小值。 正规方程不可逆 当训练样本$m\ \le$特征$n$时，X转置乘以X的矩阵是不可逆的，算法： \[\begin{equation*} \theta=(x^{T}X)^{-1}X^{T}y \end{equation*}\] 解决方案是减少$\theta_{j}$参数来解决的，正则化考虑到这点，只要正则化参数，严格要求$\lambda$大于0，一般可以解决不可逆的问题。 \[\begin{equation*} \theta=(x^{T}X+\lambda \begin{bmatrix} 0&amp;0&amp;0&amp;\ldots&amp;0 \\ 0&amp;1&amp;0&amp;\ldots&amp;0 \\ 0&amp;0&amp;1&amp;\ldots&amp;0 \\ 0&amp;0&amp;0&amp;\ldots&amp;0 \\ 0&amp;0&amp;0&amp;\ldots&amp;1 \end{bmatrix} )^{-1}X^{T}y \end{equation*}\] 逻辑回归的正则化 逻辑回归的函数有众多相关度较低的特征，损失函数为： \[\begin{equation*} -\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))] \end{equation*}\] 正则化的逻辑回归损失函数： \[\begin{equation*} -\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{m}\theta_{j} \end{equation*}\] 依旧是针对$\theta_{1},\theta_{2},\ldots,\theta_{n}$参数，关于参数的更新，还是分开对待的。 梯度下降算法 \[\begin{equation*} \theta_{j}:=\theta_{j}(1-\alpha\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\ \theta_{j}:=\theta_{j}(1-\alpha[\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}] \\ \end{equation*}\] 其中的$\frac{1}{m})-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}+\frac{\lambda}{m}\theta_{j}$是损失函数 关于更高级的优化算法使用正则化，假设有n个参数的n维矩阵： \[\begin{equation*} \theta= \begin{bmatrix} \theta_{0} \\ \theta_{1} \\ \theta_{2} \\ \ldots \\ \theta_{n} \end{bmatrix} \end{equation*}\] 优化算法 先计算所有样本的$J(\theta)$： \[\begin{equation*} -\frac{1}{m}[\displaystyle\sum\limits_{i=1}^{m}y^{(i)}log h_{\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\theta}(x))]+\frac{\lambda}{m}\theta_{j} \end{equation*}\] 然后计算$\frac{\partial}{\partial\theta_{0}}J(\theta)$对\theta_{0}$的偏导数： \(\begin{equation*} \frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{0}^{(i)}) \end{equation*}\) $\frac{\partial}{\partial\theta_{1}}J(\theta)$对于$\theta_{1}$的导数项： \(\begin{equation*} [\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{1}^{(i)})]+\frac{\lambda}{m}\theta_{1} \end{equation*}\) $\frac{\partial}{\partial\theta_{2}}J(\theta)$对于$\theta_{2}$导数项： \(\begin{equation*} [\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta}(x_{2}^{(i)})]+\frac{\lambda}{m}\theta_{2} \end{equation*}\) 后面]]></summary></entry><entry><title type="html">吴恩达机器学习笔记 Day 2</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/12/Andrew-Ng-DL-2.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 2" /><published>2022-02-12T00:00:00+00:00</published><updated>2022-02-12T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/12/Andrew-Ng-DL-2</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/12/Andrew-Ng-DL-2.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ 
$\displaystyle\underbrace{a_i}_{\text{i从1到n}}$

$\displaystyle\mathop{a_i}\limits_{i\text{从1到}n}$
-->
<h2 id="矩阵和向量">矩阵和向量</h2>

<p>矩阵是矩形数组的，由行数i和列数j构成</p>

<p>4*2矩阵，用数学符号表现为$R^{4*2}$</p>

\[A=
\begin{equation*}\begin{bmatrix}
1402&amp;191  \\
1372&amp;821  \\
949&amp;1437  \\
147&amp;1448
\end{bmatrix}\end{equation*}\]

<p>$A_{ij}$，那么$A_{11}=1402,A_{22}=821,A_{32}=1437$</p>

<p>2*3矩阵，用数学符号表现为$R^{2*3}$</p>

\[\\
\begin{equation*}\begin{bmatrix}
1&amp;2&amp;3  \\
4&amp;5&amp;6
\end{bmatrix}\end{equation*}\]

<p>由n*1的向量矩阵，$R^{4}$
\(y=
\begin{equation*}\begin{bmatrix}
460  \\
232  \\
315  \\
178  
\end{bmatrix}\end{equation*}\)</p>

<p>$y_{1}=460,y_{2}=232$</p>

<p>它是从下标1开始的，计算机中是从0开始的。矩阵通常以大写字母表示的，小写字母表现为标量向量的。</p>

<h3 id="矩阵计算">矩阵计算</h3>

<p>$h_{\theta}(x)=-40+0.25x$</p>

<p>那么矩阵计算过程为
\(\begin{bmatrix}
1&amp;2104  \\
1&amp;1416  \\
1&amp;1534  \\
1&amp;852
\end{bmatrix}
*
\begin{bmatrix}
-40  \\
0.25
\end{bmatrix}
=
\begin{bmatrix}
-40*1+0.25*2104  \\
-40*1+0.25*1416  \\
-40*1+0.25*1534  \\
-40*1+0.25*852
\end{bmatrix}\)</p>

<p>给定3个假定函数</p>
<ol>
  <li>$h_{\theta}(x)=-40+0.25x$</li>
  <li>$h_{\theta}(x)=200+0.1x$</li>
  <li>$h_{\theta}(x)=-150+0.4x$</li>
</ol>

\[\begin{equation*}
\begin{bmatrix}
1&amp;2104  \\
1&amp;1416  \\
1&amp;1534  \\
1&amp;852
\end{bmatrix}
*
\begin{bmatrix}
-40&amp;200&amp;-150  \\
0.25&amp;0.1&amp;0.4
\end{bmatrix}
=
\begin{bmatrix}
486&amp;410&amp;692  \\
314&amp;342&amp;416  \\
344&amp;353&amp;464  \\
173&amp;285&amp;191
\end{bmatrix}
\end{equation*}\]

<h3 id="矩阵特性">矩阵特性</h3>

<p>参考《线性代数》矩阵相关的公式、定义等</p>

<p>链接：<a href="https://github.com/allrobot/Study-Blog/raw/main/docs/about_data/线性代数.pdf">https://github.com/allrobot/Study-Blog/raw/main/docs/about_data/线性代数.pdf</a></p>

<table>
  <tbody>
    <tr>
      <td>AB未必与BA相等</td>
    </tr>
    <tr>
      <td>AX=AY 不能推出X=Y</td>
    </tr>
    <tr>
      <td>$(AB)^k$与$A^{k}B^{k}$不一定相等</td>
    </tr>
    <tr>
      <td>$A^{2}+(k+j)AB+kjB^{2}$ 与 $(A+kB)(A+jB)$ 不一定相等,但$A^{2}+(k+j)A+kjE=A^{2}+(k+j)AE+kjE^{2}=(A+kE)(A+jE)$</td>
    </tr>
    <tr>
      <td>$|\lambda A|=\lambda^{n}|A|$</td>
    </tr>
    <tr>
      <td>$|(AB)^{T}|=B^{T}A^{T}$</td>
    </tr>
    <tr>
      <td>$A*A^{1}=E\ or\ A^{1}*A=E$</td>
    </tr>
    <tr>
      <td>$A*A^{*}=|A|E\ or\ A^{*}*A=|A|E$</td>
    </tr>
  </tbody>
</table>

<table>
	<tr>
		<td>条件</td>
		<td colspan="4">解的情况</td>
	</tr>
	<tr>
		<td rowspan="2">齐次</td>
		<td>$R(A)=$未知数个数</td>
		
		<td colspan="3">唯一解（零解）</td>
		
	</tr>
	<tr>
		<td>$R(A)&lt;$未知数个数</td>
		<td colspan="3">多个解（零解和多个非零解）</td>
	</tr>
	<tr>
		<td rowspan="3">非齐次</td>
		<td>$R(A)\ne R(A|b)$</td>
		<td colspan="3">无解</td>
	</tr>
	<tr>
		<td rowspan="2">$R(A)=R(A|b)$</td>
		<td rowspan="2">有解</td>
		<td>$R(A)=R(A|b)=$未知数个数</td>
		<td>一个非零解</td>
	</tr>
	<tr>
		<td>$R(A)=R(A|b)&lt;$未知数个数</td>
		<td>多个非零解</td>
	</tr>
</table>

<h2 id="多种特征">多种特征</h2>

<p>特征向量，以房价为例</p>

<table>
  <thead>
    <tr>
      <th>面积$m^2$</th>
      <th>卧室数量</th>
      <th>门数量</th>
      <th>房龄</th>
      <th>价格</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2104</td>
      <td>5</td>
      <td>1</td>
      <td>45</td>
      <td>460</td>
    </tr>
    <tr>
      <td>1416</td>
      <td>3</td>
      <td>2</td>
      <td>40</td>
      <td>232</td>
    </tr>
    <tr>
      <td>1534</td>
      <td>3</td>
      <td>2</td>
      <td>30</td>
      <td>315</td>
    </tr>
    <tr>
      <td>852</td>
      <td>2</td>
      <td>1</td>
      <td>36</td>
      <td>178</td>
    </tr>
    <tr>
      <td>$\ldots$</td>
      <td>$\ldots$</td>
      <td>$\ldots$</td>
      <td>$\ldots$</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<ul>
  <li>n=特征数量</li>
</ul>

\[\begin{equation*}
n=4
\end{equation*}\]

<ul>
  <li>$x^{(i)}$=$i^{th}$训练样本的特征向量</li>
</ul>

\[\begin{equation*}x^{2}=
\begin{bmatrix}
1416  \\
3  \\
2  \\
40
\end{bmatrix}
\end{equation*}
\epsilon R^{4}\]

<ul>
  <li>$x_{j}^{(i)}$=$i^{th}$训练样本的第j个特征量的值</li>
</ul>

\[\begin{equation*}
x_{3}^{(2)}=2
\end{equation*}\]

<p>线性回归的假定函数$h_{\theta}=\theta_{0}+\theta_{1}x$不适用此情况，此时的函数模型应为：</p>

\[\begin{equation*}
h_{\theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}-\theta_{4}x_{4}
\end{equation*}\]

<p>意味着后续特征增加时，模型可应为:</p>

\[\begin{equation*}
h_{\theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{1}x_{2}+\ldots+\theta_{n}x_{n}  \\
x=
\begin{bmatrix}
x_{0}  \\
x_{1}  \\
x_{2}  \\
\ldots  \\
x_{n}
\end{bmatrix}\ 
\epsilon\  R^{n+1}
\qquad
\theta=
\begin{bmatrix}
R_{0}  \\
R_{1}  \\
R_{2}  \\
\ldots  \\
R_{n}
\end{bmatrix}
\epsilon\  R^{n+1}  \\
  \begin{split}
    h_{\theta}
    &amp; = \theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{1}x_{2}+\ldots-\theta_{n}x_{n}   \\
	&amp; = \theta_{x}^{T}  \\
  \end{split}  \\
  
\text{其中}  \\

\begin{bmatrix}
\theta_{0}x_{0}&amp;\theta_{1}x_{1}&amp;\ldots&amp;\theta_{n}x_{n}  \\
\end{bmatrix}

\begin{bmatrix}
x_{0}  \\
x_{1}  \\
\ldots  \\
x_{n} 
\end{bmatrix}

\end{equation*}\]

<blockquote>
  <p>为了方便，$\theta_{0}x{0}$的$x{0}=1$,故省去</p>
</blockquote>

<h2 id="多种参数的梯度下降算法">多种参数的梯度下降算法</h2>

<p>之前最简函数$h_{\theta}=\theta_{0}+\theta_{1}x$的$\theta_{0}$，给出的梯度下降算法：</p>

<p>$$
\begin{equation*}
\theta_{0}:=\theta_{0}-\alpha\underbrace{\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}}<em>{\frac{\partial}{\partial \theta</em>{0}}J(\theta)}  \</p>

<p>\end{equation<em>}
\(微分推导后：\)
\begin{equation</em>}</p>

<p>\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x^{(i)}
\end{equation*}
$$</p>

<p>意味着其它参数可同样推导：</p>

\[\begin{equation*}
\theta_{j}:=\theta_{j}-\alpha\underbrace{\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x^{(i)}}_{\text{同时更新所有}\theta_{j}}  \\

text{类推 }  \\
\theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{0}^{(i)}  \\

\theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{1}^{(i)}  \\

\theta_{2}:=\theta_{2}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{2}^{(i)}  \\

\ldots

\end{equation*}\]

<p>以后的多元梯度下降算法用到类似的更新规则。</p>

<h2 id="梯度下降训练特征缩放">梯度下降训练：特征缩放</h2>

<p>假定$x_{1}$=面积$(0$~$2000/m^2),x_{2}=$卧室数量$(1$~$5)$，呈现的等高线图比较瘦长，梯度下降的点会反复来回震荡很长时间才能抵达全局最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/1.png" alt="" /></p>

<p>解决办法是<strong>特征缩放</strong>把两个x值设为$0\le x_{1}\le 1, \  0\le x_{2}\le 1$，椭圆等高线图呈现比较扁平，偏移没那么严重。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/2.png" alt="" /></p>

<p>一般特征缩放后的值范围为$-1\le x_{i} \le 1$</p>

<p><code class="language-plaintext success highlighter-rouge">正确</code>
$0\le x_{1} \le 3 \qquad -2\le x_{2} \le 0.5  \ \qquad \qquad
-3\ to\ 3 \qquad -\frac{1}{3}\ to\ \frac{1}{3}$</p>

<p><code class="language-plaintext error highlighter-rouge">错误</code>
$-100\le x_{3}\le 100 \qquad -0.0001\le x_{4}\le 0.0001$</p>

<p>只有这样，梯度下降算法才会<strong>正常工作</strong>！</p>

<h2 id="均值归一化">均值归一化</h2>

<p>用$x_{i}$替换$x_{i}-u_{i}$使特征的均值近似为零(不使$x_{0}=1$)</p>

<p>假若房子面积平均值为1000，每套房子的卧室平均数量为2</p>

\[\begin{equation*}
x_{1}=\frac{\text{面积}-1000}{2000}  \\

x_{2}=\frac{\text{卧室数量}-2}{5}  \\

-0.5\le x_{1}\le 0.5,-0.5\le x_{2}\le 0.5

\end{equation*}\]

<p>取值范围可以为-1~5一切是为了梯度下降的速度更快。</p>

\[x_{i}=\frac{x_{1}-u_{1}}{s_{1}}\]

<h2 id="梯度下降训练学习率">梯度下降训练：学习率</h2>

<p>学习率选择比较重要，如果选大了，也许梯度下降只要30步就完成了；选小了，梯度也可能要3000，甚至3000000步才能抵达全局最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/3.png" alt="" /></p>

<p>如何判断梯度下降曲线，从而选择合适的学习率。当判断梯度下降的时候发现$J(\theta)$图正上升，这里学习率可能选大了，应选小，不然会发生梯度爆炸的情况。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/4.png" alt="" /></p>

<p>也有$J(\theta)$显示波浪线，这里暂不讨论，上述学习率的选择适用于线性回归问题。</p>

<p>有一个解决方案比较适合：</p>

\[\ldots,0.001,\ldots,0.01,\ldots,0.1,\ldots,1,\ldots\]

<p>每迭代10~100轮，学习率0.001递增3倍，若不影响梯度下降就升为0.03，再递增…出现梯度上升的情况则反之。</p>

\[\ldots,0.001,0.003,0.01,\ldots,0.1,\ldots,1,\ldots\]

<h2 id="多变量的线性回归">多变量的线性回归</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-12/5.png" alt="" /></p>

<p>要设计函数模型输出房子的售价，要考虑提取哪些特征，上图有2个特征，房子正面和纵深的长度乘号得面积，模型也许二次项$h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$能很好预测价格，但随着房子面积增加且价格下降，这就不能很好拟合数据集了，改成三次方$h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3}$</p>

<p>\(\begin{equation*}
  \begin{split}
    h_{\theta}(x)
	&amp; = h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3} \\
	&amp; = h_{\theta}=\theta_{0}+\theta_{1}\text{(面积)}+\theta_{2}\text{(面积)}^{2}+\theta_{3}\text{(面积)}^{3} \\
  \end{split}
\end{equation*}\)
\(\begin{equation*}
  x_{1}=\text{(面积)}  \qquad \qquad \text{(面积:1~1000)}\\
  x_{2}=\text{(面积)}^2\qquad \qquad \text{(面积:1~10^5)} \\
  x_{3}=\text{(面积)}^3\qquad \qquad \text{(面积:1~10^9)} 
\end{equation*}\)</p>

<p>三个参数的值较大，采用特征缩放为宜。</p>

<p>除此之外，当房子面积增加时二次项函数不太拟合数据集，除了改至三次项，第二个参数可以采用平方根函数：</p>

\[\begin{equation*}
h_{\theta}=\theta_{0}+\theta_{1}\text{(面积)}+\theta_{2}\sqrt{\text{(面积)}}
\end{equation*}\]

<p>特征的选择需要多种合适的角度去看待。</p>

<h2 id="梯度下降算法正规方程">梯度下降算法：正规方程</h2>

<p>正规方程法相比batch梯度下降一次次迭代获取全局最优解，它能<strong>一次性</strong>直接获取$\theta$值。batch梯度下降算法，需要分别去计算$J(\theta)$对各个特征的偏导数$\frac{\partial}{\partial\theta_{j}}J(\theta)$，步骤相当多，推导又麻烦</p>

<p>正规方程法的训练步骤：训练样本m为4个</p>

<table>
  <thead>
    <tr>
      <th>面积$x_{1}$</th>
      <th>卧室数量$x_{2}$</th>
      <th>门数量$x_{3}$</th>
      <th>房龄$x_{4}$</th>
      <th>价格$y$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2104</td>
      <td>5</td>
      <td>1</td>
      <td>45</td>
      <td>460</td>
    </tr>
    <tr>
      <td>1416</td>
      <td>3</td>
      <td>2</td>
      <td>40</td>
      <td>232</td>
    </tr>
    <tr>
      <td>1534</td>
      <td>3</td>
      <td>2</td>
      <td>30</td>
      <td>315</td>
    </tr>
    <tr>
      <td>852</td>
      <td>2</td>
      <td>1</td>
      <td>36</td>
      <td>178</td>
    </tr>
  </tbody>
</table>

<p>n+1维特征向量
\(\begin{equation*}
  X=
  \begin{bmatrix}
    1&amp;2104&amp;5&amp;1&amp;45&amp;460  \\
    1&amp;1416&amp;3&amp;2&amp;40&amp;232  \\
    1&amp;1534&amp;3&amp;2&amp;30&amp;315  \\
    1&amp;852&amp;2&amp;1&amp;36&amp;178
  \end{bmatrix}
  \qquad
  y=
    \begin{bmatrix}
    460  \\
    232  \\
    315  \\
    178
  \end{bmatrix}
\end{equation*}\)</p>

<p>$m*(n+1)$维矩阵，正规方程法如下</p>

\[\begin{equation*}
  \theta=(X^{T}X)^{-1}X^{T}y
\end{equation*}\]

<p>举个例子，训练样本共m个，它的数据集是:$(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\ldots,(x^{(n)},y^{(n)})$，有n个特征。</p>

<p>特征矩阵，有n+1个特征向量
\(\begin{equation*}
  x^{(i)}=
  \begin{bmatrix}
    x_{0}^{(i)}  \\
    x_{1}^{(i)}  \\
    \ldots     \\
    x_{n}^{(i)}
  \end{bmatrix}
\end{equation*}\)</p>

<p>然后计算X的转置
\(\begin{equation*}
  X=
  \begin{bmatrix}
    ——(x^{(1)})^{T}——  \\
    ——(x^{(2)})^{T}——  \\
    \ldots  \\
    ——(x^{(m)})^{T}——
  \end{bmatrix}
\end{equation*}\)</p>

<p>举一个具体的例子：
假如特征矩阵有2个特征向量
\(\begin{equation*}
  x^{(i)}=\begin{bmatrix}
    1  \\
	x_{1}^{(i)}
  \end{bmatrix}
\end{equation*}\)</p>

\[\begin{equation*}
  X=\begin{bmatrix}
    1&amp;x_{1}^{(1)}  \\
	1&amp;x_{2}^{(1)}  \\
	\ldots  \\
	1&amp;x_{m}^{(1)}  \\
	
  \end{bmatrix}
  y=\begin{bmatrix}
    y^{(1)}  \\
	y^{(2)}  \\
	\ldots   \\
	y^{(m)}  \\
  \end{bmatrix}
\end{equation*}\]

<p>这个X矩阵构建后，计算$\theta=(X^{T}X)^{-1}X^{T}y$能一次性训练出$x_{1}$的$\theta$值</p>

<p><strong>正规方程法</strong></p>
<ul>
  <li>不需要学习率</li>
  <li>不需要迭代</li>
  <li>需要计算$(X^{T}X)^{-1}$</li>
  <li>如果n个特征向量的数很大，计算缓慢</li>
</ul>

<p><strong>批量梯度下降算法</strong></p>
<ul>
  <li>需要学习率</li>
  <li>需要迭代</li>
  <li>n个特征向量很大，能很好工作</li>
</ul>

<p>正规方程法适用计算1万以下的特征向量，1万以上的适用批量梯度下降算法。</p>

<h3 id="正规方程和不可逆转性">正规方程和不可逆转性</h3>

<p>如果出现$X^{T}X$矩阵不可逆，可以考虑特征矩阵删除一些多余的特征，使之可逆。</p>

<h2 id="例子代码">例子代码</h2>

<h3 id="数据集链接">数据集链接</h3>

<p><a href="https://github.com/Yongsgithub/picturebed/tree/Yongsgithub-NG-Machine-Learning">https://github.com/Yongsgithub/picturebed/tree/Yongsgithub-NG-Machine-Learning</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">'D:</span><span class="se">\\</span><span class="s">note</span><span class="se">\\</span><span class="s">ex1-linear regression</span><span class="se">\\</span><span class="s">ex1data1.txt'</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s">'横坐标'</span><span class="p">,</span> <span class="s">'纵坐标'</span><span class="p">])</span>

<span class="c1"># # 输出散点图
# sns.lmplot('横坐标', '纵坐标', data=df, size=6, fit_reg=True)
# # fit_reg:拟合回归参数,如果fit_reg=True则散点图中则出现拟合直线
# plt.show()
</span>

<span class="k">def</span> <span class="nf">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">):</span>
    <span class="c1"># 矩阵相乘，正好((x1 * theta_1 + x2 * theta_2)-2)^2
</span>    <span class="n">inner</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">power</span><span class="p">((</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

    <span class="c1"># theta.T就是矩阵theta的转置矩阵
</span>    <span class="c1"># np.power(A,B)   ## 对A中的每个元素求B次方
</span>    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">inner</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>


<span class="n">df</span><span class="p">.</span><span class="n">insert</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="s">'ONE'</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">cols</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># 获取表格df的列数
</span><span class="n">X</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="n">cols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># 除最后一列外，取其他列的所有行，即X为O和人口组成的列表
</span><span class="n">y</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">iloc</span><span class="p">[:,</span> <span class="n">cols</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span><span class="n">cols</span><span class="p">]</span>  <span class="c1"># 取最后一列的所有行，即y为利润
</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">y</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>

<span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iters</span><span class="p">):</span>  <span class="c1"># alpha是学习率,iters为迭代次数
</span>    <span class="n">temp</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">matrix</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">theta</span><span class="p">.</span><span class="n">shape</span><span class="p">))</span>  <span class="c1"># np.zeros(theta.shape)=[0.,0.],然后将temp变为矩阵[0.,0.]
</span>    <span class="n">parameters</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">theta</span><span class="p">.</span><span class="n">ravel</span><span class="p">().</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
    <span class="c1"># theta.ravel()：将多维数组theta降为一维，.shape[1]是统计这个一维数组有多少个元
</span>    <span class="c1"># parameters表示参数
</span>    <span class="n">cost</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">iters</span><span class="p">)</span>  <span class="c1"># 初始化代价函数值为0数组，元素个数为迭代次数
</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">):</span>  <span class="c1"># 循环iters次
</span>        <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">theta</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span>

        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">parameters</span><span class="p">):</span>
            <span class="n">term</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">multiply</span><span class="p">(</span><span class="n">error</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">j</span><span class="p">])</span>  <span class="c1"># 将误差与训练数据相乘，term为偏导数，参考笔记P27
</span>            <span class="n">temp</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">-</span> <span class="p">((</span><span class="n">alpha</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">term</span><span class="p">))</span>  <span class="c1"># 更新theta
</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">temp</span>
        <span class="n">cost</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">computeCost</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">)</span>  <span class="c1"># 计算每一次的代价函数
</span>
    <span class="k">return</span> <span class="n">theta</span><span class="p">,</span> <span class="n">cost</span>


<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">iters</span> <span class="o">=</span> <span class="mi">2000</span>

<span class="n">g</span><span class="p">,</span> <span class="n">cost</span> <span class="o">=</span> <span class="n">gradientDescent</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">theta</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">iters</span><span class="p">)</span>  <span class="c1"># 令g和cost分别等于函数的两个返回值
</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">横坐标</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">df</span><span class="p">.</span><span class="n">横坐标</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span> <span class="mi">100</span><span class="p">)</span>  <span class="c1"># 以人口最小值为起点，最大值为终点，创建元素个数为100的等差数列
</span><span class="n">f</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>  <span class="c1"># f是假设函数H
</span>
<span class="c1"># 输出数据集的拟合情况
</span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">横坐标</span><span class="p">.</span><span class="nb">min</span><span class="p">(),</span><span class="n">df</span><span class="p">.</span><span class="n">横坐标</span><span class="p">.</span><span class="nb">max</span><span class="p">(),</span><span class="mi">100</span><span class="p">)</span><span class="c1">#以人口最小值为起点，最大值为终点，创建元素个数为100的等差数列
</span><span class="n">f</span> <span class="o">=</span> <span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="p">(</span><span class="n">g</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="c1">#f是假设函数H
</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">8</span><span class="p">))</span><span class="c1">#以其他关键字参数**fig_kw来创建图
#figsize=(a,b):figsize 设置图形的大小,b为图形的宽,b为图形的高,单位为英寸
</span><span class="n">ax</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Prediction'</span><span class="p">)</span>   <span class="c1">#设置点的横坐标，纵坐标，用红色线，并且设置Prediction为关键字参数
</span><span class="n">ax</span><span class="p">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">df</span><span class="p">.</span><span class="n">横坐标</span><span class="p">,</span> <span class="n">df</span><span class="p">.</span><span class="n">纵坐标</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">'Traning Data'</span><span class="p">)</span>  <span class="c1">#以人口为横坐标，利润为纵坐标并且设置Traning Data为关键字参数
</span><span class="n">ax</span><span class="p">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>  <span class="c1">#legend为显示图例函数，loc为设置图例显示的位置，loc=2即在左上方
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">'Population'</span><span class="p">)</span>  <span class="c1">#设置x轴变量
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">'Profit'</span><span class="p">)</span>  <span class="c1">#设置x轴变量
</span><span class="n">ax</span><span class="p">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">'Predicted Profit vs. Population Size'</span><span class="p">)</span> <span class="c1">#设置表头
</span><span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># 代价数据可视化1
</span><span class="n">c</span><span class="o">=</span><span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'inters'</span><span class="p">:</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">iters</span><span class="p">)),</span><span class="s">'cost'</span><span class="p">:</span><span class="n">cost</span>  <span class="p">})</span>
<span class="n">ax</span><span class="o">=</span><span class="n">sns</span><span class="p">.</span><span class="n">lmplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'inters'</span><span class="p">,</span><span class="n">y</span><span class="o">=</span><span class="s">'cost'</span><span class="p">,</span><span class="n">data</span><span class="o">=</span><span class="n">c</span><span class="p">,</span><span class="n">fit_reg</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>


<span class="c1"># # 代价数据可视化2
# fig, ax = plt.subplots(figsize=(12,8)) #以其他关键字参数**fig_kw来创建图
# #figsize=(a,b):figsize 设置图形的大小,b为图形的宽,b为图形的高,单位为英寸
# ax.plot(np.arange(iters), cost, 'b') #作图:以迭代次数为x，代价函数值为y,线条颜色为红色
# ax.set_xlabel('Iterations')  #设置x轴变量
# ax.set_ylabel('Cost')  #设置y轴变量
# ax.set_title('Error vs. Training Epoch') #设置表头
# plt.show()
</span>
</code></pre></div></div>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[矩阵和向量 矩阵是矩形数组的，由行数i和列数j构成 4*2矩阵，用数学符号表现为$R^{4*2}$ \[A= \begin{equation*}\begin{bmatrix} 1402&amp;191 \\ 1372&amp;821 \\ 949&amp;1437 \\ 147&amp;1448 \end{bmatrix}\end{equation*}\] $A_{ij}$，那么$A_{11}=1402,A_{22}=821,A_{32}=1437$ 2*3矩阵，用数学符号表现为$R^{2*3}$ \[\\ \begin{equation*}\begin{bmatrix} 1&amp;2&amp;3 \\ 4&amp;5&amp;6 \end{bmatrix}\end{equation*}\] 由n*1的向量矩阵，$R^{4}$ \(y= \begin{equation*}\begin{bmatrix} 460 \\ 232 \\ 315 \\ 178 \end{bmatrix}\end{equation*}\) $y_{1}=460,y_{2}=232$ 它是从下标1开始的，计算机中是从0开始的。矩阵通常以大写字母表示的，小写字母表现为标量向量的。 矩阵计算 $h_{\theta}(x)=-40+0.25x$ 那么矩阵计算过程为 \(\begin{bmatrix} 1&amp;2104 \\ 1&amp;1416 \\ 1&amp;1534 \\ 1&amp;852 \end{bmatrix} * \begin{bmatrix} -40 \\ 0.25 \end{bmatrix} = \begin{bmatrix} -40*1+0.25*2104 \\ -40*1+0.25*1416 \\ -40*1+0.25*1534 \\ -40*1+0.25*852 \end{bmatrix}\) 给定3个假定函数 $h_{\theta}(x)=-40+0.25x$ $h_{\theta}(x)=200+0.1x$ $h_{\theta}(x)=-150+0.4x$ \[\begin{equation*} \begin{bmatrix} 1&amp;2104 \\ 1&amp;1416 \\ 1&amp;1534 \\ 1&amp;852 \end{bmatrix} * \begin{bmatrix} -40&amp;200&amp;-150 \\ 0.25&amp;0.1&amp;0.4 \end{bmatrix} = \begin{bmatrix} 486&amp;410&amp;692 \\ 314&amp;342&amp;416 \\ 344&amp;353&amp;464 \\ 173&amp;285&amp;191 \end{bmatrix} \end{equation*}\] 矩阵特性 参考《线性代数》矩阵相关的公式、定义等 链接：https://github.com/allrobot/Study-Blog/raw/main/docs/about_data/线性代数.pdf AB未必与BA相等 AX=AY 不能推出X=Y $(AB)^k$与$A^{k}B^{k}$不一定相等 $A^{2}+(k+j)AB+kjB^{2}$ 与 $(A+kB)(A+jB)$ 不一定相等,但$A^{2}+(k+j)A+kjE=A^{2}+(k+j)AE+kjE^{2}=(A+kE)(A+jE)$ $|\lambda A|=\lambda^{n}|A|$ $|(AB)^{T}|=B^{T}A^{T}$ $A*A^{1}=E\ or\ A^{1}*A=E$ $A*A^{*}=|A|E\ or\ A^{*}*A=|A|E$ 条件 解的情况 齐次 $R(A)=$未知数个数 唯一解（零解） $R(A)&lt;$未知数个数 多个解（零解和多个非零解） 非齐次 $R(A)\ne R(A|b)$ 无解 $R(A)=R(A|b)$ 有解 $R(A)=R(A|b)=$未知数个数 一个非零解 $R(A)=R(A|b)&lt;$未知数个数 多个非零解 多种特征 特征向量，以房价为例 面积$m^2$ 卧室数量 门数量 房龄 价格 2104 5 1 45 460 1416 3 2 40 232 1534 3 2 30 315 852 2 1 36 178 $\ldots$ $\ldots$ $\ldots$ $\ldots$   n=特征数量 \[\begin{equation*} n=4 \end{equation*}\] $x^{(i)}$=$i^{th}$训练样本的特征向量 \[\begin{equation*}x^{2}= \begin{bmatrix} 1416 \\ 3 \\ 2 \\ 40 \end{bmatrix} \end{equation*} \epsilon R^{4}\] $x_{j}^{(i)}$=$i^{th}$训练样本的第j个特征量的值 \[\begin{equation*} x_{3}^{(2)}=2 \end{equation*}\] 线性回归的假定函数$h_{\theta}=\theta_{0}+\theta_{1}x$不适用此情况，此时的函数模型应为： \[\begin{equation*} h_{\theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{2}x_{2}+\theta_{3}x_{3}-\theta_{4}x_{4} \end{equation*}\] 意味着后续特征增加时，模型可应为: \[\begin{equation*} h_{\theta}=\theta_{0}+\theta_{1}x_{1}+\theta_{1}x_{2}+\ldots+\theta_{n}x_{n} \\ x= \begin{bmatrix} x_{0} \\ x_{1} \\ x_{2} \\ \ldots \\ x_{n} \end{bmatrix}\ \epsilon\ R^{n+1} \qquad \theta= \begin{bmatrix} R_{0} \\ R_{1} \\ R_{2} \\ \ldots \\ R_{n} \end{bmatrix} \epsilon\ R^{n+1} \\ \begin{split} h_{\theta} &amp; = \theta_{0}x_{0}+\theta_{1}x_{1}+\theta_{1}x_{2}+\ldots-\theta_{n}x_{n} \\ &amp; = \theta_{x}^{T} \\ \end{split} \\ \text{其中} \\ \begin{bmatrix} \theta_{0}x_{0}&amp;\theta_{1}x_{1}&amp;\ldots&amp;\theta_{n}x_{n} \\ \end{bmatrix} \begin{bmatrix} x_{0} \\ x_{1} \\ \ldots \\ x_{n} \end{bmatrix} \end{equation*}\] 为了方便，$\theta_{0}x{0}$的$x{0}=1$,故省去 多种参数的梯度下降算法 之前最简函数$h_{\theta}=\theta_{0}+\theta_{1}x$的$\theta_{0}$，给出的梯度下降算法： $$ \begin{equation*} \theta_{0}:=\theta_{0}-\alpha\underbrace{\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}}{\frac{\partial}{\partial \theta{0}}J(\theta)} \ \end{equation} \(微分推导后：\) \begin{equation} \theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x^{(i)} \end{equation*} $$ 意味着其它参数可同样推导： \[\begin{equation*} \theta_{j}:=\theta_{j}-\alpha\underbrace{\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x^{(i)}}_{\text{同时更新所有}\theta_{j}} \\ text{类推 } \\ \theta_{0}:=\theta_{0}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{0}^{(i)} \\ \theta_{1}:=\theta_{1}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{1}^{(i)} \\ \theta_{2}:=\theta_{2}-\alpha\frac{1}{m}\sum\limits_{i=1}^m (h_{\theta_{(i)}-y^{(i)})}x_{2}^{(i)} \\ \ldots \end{equation*}\] 以后的多元梯度下降算法用到类似的更新规则。 梯度下降训练：特征缩放 假定$x_{1}$=面积$(0$~$2000/m^2),x_{2}=$卧室数量$(1$~$5)$，呈现的等高线图比较瘦长，梯度下降的点会反复来回震荡很长时间才能抵达全局最优解。 解决办法是特征缩放把两个x值设为$0\le x_{1}\le 1, \ 0\le x_{2}\le 1$，椭圆等高线图呈现比较扁平，偏移没那么严重。 一般特征缩放后的值范围为$-1\le x_{i} \le 1$ 正确 $0\le x_{1} \le 3 \qquad -2\le x_{2} \le 0.5 \ \qquad \qquad -3\ to\ 3 \qquad -\frac{1}{3}\ to\ \frac{1}{3}$ 错误 $-100\le x_{3}\le 100 \qquad -0.0001\le x_{4}\le 0.0001$ 只有这样，梯度下降算法才会正常工作！ 均值归一化 用$x_{i}$替换$x_{i}-u_{i}$使特征的均值近似为零(不使$x_{0}=1$) 假若房子面积平均值为1000，每套房子的卧室平均数量为2 \[\begin{equation*} x_{1}=\frac{\text{面积}-1000}{2000} \\ x_{2}=\frac{\text{卧室数量}-2}{5} \\ -0.5\le x_{1}\le 0.5,-0.5\le x_{2}\le 0.5 \end{equation*}\] 取值范围可以为-1~5一切是为了梯度下降的速度更快。 \[x_{i}=\frac{x_{1}-u_{1}}{s_{1}}\] 梯度下降训练：学习率 学习率选择比较重要，如果选大了，也许梯度下降只要30步就完成了；选小了，梯度也可能要3000，甚至3000000步才能抵达全局最优解。 如何判断梯度下降曲线，从而选择合适的学习率。当判断梯度下降的时候发现$J(\theta)$图正上升，这里学习率可能选大了，应选小，不然会发生梯度爆炸的情况。 也有$J(\theta)$显示波浪线，这里暂不讨论，上述学习率的选择适用于线性回归问题。 有一个解决方案比较适合： \[\ldots,0.001,\ldots,0.01,\ldots,0.1,\ldots,1,\ldots\] 每迭代10~100轮，学习率0.001递增3倍，若不影响梯度下降就升为0.03，再递增…出现梯度上升的情况则反之。 \[\ldots,0.001,0.003,0.01,\ldots,0.1,\ldots,1,\ldots\] 多变量的线性回归 要设计函数模型输出房子的售价，要考虑提取哪些特征，上图有2个特征，房子正面和纵深的长度乘号得面积，模型也许二次项$h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}$能很好预测价格，但随着房子面积增加且价格下降，这就不能很好拟合数据集了，改成三次方$h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3}$ \(\begin{equation*} \begin{split} h_{\theta}(x) &amp; = h_{\theta}=\theta_{0}+\theta_{1}x+\theta_{2}x^{2}+\theta_{3}x^{3} \\ &amp; = h_{\theta}=\theta_{0}+\theta_{1}\text{(面积)}+\theta_{2}\text{(面积)}^{2}+\theta_{3}\text{(面积)}^{3} \\ \end{split} \end{equation*}\) \(\begin{equation*} x_{1}=\text{(面积)} \qquad \qquad \text{(面积:1~1000)}\\ x_{2}=\text{(面积)}^2\qquad \qquad \text{(面积:1~10^5)} \\ x_{3}=\text{(面积)}^3\qquad \qquad \text{(面积:1~10^9)} \end{equation*}\) 三个参数的值较大，采用特征缩放为宜。 除此之外，当房子面积增加时二次项函数不太拟合数据集，除了改至三次项，第二个参数可以采用平方根函数： \[\begin{equation*} h_{\theta}=\theta_{0}+\theta_{1}\text{(面积)}+\theta_{2}\sqrt{\text{(面积)}} \end{equation*}\] 特征的选择需要多种合适的角度去看待。 梯度下降算法：正规方程 正规方程法相比batch梯度下降一次次迭代获取全局最优解，它能一次性直接获取$\theta$值。batch梯度下降算法，需要分别去计算$J(\theta)$对各个特征的偏导数$\frac{\partial}{\partial\theta_{j}}J(\theta)$，步骤相当多，推导又麻烦 正规方程法的训练步骤：训练样本m为4个 面积$x_{1}$ 卧室数量$x_{2}$ 门数量$x_{3}$ 房龄$x_{4}$ 价格$y$ 2104 5 1 45 460 1416 3 2 40 232 1534 3 2 30 315 852 2 1 36 178 n+1维特征向量 \(\begin{equation*} X= \begin{bmatrix} 1&amp;2104&amp;5&amp;1&amp;45&amp;460 \\ 1&amp;1416&amp;3&amp;2&amp;40&amp;232 \\ 1&amp;1534&amp;3&amp;2&amp;30&amp;315 \\ 1&amp;852&amp;2&amp;1&amp;36&amp;178 \end{bmatrix} \qquad y= \begin{bmatrix} 460 \\ 232 \\ 315 \\ 178 \end{bmatrix} \end{equation*}\) $m*(n+1)$维矩阵，正规方程法如下 \[\begin{equation*} \theta=(X^{T}X)^{-1}X^{T}y \end{equation*}\] 举个例子，训练样本共m个，它的数据集是:$(x^{(1)},y^{(1)}),x^{(2)},y^{(2)}),\ldots,(x^{(n)},y^{(n)})$，有n个特征。 特征矩阵，有n+1个特征向量 \(\begin{equation*} x^{(i)}= \begin{bmatrix} x_{0}^{(i)} \\ x_{1}^{(i)} \\ \ldots \\ x_{n}^{(i)} \end{bmatrix} \end{equation*}\) 然后计算X的转置 \(\begin{equation*} X= \begin{bmatrix} ——(x^{(1)})^{T}—— \\ ——(x^{(2)})^{T}—— \\ \ldots \\ ——(x^{(m)})^{T}—— \end{bmatrix} \end{equation*}\) 举一个具体的例子： 假如特征矩阵有2个特征向量 \(\begin{equation*} x^{(i)}=\begin{bmatrix} 1 \\ x_{1}^{(i)} \end{bmatrix} \end{equation*}\) \[\begin{equation*} X=\begin{bmatrix} 1&amp;x_{1}^{(1)} \\ 1&amp;x_{2}^{(1)} \\ \ldots \\ 1&amp;x_{m}^{(1)} \\ \end{bmatrix} y=\begin{bmatrix} y^{(1)} \\ y^{(2)} \\ \ldots \\ y^{(m)} \\ \end{bmatrix} \end{equation*}\] 这个X矩阵构建后，计算$\theta=(X^{T}X)^{-1}X^{T}y$能一次性训练出$x_{1}$的$\theta$值 正规方程法 不需要学习率 不需要迭代 需要计算$(X^{T}X)^{-1}$ 如果n个特征向量的数很大，计算缓慢 批量梯度下降算法 需要学习率 需要迭代 n个特征向量很大，能很好工作 正规方程法适用计算1万以下的特征向量，1万以上的适用批量梯度下降算法。 正规方程和不可逆转性 如果出现$X^{T}X$矩阵不可逆，可以考虑特征矩阵删除一些多余的特征，使之可逆。 例子代码 数据集链接 https://github.com/Yongsgithub/picturebed/tree/Yongsgithub-NG-Machine-Learning import pandas as pd import seaborn as sns import numpy as np from matplotlib import pyplot as plt df = pd.read_csv('D:\\note\\ex1-linear regression\\ex1data1.txt', names=['横坐标', '纵坐标']) # # 输出散点图 # sns.lmplot('横坐标', '纵坐标', data=df, size=6, fit_reg=True) # # fit_reg:拟合回归参数,如果fit_reg=True则散点图中则出现拟合直线 # plt.show() def computeCost(X, y, theta): # 矩阵相乘，正好((x1 * theta_1 + x2 * theta_2)-2)^2 inner = np.power((X * theta.T) - y, 2) # theta.T就是矩阵theta的转置矩阵 # np.power(A,B) ## 对A中的每个元素求B次方 return np.sum(inner) / (2 * len(X)) df.insert(0, 'ONE', 1) cols = df.shape[1] # 获取表格df的列数 X = df.iloc[:, 0:cols - 1] # 除最后一列外，取其他列的所有行，即X为O和人口组成的列表 y = df.iloc[:, cols - 1:cols] # 取最后一列的所有行，即y为利润 X = np.matrix(X.values) y = np.matrix(y.values) theta = np.matrix(np.array([0, 0])) computeCost(X, y, theta) def gradientDescent(X, y, theta, alpha, iters): # alpha是学习率,iters为迭代次数 temp = np.matrix(np.zeros(theta.shape)) # np.zeros(theta.shape)=[0.,0.],然后将temp变为矩阵[0.,0.] parameters = int(theta.ravel().shape[1]) # theta.ravel()：将多维数组theta降为一维，.shape[1]是统计这个一维数组有多少个元 # parameters表示参数 cost = np.zeros(iters) # 初始化代价函数值为0数组，元素个数为迭代次数 for i in range(iters): # 循环iters次 error = (X * theta.T) - y for j in range(parameters): term = np.multiply(error, X[:, j]) # 将误差与训练数据相乘，term为偏导数，参考笔记P27 temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term)) # 更新theta theta = temp cost[i] = computeCost(X, y, theta) # 计算每一次的代价函数 return theta, cost alpha = 0.01 iters = 2000 g, cost = gradientDescent(X, y, theta, alpha, iters) # 令g和cost分别等于函数的两个返回值 x = np.linspace(df.横坐标.min(), df.横坐标.max(), 100) # 以人口最小值为起点，最大值为终点，创建元素个数为100的等差数列 f = g[0, 0] + (g[0, 1] * x) # f是假设函数H # 输出数据集的拟合情况 x = np.linspace(df.横坐标.min(),df.横坐标.max(),100)#以人口最小值为起点，最大值为终点，创建元素个数为100的等差数列 f = g[0,0] + (g[0,1] * x) #f是假设函数H fig, ax = plt.subplots(figsize=(12,8))#以其他关键字参数**fig_kw来创建图 #figsize=(a,b):figsize 设置图形的大小,b为图形的宽,b为图形的高,单位为英寸 ax.plot(x, f, 'r', label='Prediction') #设置点的横坐标，纵坐标，用红色线，并且设置Prediction为关键字参数 ax.scatter(df.横坐标, df.纵坐标, label='Traning Data') #以人口为横坐标，利润为纵坐标并且设置Traning Data为关键字参数 ax.legend(loc=2) #legend为显示图例函数，loc为设置图例显示的位置，loc=2即在左上方 ax.set_xlabel('Population') #设置x轴变量 ax.set_ylabel('Profit') #设置x轴变量 ax.set_title('Predicted Profit vs. Population Size') #设置表头 plt.show() # 代价数据可视化1 c=pd.DataFrame({'inters':list(range(iters)),'cost':cost }) ax=sns.lmplot(x='inters',y='cost',data=c,fit_reg=False) plt.show() # # 代价数据可视化2 # fig, ax = plt.subplots(figsize=(12,8)) #以其他关键字参数**fig_kw来创建图 # #figsize=(a,b):figsize 设置图形的大小,b为图形的宽,b为图形的高,单位为英寸 # ax.plot(np.arange(iters), cost, 'b') #作图:以迭代次数为x，代价函数值为y,线条颜色为红色 # ax.set_xlabel('Iterations') #设置x轴变量 # ax.set_ylabel('Cost') #设置y轴变量 # ax.set_title('Error vs. Training Epoch') #设置表头 # plt.show()]]></summary></entry><entry><title type="html"></title><link href="http://localhost:4000/Study-Blog/2022/02/11/.html" rel="alternate" type="text/html" title="" /><published>2022-02-11T00:00:00+00:00</published><updated>2022-02-11T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/2022/02/11/</id><content type="html" xml:base="http://localhost:4000/Study-Blog/2022/02/11/.html"><![CDATA[<!-- 
categories: 机器学习
tags: [机器学习,吴恩达]
 -->

<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ -->]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">吴恩达机器学习笔记 Day 1</title><link href="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/11/Andrew-Ng-DL-1.html" rel="alternate" type="text/html" title="吴恩达机器学习笔记 Day 1" /><published>2022-02-11T00:00:00+00:00</published><updated>2022-02-11T00:00:00+00:00</updated><id>http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/11/Andrew-Ng-DL-1</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2022/02/11/Andrew-Ng-DL-1.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ 
$\displaystyle\underbrace{a_i}_{\text{i从1到n}}$

$\displaystyle\mathop{a_i}\limits_{i\text{从1到}n}$
-->

<h2 id="监督学习">监督学习</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/2.png" alt="预测房价" /></p>

<p>y轴是房价，x轴是房子面积，此时用一条直线拟合数据,假设卖家的房子尺寸是$750^2$，那么它的售价为150*1000美元，如上图所示。</p>

<p>如果函数用二阶多项式，甚至更高的高阶多项式来表达，那么拟合数据的表现可能会更佳。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/3.png" alt="" /></p>

<p>这回归问题的价格是一个离散值，预测离散值的输出。</p>

<p>无论采用哪种模型算法，都属于监督学习的类型，监督学习算法目的是给定训练数据集，迭代函数的权重获得正确的答案，下次输入未知的x也能输出需要的答案。</p>

<h2 id="无监督学习">无监督学习</h2>

<p>假设某函数判断病人的肿瘤<strong>是否良性或恶性</strong>，用<strong>输出0与1</strong>来表示，这类有答案的。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/4.png" alt="" /></p>

<p>当这些数据集没有任何标签，没有答案，算法<strong>自行学习其中的结构</strong>，将其给<strong>不同数据自行分类</strong>。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/5.png" alt="" /></p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/6.png" alt="" /></p>

<h2 id="模型描述">模型描述</h2>

<p>线性<code class="language-plaintext highlighter-rouge">回归</code>，这里的回归是指<strong>分类</strong>问题，用来预测离散值输出。</p>

<p>举个例子：城市不同房子对应不同售价，假设某房子1250英尺，它的售价220000美元，数据集包含卖出的房子面积大小和价格。</p>

<table>
  <thead>
    <tr>
      <th>$面积/m^{2} (x)$</th>
      <th>$价格/1000美元(y)$</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2014</td>
      <td>460</td>
    </tr>
    <tr>
      <td>1416</td>
      <td>212</td>
    </tr>
    <tr>
      <td>1436</td>
      <td>315</td>
    </tr>
    <tr>
      <td>859</td>
      <td>178</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p><strong>符号</strong></p>

<ul>
  <li><strong>m</strong>=训练样本的数量：
    <ul>
      <li>假如表格有47列，那么m=47</li>
    </ul>
  </li>
  <li><strong>x’s</strong>=输入变量/特征：
    <ul>
      <li>表格左侧的面积是输入变量或特征</li>
    </ul>
  </li>
  <li><strong>y’s</strong>=输出变量/目标变量：
    <ul>
      <li>表格右侧的价格</li>
    </ul>
  </li>
</ul>

<p>模型的工作是从这个数据中训练的。</p>

<p>$(x,y)$</p>

<ul>
  <li>表示特定训练样本</li>
</ul>

<p>$(x^{(i)},y^{(i)})$</p>

<ul>
  <li>上标i表示第i个训练样本，充当索引作用，$x^{(1)}$是$2104$，$x^{(2)}$是$1416$，$y^{(1)}$是$460$</li>
</ul>

<pre><code class="language-mermaid">graph TB;
	A[训练集]
	B[学习算法]
	C[h(函数)]
	D[房子面积]
	E[售价]
	A--&gt;B;
	B--&gt;C;
	D--&gt;C;
	C--&gt;E;
</code></pre>

<p>得到h函数，房子面积作为x输入，得到y的输出。这种根据房子尺寸进行房价预测的函数，<strong>函数</strong>取决于于用户<strong>用什么来表达</strong>。</p>

\[\begin{equation*}
h_{\theta}=\theta_{0}+\theta_{1}x
\end{equation*}\]

<p>缩写: \(h_{\theta}(x)\)</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/1.png" alt="线性回归" /></p>

<p>这种线性回归类型，是取决于x单一变量的。</p>

<h2 id="损失函数">损失函数</h2>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/7.png" alt="" /></p>

<p>$\theta_{0},\theta_{1}$是常数，输出的值可能不同，通过数据集的训练使直线尽可能靠拢、拟合数据。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/7.png" alt="未分类的数据集" /></p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/8.png" alt="已分类的数据集" /></p>

<p>如何得到正确的$\theta_{0},\theta_{1}$呢？</p>

<p>损失函数定义：$\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2$，其中$h_{\theta}(x^{(i)})=\theta_{0}+\theta_{1}x^{(i)}$</p>

<p>m为47个训练样本，通过损失函数计算它们的输出值$h_{\theta}$减去实际值$y^{(i)}$的二次方和从而得到的$J(\theta)$，目的是尽可能<strong>降低这个值</strong>。</p>

<p>为了让损失函数变得容易理解，减少至$\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2$</p>

<p>这是线性回归的整体目标函数，目标是减小$\theta_{0},\theta_{1}$的值，使<strong>$J(\theta)$最小化</strong>。</p>

<h3 id="损失函数公式">损失函数公式</h3>

\[\begin{equation}
  J(\theta_{0},\theta_{1})=\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2  \\
  \mathop{minimize}\limits_{\theta_{0},\theta_{1}} \mathop{J(\theta_{0},\theta_{1})}_{\text{损失函数}}
\end{equation}\]

<p>minimize v. 使减少到最低限度。</p>

<p>函数模型的参数有各种各样,为了计算方便，这里先假定函数$h_{\theta}=\theta_{1}x^{(i)}$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/10.png" alt="" /></p>

<p>假定$\theta_{1}=1$</p>

\[\begin{equation*}
  \begin{split}
    J(1)
    &amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2  \\
	&amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (\theta-y^{(i)})^2  \\
	&amp; = \frac{1}{2m}(0^{2}+0^{2}+0^{2})  \\
	&amp; = 0^{2}
  \end{split}
\end{equation*}\]

<p>函数拟合数据得到最好的表现，<strong>$J(\theta)$为0</strong>，这是最理想的情况。</p>

<p>假定$\theta_{1}=0.5$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/11.png" alt="" /></p>

\[\begin{equation*}
  \begin{split}
    J(0.5)
	&amp; = \frac{1}{2m}[(0*5-1)^{2}+(1-2)^{2}+(1*5-3)^{2})] \\
	&amp; = \frac{1}{2*3}(3*5) \\
	&amp; = \frac{3*5}{6} \\
	&amp; = 0.58
  \end{split}
\end{equation*}\]

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/12.png" alt="" /></p>

\[\begin{equation*}
  \begin{split}
    J(0)
	&amp; = \frac{1}{2m}[1^{2}+2^{2}+3^{2})] \\
	&amp; = \frac{1}{6}*14 \\
	&amp; = 2.3
  \end{split}
\end{equation*}\]

<p>同理，当$\theta_{1}=-0.5$时，$J(\theta)$为5.25，<strong>误差非常大</strong>。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/13.png" alt="梯度" /></p>

<p>横轴为$\theta_{1}$，纵轴为$J(\theta)$，之前$\theta_{1}=1、\theta_{0}=1\ldots$给出的$J(\theta)$纵轴。</p>

<p>损失函数的优化目标是获得最小值的$J(\theta_{1})$，此时上图$\theta_{1}=1$是全局最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/14.png" alt="" /></p>

<p>$\theta_{0}=50,\theta_{1}=0.06$,绘制$J(\theta_{0},\theta_{1})$就多了一个$\theta_{0}=50$，它与训练集相关。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/15.png" alt="" /></p>

<p>这是等高线图，用2D表现如下图(几圈圈椭圆)，最小值位于最小的椭圆中心。</p>

<p>首先假定$\theta_{0}=800,\theta_{1}=-0.15$，位于下图$J(\theta_{0},\theta_{1})$手绘篮圈的红叉号。左边的$h(x)$显然不符合，离最小值甚远，因为拟合的不好。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/16.png" alt="" /></p>

<p>假定$\theta_{0}=360,\theta_{1}=0$，离最小值有些接近了。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/17.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/18.png" alt="" /></p>

<p>到了这里，虽然不是最小值，这是训练样本实际值与预测值之间的$J(\theta)$，$J(\theta_{0},\theta_{1})$表示损失函数J的意义，接近损失函数J最小值的点。</p>

<p>我们目标是寻找一个高效的算法，来自动寻找损失函数J最小值，之后由于涉及众多参数、更高维的图形，只能通过代码实现。</p>

<h2 id="梯度下降">梯度下降</h2>

<p>用梯度下降算法最小化任意函数J，包括线性函数、非线性函数等。</p>

<p>$J(\theta_{0},\theta_{1})$可扩展$J(\theta_{0},\theta_{1},\theta_{2}\ldots\theta_{n})$</p>

<p>最小化$\mathop{min}\limits_{\theta_{0},\theta_{1}}J(\theta_{0},\theta_{1})$亦可$\mathop{min}\limits_{\theta_{0},\theta_{1}}J(\theta_{0},\theta_{1},\theta_{2}\ldots\theta_{n})$</p>

<p>梯度下降，一般设$\theta_{0}=0,\theta_{1}=0$，假如能看到哪里是最低点，如下图一步一步走不断收敛至局部最低点。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/19.png" alt="" /></p>

<p>实际上，我们并不能环顾四周看哪里是最低的，是碰运气不断走着，直到获得完全不同的局部最优解。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/20.png" alt="" /></p>

<h3 id="梯度下降算法">梯度下降算法</h3>

\[\text{重复直至收敛}{\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad (\text{同时更新 j=0 和 j=1})}\]

<p><code class="language-plaintext success highlighter-rouge">正确：同时更新</code>
\(temp0:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \\
temp1:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1}) \\
\theta_{0}:=temp0  \\
\theta_{1}:=temp1\)</p>

<p><code class="language-plaintext error highlighter-rouge">错误：异步更新</code>
\(temp0:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \\
\theta_{0}:=temp0  \\
temp1:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1}) \\
\theta_{1}:=temp1\)</p>

<blockquote>
  <p><code class="language-plaintext info highlighter-rouge">:=</code>这是计算机赋值的意思”=”，如a=1，b=1，a==b返回true。</p>
</blockquote>

<p>$\alpha$是<strong>学习率</strong>，如果这个常数很<strong>大</strong>，那么梯度下降$J(\theta_{0},\theta_{1})$容易<strong>越过最小值</strong>，左右横跳导致梯度上升(自己算)；如果太<strong>小</strong>，要迭代很<strong>多</strong>次才能收敛。</p>

<p>反复迭代，直至收敛为止。如果使用异步更新，那就不是正确的梯度下降算法了。</p>

<h3 id="梯度下降算法的意义">梯度下降算法的意义</h3>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/21.png" alt="" /></p>

<p>公式的$\frac{\partial}{\partial \theta_{1}}J(\theta_{0})$是取坐标轴曲线一点的切线(红色直线的斜率)，这条线有正斜率，说明它有正导数。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/22.png" alt="" /></p>

<p>因为是正数，$\theta_{1}$减去$\partial$乘以的正数，在坐标轴体现的是向左移动，然后迭代几轮收敛到最小值。</p>

<p>如果$\theta_{1}$在左边，那么$\frac{\partial}{\partial \theta_{1}} J(\theta_{1})$的导数是负的，通过梯度下降迭代给$\theta_{1}$值不断递增，直至梯度最小值为止。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/23.png" alt="" /></p>

<h4 id="梯度消失">梯度消失</h4>

<p>假设$\theta_{1}$处于局部最优解，这样的话$\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})$的斜率等于0，不上升也不下降。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/24.png" alt="" /></p>

<p>是因为$\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})$的斜率更新，刚开始从右边紫点出发，这时导数值值比较大，绿点随着接近最小值，得到导数值变小了，得到红点，再继续梯度下降，直至收敛到局部极小值。根据定义，在局部最低点时导数等于0，这是梯度下降算法的运行方式，所以没必要减小$\partial$的值</p>

\[\begin{equation*}
  \begin{split}
    \frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})
	&amp; = \frac{\partial}{\partial \theta_{1}}\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2  \\
	&amp; = \frac{\partial}{\partial \theta_{1}}\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (\theta_{0}+\theta_{1}x^{(i)}-y^{(i)})^2  \\
	&amp; = 
  \end{split}
\end{equation*}\]

<p>推导函数的两个$\theta$值，按上述推导可以明白(多元微分推导)</p>

<p>对应$\theta_{0}$的$\frac{\partial}{\vartheta \theta_{0}}J(\theta_{0},\theta_{1})$的偏导数</p>

\[\begin{equation*}
  \begin{split}
    \theta_{0}
	&amp; = 0:\frac{\partial}{\partial \theta_{0}}J(\theta_{0},\theta_{1})
	&amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})
  \end{split}
\end{equation*}\]

<p>对应$\theta_{1}$的$\frac{\partial}{\vartheta \theta_{1}}J(\theta_{0},\theta_{1})$的偏导数</p>

\[\begin{equation*}
  \begin{split}
    \theta_{1}
	&amp; = 1:\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})
	&amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})*x^{(i)}
  \end{split}
\end{equation*}\]

<p>然后$\theta_{0},\theta_{1}$不断重复该过程直到收敛。线性回归问题通常表现为弓状函数(凸函数)，这种看起来像弓形，只有一个全局最优解，当计算这种损失函数的梯度下降，总是收敛到全局最优。</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/26.png" alt="" /></p>

<p>房价的数据集，先初始设定$\theta_{0}=-900,\theta_{1}=-0.1$</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/27.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/28.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/29.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/30.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/31.png" alt="" />
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-11/32.png" alt="" /></p>

<p>这种梯度下降算法叫<strong>“Batch”</strong>梯度下降，遍历所有训练样本并计算偏导数，每一次梯度下降总要计算m个样本的总和。</p>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="机器学习" /><category term="机器学习" /><category term="吴恩达" /><summary type="html"><![CDATA[监督学习 y轴是房价，x轴是房子面积，此时用一条直线拟合数据,假设卖家的房子尺寸是$750^2$，那么它的售价为150*1000美元，如上图所示。 如果函数用二阶多项式，甚至更高的高阶多项式来表达，那么拟合数据的表现可能会更佳。 这回归问题的价格是一个离散值，预测离散值的输出。 无论采用哪种模型算法，都属于监督学习的类型，监督学习算法目的是给定训练数据集，迭代函数的权重获得正确的答案，下次输入未知的x也能输出需要的答案。 无监督学习 假设某函数判断病人的肿瘤是否良性或恶性，用输出0与1来表示，这类有答案的。 当这些数据集没有任何标签，没有答案，算法自行学习其中的结构，将其给不同数据自行分类。 模型描述 线性回归，这里的回归是指分类问题，用来预测离散值输出。 举个例子：城市不同房子对应不同售价，假设某房子1250英尺，它的售价220000美元，数据集包含卖出的房子面积大小和价格。 $面积/m^{2} (x)$ $价格/1000美元(y)$ 2014 460 1416 212 1436 315 859 178 … … 符号 m=训练样本的数量： 假如表格有47列，那么m=47 x’s=输入变量/特征： 表格左侧的面积是输入变量或特征 y’s=输出变量/目标变量： 表格右侧的价格 模型的工作是从这个数据中训练的。 $(x,y)$ 表示特定训练样本 $(x^{(i)},y^{(i)})$ 上标i表示第i个训练样本，充当索引作用，$x^{(1)}$是$2104$，$x^{(2)}$是$1416$，$y^{(1)}$是$460$ graph TB; A[训练集] B[学习算法] C[h(函数)] D[房子面积] E[售价] A--&gt;B; B--&gt;C; D--&gt;C; C--&gt;E; 得到h函数，房子面积作为x输入，得到y的输出。这种根据房子尺寸进行房价预测的函数，函数取决于于用户用什么来表达。 \[\begin{equation*} h_{\theta}=\theta_{0}+\theta_{1}x \end{equation*}\] 缩写: \(h_{\theta}(x)\) 这种线性回归类型，是取决于x单一变量的。 损失函数 $\theta_{0},\theta_{1}$是常数，输出的值可能不同，通过数据集的训练使直线尽可能靠拢、拟合数据。 如何得到正确的$\theta_{0},\theta_{1}$呢？ 损失函数定义：$\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2$，其中$h_{\theta}(x^{(i)})=\theta_{0}+\theta_{1}x^{(i)}$ m为47个训练样本，通过损失函数计算它们的输出值$h_{\theta}$减去实际值$y^{(i)}$的二次方和从而得到的$J(\theta)$，目的是尽可能降低这个值。 为了让损失函数变得容易理解，减少至$\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2$ 这是线性回归的整体目标函数，目标是减小$\theta_{0},\theta_{1}$的值，使$J(\theta)$最小化。 损失函数公式 \[\begin{equation} J(\theta_{0},\theta_{1})=\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2 \\ \mathop{minimize}\limits_{\theta_{0},\theta_{1}} \mathop{J(\theta_{0},\theta_{1})}_{\text{损失函数}} \end{equation}\] minimize v. 使减少到最低限度。 函数模型的参数有各种各样,为了计算方便，这里先假定函数$h_{\theta}=\theta_{1}x^{(i)}$ 假定$\theta_{1}=1$ \[\begin{equation*} \begin{split} J(1) &amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2 \\ &amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (\theta-y^{(i)})^2 \\ &amp; = \frac{1}{2m}(0^{2}+0^{2}+0^{2}) \\ &amp; = 0^{2} \end{split} \end{equation*}\] 函数拟合数据得到最好的表现，$J(\theta)$为0，这是最理想的情况。 假定$\theta_{1}=0.5$ \[\begin{equation*} \begin{split} J(0.5) &amp; = \frac{1}{2m}[(0*5-1)^{2}+(1-2)^{2}+(1*5-3)^{2})] \\ &amp; = \frac{1}{2*3}(3*5) \\ &amp; = \frac{3*5}{6} \\ &amp; = 0.58 \end{split} \end{equation*}\] \[\begin{equation*} \begin{split} J(0) &amp; = \frac{1}{2m}[1^{2}+2^{2}+3^{2})] \\ &amp; = \frac{1}{6}*14 \\ &amp; = 2.3 \end{split} \end{equation*}\] 同理，当$\theta_{1}=-0.5$时，$J(\theta)$为5.25，误差非常大。 横轴为$\theta_{1}$，纵轴为$J(\theta)$，之前$\theta_{1}=1、\theta_{0}=1\ldots$给出的$J(\theta)$纵轴。 损失函数的优化目标是获得最小值的$J(\theta_{1})$，此时上图$\theta_{1}=1$是全局最优解。 $\theta_{0}=50,\theta_{1}=0.06$,绘制$J(\theta_{0},\theta_{1})$就多了一个$\theta_{0}=50$，它与训练集相关。 这是等高线图，用2D表现如下图(几圈圈椭圆)，最小值位于最小的椭圆中心。 首先假定$\theta_{0}=800,\theta_{1}=-0.15$，位于下图$J(\theta_{0},\theta_{1})$手绘篮圈的红叉号。左边的$h(x)$显然不符合，离最小值甚远，因为拟合的不好。 假定$\theta_{0}=360,\theta_{1}=0$，离最小值有些接近了。 到了这里，虽然不是最小值，这是训练样本实际值与预测值之间的$J(\theta)$，$J(\theta_{0},\theta_{1})$表示损失函数J的意义，接近损失函数J最小值的点。 我们目标是寻找一个高效的算法，来自动寻找损失函数J最小值，之后由于涉及众多参数、更高维的图形，只能通过代码实现。 梯度下降 用梯度下降算法最小化任意函数J，包括线性函数、非线性函数等。 $J(\theta_{0},\theta_{1})$可扩展$J(\theta_{0},\theta_{1},\theta_{2}\ldots\theta_{n})$ 最小化$\mathop{min}\limits_{\theta_{0},\theta_{1}}J(\theta_{0},\theta_{1})$亦可$\mathop{min}\limits_{\theta_{0},\theta_{1}}J(\theta_{0},\theta_{1},\theta_{2}\ldots\theta_{n})$ 梯度下降，一般设$\theta_{0}=0,\theta_{1}=0$，假如能看到哪里是最低点，如下图一步一步走不断收敛至局部最低点。 实际上，我们并不能环顾四周看哪里是最低的，是碰运气不断走着，直到获得完全不同的局部最优解。 梯度下降算法 \[\text{重复直至收敛}{\theta_{j}:=\theta_{j}-\alpha\frac{\partial}{\partial\theta_{j}}J(\theta_{0},\theta_{1})\qquad (\text{同时更新 j=0 和 j=1})}\] 正确：同时更新 \(temp0:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \\ temp1:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1}) \\ \theta_{0}:=temp0 \\ \theta_{1}:=temp1\) 错误：异步更新 \(temp0:=\theta_{0}-\alpha\frac{\partial}{\partial\theta_{0}}J(\theta_{0},\theta_{1}) \\ \theta_{0}:=temp0 \\ temp1:=\theta_{1}-\alpha\frac{\partial}{\partial\theta_{1}}J(\theta_{0},\theta_{1}) \\ \theta_{1}:=temp1\) :=这是计算机赋值的意思”=”，如a=1，b=1，a==b返回true。 $\alpha$是学习率，如果这个常数很大，那么梯度下降$J(\theta_{0},\theta_{1})$容易越过最小值，左右横跳导致梯度上升(自己算)；如果太小，要迭代很多次才能收敛。 反复迭代，直至收敛为止。如果使用异步更新，那就不是正确的梯度下降算法了。 梯度下降算法的意义 公式的$\frac{\partial}{\partial \theta_{1}}J(\theta_{0})$是取坐标轴曲线一点的切线(红色直线的斜率)，这条线有正斜率，说明它有正导数。 因为是正数，$\theta_{1}$减去$\partial$乘以的正数，在坐标轴体现的是向左移动，然后迭代几轮收敛到最小值。 如果$\theta_{1}$在左边，那么$\frac{\partial}{\partial \theta_{1}} J(\theta_{1})$的导数是负的，通过梯度下降迭代给$\theta_{1}$值不断递增，直至梯度最小值为止。 梯度消失 假设$\theta_{1}$处于局部最优解，这样的话$\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})$的斜率等于0，不上升也不下降。 是因为$\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1})$的斜率更新，刚开始从右边紫点出发，这时导数值值比较大，绿点随着接近最小值，得到导数值变小了，得到红点，再继续梯度下降，直至收敛到局部极小值。根据定义，在局部最低点时导数等于0，这是梯度下降算法的运行方式，所以没必要减小$\partial$的值 \[\begin{equation*} \begin{split} \frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1}) &amp; = \frac{\partial}{\partial \theta_{1}}\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})^2 \\ &amp; = \frac{\partial}{\partial \theta_{1}}\frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (\theta_{0}+\theta_{1}x^{(i)}-y^{(i)})^2 \\ &amp; = \end{split} \end{equation*}\] 推导函数的两个$\theta$值，按上述推导可以明白(多元微分推导) 对应$\theta_{0}$的$\frac{\partial}{\vartheta \theta_{0}}J(\theta_{0},\theta_{1})$的偏导数 \[\begin{equation*} \begin{split} \theta_{0} &amp; = 0:\frac{\partial}{\partial \theta_{0}}J(\theta_{0},\theta_{1}) &amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)}) \end{split} \end{equation*}\] 对应$\theta_{1}$的$\frac{\partial}{\vartheta \theta_{1}}J(\theta_{0},\theta_{1})$的偏导数 \[\begin{equation*} \begin{split} \theta_{1} &amp; = 1:\frac{\partial}{\partial \theta_{1}}J(\theta_{0},\theta_{1}) &amp; = \frac{1}{2m}\displaystyle\sum\limits_{i=1}^m (h_{\theta}-y^{(i)})*x^{(i)} \end{split} \end{equation*}\] 然后$\theta_{0},\theta_{1}$不断重复该过程直到收敛。线性回归问题通常表现为弓状函数(凸函数)，这种看起来像弓形，只有一个全局最优解，当计算这种损失函数的梯度下降，总是收敛到全局最优。 房价的数据集，先初始设定$\theta_{0}=-900,\theta_{1}=-0.1$ 这种梯度下降算法叫“Batch”梯度下降，遍历所有训练样本并计算偏导数，每一次梯度下降总要计算m个样本的总和。]]></summary></entry><entry><title type="html">Jekyll博客配置</title><link href="http://localhost:4000/Study-Blog/%E5%8D%9A%E5%AE%A2/2022/02/06/configuration.html" rel="alternate" type="text/html" title="Jekyll博客配置" /><published>2022-02-06T11:26:41+00:00</published><updated>2022-02-06T11:26:41+00:00</updated><id>http://localhost:4000/Study-Blog/%E5%8D%9A%E5%AE%A2/2022/02/06/configuration</id><content type="html" xml:base="http://localhost:4000/Study-Blog/%E5%8D%9A%E5%AE%A2/2022/02/06/configuration.html"><![CDATA[<p>Jekyll 允许你很轻松的设计你的网站，这很大程度上归功于灵活强大的配置功能。既可以配置在网站根目录下的  <em>_config.yml</em> 文件，也可以作为命令行的标记来配置。</p>

<p class="warning"><em>_config.yml</em> 包括一些在运行时一次性读入的全局配置和变量定义， 在自动生成的过程中并<strong>不会</strong>重新加载，除非重新运行。注意 Data Files 包括在自动生成范围内，可以在更改后自动重新加载。</p>

<!--more-->

<h2 id="网站配置项">网站配置项</h2>

<h3 id="主题">主题</h3>

<p>如果是通过主题方式安装的，你需要这样配置以启用主题:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">theme</span><span class="pi">:</span> <span class="s">jekyll-text-theme</span>
</code></pre></div></div>

<h3 id="皮肤">皮肤</h3>

<p>TeXt 内置有 6 套皮肤，你也可以定制自己的皮肤。</p>

<table>
  <thead>
    <tr>
      <th><code class="language-plaintext highlighter-rouge">default</code></th>
      <th><code class="language-plaintext highlighter-rouge">dark</code></th>
      <th><code class="language-plaintext highlighter-rouge">forest</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_default.jpg" alt="Default" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_dark.jpg" alt="Dark" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_forest.jpg" alt="Forest" /></td>
    </tr>
  </tbody>
</table>

<table>
  <thead>
    <tr>
      <th><code class="language-plaintext highlighter-rouge">ocean</code></th>
      <th><code class="language-plaintext highlighter-rouge">chocolate</code></th>
      <th><code class="language-plaintext highlighter-rouge">orange</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_ocean.jpg" alt="Ocean" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_chocolate.jpg" alt="Chocolate" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/skins_orange.jpg" alt="Orange" /></td>
    </tr>
  </tbody>
</table>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">text_skin</span><span class="pi">:</span> <span class="s">default</span> <span class="c1"># "default" (default), "dark", "forest", "ocean", "chocolate", "orange"</span>
</code></pre></div></div>

<h3 id="代码高亮主题">代码高亮主题</h3>

<p>TeXt 使用 <a href="https://github.com/chriskempson/tomorrow-theme">Tomorrow</a> 作为它的代码高亮主题。</p>

<table>
  <thead>
    <tr>
      <th><code class="language-plaintext highlighter-rouge">tomorrow</code></th>
      <th><code class="language-plaintext highlighter-rouge">tomorrow-night</code></th>
      <th><code class="language-plaintext highlighter-rouge">tomorrow-night-eighties</code></th>
      <th><code class="language-plaintext highlighter-rouge">tomorrow-night-blue</code></th>
      <th><code class="language-plaintext highlighter-rouge">tomorrow-night-bright</code></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/highlight_tomorrow.png" alt="Tomorrow" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/highlight_tomorrow-night.png" alt="Tomorrow Night" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/highlight_tomorrow-night-eighties.png" alt="Tomorrow Night Eighties" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/highlight_tomorrow-night-blue.png" alt="Tomorrow Night Blue" /></td>
      <td><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/screenshots/highlight_tomorrow-night-bright.png" alt="Tomorrow Night Bright" /></td>
    </tr>
  </tbody>
</table>

<p>每个皮肤有一个默认的代码高亮主题，当然你可以通过 <code class="language-plaintext highlighter-rouge">highlight_theme</code> 配置项来指定其他主题。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">highlight_theme</span><span class="pi">:</span> <span class="s">default</span> <span class="c1"># "default" (default), "tomorrow", "tomorrow-night", "tomorrow-night-eighties", "tomorrow-night-blue", "tomorrow-night-bright"</span>
</code></pre></div></div>

<h3 id="url">URL</h3>

<p>网站的协议和域名。</p>

<p>如果你的网站是搭建在 Github Pages 上的，这个只会被设置为 GitHub Pages 域名（CNAME 或个人域名）<sup id="fnref:gitHub_metadata" role="doc-noteref"><a href="#fn:gitHub_metadata" class="footnote" rel="footnote">1</a></sup>. 举个例子, https://kitian616.github.io 或者 https://tianqi.name 如果设置了 CNAME。</p>

<p class="warning">在 3.3 及更高版本的 Jekyll 中运行 <code class="language-plaintext highlighter-rouge">jekyll serve</code> 命令会在开发模式时将其设置为 url: http://localhost:4000，你可以通过 <code class="language-plaintext highlighter-rouge">JEKYLL_ENV=production</code> 设置生产环境。</p>

<h3 id="base-url">Base URL</h3>

<p>网站的根路径，不包含域名，其默认值为“/”。如果你的网站是搭建在 Github Pages 上的，那么这个值会默认设置为网站对应的项目名<sup id="fnref:gitHub_metadata:1" role="doc-noteref"><a href="#fn:gitHub_metadata" class="footnote" rel="footnote">1</a></sup>。</p>

<h3 id="网站标题">网站标题</h3>

<p>网站的标题。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">title</span><span class="pi">:</span> <span class="s2">"</span><span class="s">My</span><span class="nv"> </span><span class="s">Awesome</span><span class="nv"> </span><span class="s">Website"</span>
</code></pre></div></div>

<h3 id="网站描述">网站描述</h3>

<p>使用一些简短的语言来描述你的网站。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">description</span><span class="pi">:</span> <span class="pi">&gt;</span> <span class="c1"># this means to ignore newlines until "nav_lists:"</span>
  <span class="s">A website with awesome stories.</span>
</code></pre></div></div>

<h2 id="语言与时区">语言与时区</h2>

<h3 id="语言">语言</h3>

<p>网站的语言，当然你可以在头信息<sup id="fnref:font_matter" role="doc-noteref"><a href="#fn:font_matter" class="footnote" rel="footnote">2</a></sup>里指定特定的文章或页面的语言，你可以在<a href="https://tianqi.name/jekyll-TeXt-theme/docs/zh/i18n">国际化</a>找到更多信息。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">lang</span><span class="pi">:</span> <span class="s">en</span>
</code></pre></div></div>

<h3 id="时区">时区</h3>

<p>设置时区，这个设置作用于 TZ 变量， Ruby 用它来处理日期和时间。使用 IANA Time Zone Database 标准，比如 America/New_York 。其默认值为操作系统的时区。详情请戳 <a href="https://en.wikipedia.org/wiki/List_of_tz_database_time_zones">这里</a>。<sup id="fnref:jekyll_global_configuration" role="doc-noteref"><a href="#fn:jekyll_global_configuration" class="footnote" rel="footnote">3</a></sup></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">timezone</span><span class="pi">:</span> <span class="s">Asia/Shanghai</span>
</code></pre></div></div>

<h2 id="作者信息">作者信息</h2>

<p>网站作者的信息（可以是个人、团队或者组织）。</p>

<h3 id="类型-type">类型 (type)</h3>

<p>作者的类型，个人或者是组织，作为 <a href="https://schema.org/">schema.org</a> 的语义化标记使用。</p>

<h3 id="作者名-name">作者名 (name)</h3>

<p>用来表明网站的作者。</p>

<h3 id="头像-avatar">头像 (avatar)</h3>

<p>作者的图片或 Logo。</p>

<h3 id="简介-bio">简介 (bio)</h3>

<p>关于作者的一个简单介绍。</p>

<h3 id="社交">社交</h3>

<p>社交网站的用户名或用户 ID。</p>

<p>目前支持邮件，微博，Facebook，Twitter，微博，Google Plus，Telegram，Medium，知乎，豆瓣，Linkedin，Github 和 Npm，持续添加中。</p>

<p>当你设置了相应项的值后，对应的社交按钮就会出现在页面下方。</p>

<h2 id="github-源码仓库">GitHub 源码仓库</h2>

<p><a href="https://github.com/jekyll/github-metadata">GitHub Metadata</a> 插件的设置, 详情请戳 <a href="https://github.com/jekyll/github-metadata/blob/master/docs/configuration.md#configuration">这里</a>。</p>

<p>这个设置告诉 jekyll-github-metadata 插件应该从哪个项目中获取元数据，其格式为 <code class="language-plaintext highlighter-rouge">项目所有者 ID/项目名称</code>，例如：kitian616/jekyll-TeXt-theme。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">repository</span><span class="pi">:</span> <span class="s">user_name/repo_name</span>
</code></pre></div></div>

<h2 id="文章配置项">文章配置项</h2>

<h3 id="摘要">摘要</h3>

<p>该主题的摘要有两种模式——TEXT 模式和 HTML 模式。 当 <em>_config.yml</em> 配置项 <code class="language-plaintext highlighter-rouge">excerpt_type</code> 的值为 <code class="language-plaintext highlighter-rouge">text</code> 时是 TEXT 模式，为 <code class="language-plaintext highlighter-rouge">html</code> 时是 HTML 模式，<strong>默认为 TEXT 模式</strong>。</p>

<table>
  <thead>
    <tr>
      <th>模式名称</th>
      <th>描述</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>text</strong></td>
      <td>此时摘要为纯文本，会过滤掉一切非文本元素（标题，链接，列表，表格，图片等等），且截取前 350 个字符。</td>
    </tr>
    <tr>
      <td><strong>html</strong></td>
      <td>此时摘要为 HTML 文档，与文章内容一致，并且 <strong>默认展示整篇文章的内容</strong>。若想控制摘要内容，需要在文章中想要显示到的地方加上 <code class="language-plaintext highlighter-rouge">&lt;!--more--&gt;</code>，详情请戳 <a href="https://jekyllrb.com/docs/posts/#post-excerpts">这里</a>。</td>
    </tr>
  </tbody>
</table>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">excerpt_separator</span><span class="pi">:</span> <span class="s">&lt;!--more--&gt;</span>
<span class="na">excerpt_type</span><span class="pi">:</span> <span class="s">text</span> <span class="c1"># text (default), html</span>
</code></pre></div></div>

<p class="error">该设置在 2.2.0 版中移到了 Articles 和 Home 布局的头信息中，详情请戳 <a href="https://tianqi.name/jekyll-TeXt-theme/docs/cn/layouts#articles-layout">这里</a>。</p>

<h3 id="许可协议">许可协议</h3>

<table>
  <thead>
    <tr>
      <th>名称</th>
      <th>许可协议</th>
      <th>图片</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CC-BY-4.0</td>
      <td><a href="https://creativecommons.org/licenses/by/4.0/">Attribution 4.0 International</a></td>
      <td><img src="https://i.creativecommons.org/l/by/4.0/88x31.png" alt="CC-BY-4.0" /></td>
    </tr>
    <tr>
      <td>CC-BY-SA-4.0</td>
      <td><a href="https://creativecommons.org/licenses/by-sa/4.0/">Attribution-ShareAlike 4.0 International</a></td>
      <td><img src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" alt="CC-BY-SA-4.0" /></td>
    </tr>
    <tr>
      <td>CC-BY-NC-4.0</td>
      <td><a href="https://creativecommons.org/licenses/by-nc/4.0/">Attribution-NonCommercial 4.0 International</a></td>
      <td><img src="https://i.creativecommons.org/l/by-nc/4.0/88x31.png" alt="CC-BY-NC-4.0" /></td>
    </tr>
    <tr>
      <td>CC-BY-ND-4.0</td>
      <td><a href="https://creativecommons.org/licenses/by-nd/4.0/">Attribution-NoDerivatives 4.0 International</a></td>
      <td><img src="https://i.creativecommons.org/l/by-nd/4.0/88x31.png" alt="CC-BY-ND-4.0" /></td>
    </tr>
  </tbody>
</table>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">license</span><span class="pi">:</span> <span class="s">CC-BY-NC-4.0</span>
</code></pre></div></div>

<h3 id="目录">目录</h3>

<p>作为文章目录的元素。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">toc</span><span class="pi">:</span>
  <span class="na">selectors</span><span class="pi">:</span> <span class="s2">"</span><span class="s">h1,h2,h3"</span>
</code></pre></div></div>

<h3 id="markdown-增强">Markdown 增强</h3>

<p>为了增强文章的写作和阅读体验，TeXt 对 Jekyll 现有的 markdown 做了一些增强。当然这些增强都是默认禁止的，你需要设置相应的配置项为 true 来启动它们：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Mathjax</span>
<span class="na">mathjax</span><span class="pi">:</span> <span class="no">true</span>
<span class="na">mathjax_autoNumber</span><span class="pi">:</span> <span class="no">true</span>

<span class="c1"># Mermaid</span>
<span class="na">mermaid</span><span class="pi">:</span> <span class="no">true</span>

<span class="c1"># Chart</span>
<span class="na">chart</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>当然你也可以为某一篇文章或页面通过头信息<sup id="fnref:font_matter:1" role="doc-noteref"><a href="#fn:font_matter" class="footnote" rel="footnote">2</a></sup>来做单独的指定。</p>

<p>在 <a href="/">撰写博客</a> 篇中有更详细的使用说明。</p>

<h2 id="分页">分页</h2>

<p><a href="https://github.com/jekyll/jekyll-paginate">Jekyll Paginate</a> 插件的配置。</p>

<p>要想对文章列表做分页，你需要在 <em>_config.yml</em> 文件中配置每页显示的文章数：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">paginate</span><span class="pi">:</span> <span class="m">8</span>
</code></pre></div></div>

<p>这个数字代表了文章列表页每页显示的最大文章数。</p>

<p>你也可以像这样指定分页页面的地址：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">paginate_path</span><span class="pi">:</span> <span class="s">/page:num</span> <span class="c1"># don't change this unless for special need</span>
</code></pre></div></div>

<h2 id="cdn-源">CDN 源</h2>

<p>TeXt 使用 CDN<sup id="fnref:cdn" role="doc-noteref"><a href="#fn:cdn" class="footnote" rel="footnote">4</a></sup> 来加快载入速度，你可以选择 <a href="http://www.bootcdn.cn/">BootCDN</a>（默认项）或者<a href="https://unpkg.com/">unpkg</a>作为网站的 CDN 源。它们都是开源而免费的。</p>

<p>国内用户请优先使用 BootCDN。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">sources</span><span class="pi">:</span> <span class="s">bootcdn</span> <span class="c1"># bootcdn (default), unpkg</span>
</code></pre></div></div>

<h2 id="分享">分享</h2>

<table>
  <thead>
    <tr>
      <th>名称</th>
      <th>分享系统提供方</th>
      <th>最低版本</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>addtoany</strong></td>
      <td><a href="https://www.addtoany.com/">AddToAny</a></td>
      <td>2.2.2</td>
    </tr>
    <tr>
      <td><strong>addthis</strong></td>
      <td><a href="https://www.addthis.com/">AddThis</a></td>
      <td>2.2.3</td>
    </tr>
    <tr>
      <td><strong>custom</strong></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="addtoany">AddToAny</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">comments</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">addtoany</span>
</code></pre></div></div>

<h3 id="addthis">AddThis</h3>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">comments</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">addthis</span>
  <span class="na">addthis</span><span class="pi">:</span>
    <span class="na">id</span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-addthis-pubid"</span>
</code></pre></div></div>

<p class="warning">你需要在页面的头信息里设置 <code class="language-plaintext highlighter-rouge">sharing</code> 属性为 <code class="language-plaintext highlighter-rouge">true</code> 来开启该页的评论，详情请戳 <a href="https://tianqi.name/jekyll-TeXt-theme/docs/zh/layouts#article-%E5%B8%83%E5%B1%80">这里</a>。</p>

<h2 id="评论">评论</h2>

<table>
  <thead>
    <tr>
      <th>名称</th>
      <th>评论系统提供方</th>
      <th>最低版本</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>disqus</strong></td>
      <td><a href="https://disqus.com/">Disqus</a></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>gitalk</strong></td>
      <td><a href="https://github.com/gitalk/gitalk/">Gitalk</a></td>
      <td> </td>
    </tr>
    <tr>
      <td><strong>valine</strong></td>
      <td><a href="https://valine.js.org/en/">Valine</a></td>
      <td>2.2.4</td>
    </tr>
    <tr>
      <td><strong>custom</strong></td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="disqus">Disqus</h3>

<p>要想启用 Disqus 作为评论系统，你需要注册一个 Disqus 账号然后申请一个针对该网站的 <a href="https://help.disqus.com/customer/portal/articles/466208-what-s-a-shortname-">shortname</a>，完成后将 shortname 填入到 <em>_config.yml</em> 中：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">comments</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">disqus</span>
  <span class="na">disqus</span><span class="pi">:</span>
    <span class="na">shortname</span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-disqus-shortname"</span>
</code></pre></div></div>

<h3 id="gitalk">Gitalk</h3>

<p>要想启用 Gitalk 作为评论系统，首先你需要一个 GitHub Application，如果没有<a href="https://github.com/settings/applications/new">点击这里</a>申请。然后将相应的参数添加到 <em>_config.yml</em> 配置中：</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">comments</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">gitalk</span>
  <span class="na">gitalk</span><span class="pi">:</span>
    <span class="na">clientID    </span><span class="pi">:</span> <span class="s2">"</span><span class="s">github-application-client-id"</span>
    <span class="na">clientSecret</span><span class="pi">:</span> <span class="s2">"</span><span class="s">github-application-client-secret"</span>
    <span class="na">repository  </span><span class="pi">:</span> <span class="s2">"</span><span class="s">github-repo"</span>
    <span class="na">owner       </span><span class="pi">:</span> <span class="s2">"</span><span class="s">github-repo-owner"</span>
    <span class="na">admin</span><span class="pi">:</span> <span class="c1"># Github repo owner and collaborators, only these guys can initialize github issues, IT IS A LIST.</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">your-github-id"</span>
      <span class="pi">-</span> <span class="s2">"</span><span class="s">the-other-admin-github-id"</span>
</code></pre></div></div>

<h3 id="valine">Valine</h3>

<p>要想启用 Valine 作为评论系统，你需要注册一个 LeanCloud 账号然后建立一个 LeanCloud 应用，详情见 <a href="#leancloud">LeanCloud</a>。因为 Valine 会自动在应用中建立一个 Class，因此并不需要像文章点击量那样手动建立 Class。</p>

<p>可以和<a href="#文章点击量">文章点击量</a>公用一个 LeanCloud 应用。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">comments</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">valine</span>
  <span class="na">valine</span><span class="pi">:</span>
    <span class="na">app_id  </span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-leanCloud-app-id"</span>
    <span class="na">app_key </span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-leanCloud-app-key"</span>
</code></pre></div></div>

<p class="warning">对于上面所有的评论系统, 你都需要在页面的头信息里设置 <code class="language-plaintext highlighter-rouge">key</code> 属性来开启该页的评论，详情请戳 <a href="https://tianqi.name/jekyll-TeXt-theme/docs/zh/layouts#page-%E5%B8%83%E5%B1%80">这里</a>。</p>

<h2 id="文章点击量">文章点击量</h2>

<table>
  <thead>
    <tr>
      <th>名称</th>
      <th>文章点击量后台提供方</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>leancloud</strong></td>
      <td><a href="https://leancloud.cn/">LeanCloud</a></td>
    </tr>
    <tr>
      <td><strong>custom</strong></td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="leancloud">LeanCloud</h3>

<p>TeXt 使用 LeanCloud 作为点击量功能的后台服务。你需要建立一个 LeanCloud 应用，然后在应用中建立一个 Class，之后将必要的信息填写到 <em>_config.yml</em> 文件中。下面详细介绍其操作步骤。</p>

<p>在进入<a href="https://leancloud.cn/">主页</a>后点击页面右上角的“访问控制台”，然后注册账号并登录。</p>

<p>在应用面板存储子页中点击“创建应用”按钮，在弹出的对话框中填写应用名称，计价方案选择“开发版”（土豪随意），然后点击“创建”按钮创建应用。</p>

<p>创建完成后应用面板上会出现你刚刚创建的应用卡片，点击进入选择创建 Class：</p>

<p><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/docs/assets/images/leancloud-create-class-0.jpg" alt="Leancloud：创建 Class 0" style="max-height:420px" class="border" /></p>

<p>在弹出的对话框中填写 Class 的名字，权限选择“无限制”，点击“创建 Class”按钮：</p>

<p><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/docs/assets/images/leancloud-create-class-1.jpg" alt="Leancloud：创建 Class 1" style="max-height:620px" class="border" /></p>

<p>当然你可以随时更改 Class 的权限，在应用面板存储子页中选择想要修改的 Class，点击上面菜单的其他 -&gt; 权限设置项，即可进入设置页：</p>

<p><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/docs/assets/images/leancloud-authorization.jpg" alt="Leancloud：权限设置" style="max-height:300px" /></p>

<p>最后点击应用面板右侧的“设置”，点击“应用 Key” 选项，即可得到对应的 APP ID 和 APP KEY：</p>

<p><img src="https://raw.githubusercontent.com/kitian616/jekyll-TeXt-theme/master/docs/assets/images/leancloud-app-info.jpg" alt="Leancloud：App Info" class="border" /></p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">pageview</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">leancloud</span>
  <span class="na">leancloud</span><span class="pi">:</span>
    <span class="na">app_id    </span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-leanCloud-app-id"</span>
    <span class="na">app_key   </span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-leanCloud-app-key"</span>
    <span class="na">app_class </span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-leanCloud-app-class"</span>
</code></pre></div></div>

<p class="warning">对于上面所有的点击量统计, 你都需要在页面的头信息里设置 <code class="language-plaintext highlighter-rouge">key</code> 属性来开启该页的统计功能，详情请戳 <a href="https://tianqi.name/jekyll-TeXt-theme/docs/zh/layouts#page-%E5%B8%83%E5%B1%80">这里</a>。</p>

<h2 id="站点统计">站点统计</h2>

<table>
  <thead>
    <tr>
      <th>名称</th>
      <th>站点统计提供方</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>google</strong></td>
      <td><a href="https://analytics.google.com/">Google Analytics</a></td>
    </tr>
    <tr>
      <td><strong>custom</strong></td>
      <td> </td>
    </tr>
  </tbody>
</table>

<h3 id="google-analytics">Google Analytics</h3>

<p>你需要将 <code class="language-plaintext highlighter-rouge">tracking_id</code> 设置成你的 Google Analytics 账户中针对该网站的跟踪代码来启动这项功能。你也可以通过设置 <code class="language-plaintext highlighter-rouge">anonymize_ip</code> 为 <code class="language-plaintext highlighter-rouge">true</code> 将所有事件的 IP 地址匿名化。</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">analytics</span><span class="pi">:</span>
  <span class="na">provider</span><span class="pi">:</span> <span class="s">google</span>
  <span class="na">google</span><span class="pi">:</span>
    <span class="na">tracking_id</span><span class="pi">:</span> <span class="s2">"</span><span class="s">your-google-analytics-tracking-code"</span>
    <span class="na">anonymize_ip</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>文章或页面的作者默认为在 <em>_config.yml</em> 中配置的 <code class="language-plaintext highlighter-rouge">author</code>，当然你可以通过 YAML 头信息来指定特定文章的作者。</p>

<p>首先你需要有一个 <em>_data/authors.yml</em> 的文件，参考以下格式来增加作者信息。其可用参数和 <em>_config.yml</em> 中的 <code class="language-plaintext highlighter-rouge">author</code> 项的参数一致（type, name, url, avatar, bio, email, facebook 等）。</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">Tian Qi</span><span class="pi">:</span>
  <span class="na">name      </span><span class="pi">:</span> <span class="s">Tian Qi</span>
  <span class="na">url       </span><span class="pi">:</span> <span class="s">https://tianqi.name</span>
  <span class="na">avatar    </span><span class="pi">:</span> <span class="s">https://wx3.sinaimg.cn/large/73bd9e13ly1fjkqy66hl8j208c08c0td.jpg</span>
  <span class="na">bio       </span><span class="pi">:</span> <span class="s">Author of TeXt.</span>
  <span class="na">email     </span><span class="pi">:</span> <span class="s">kitian616@outlook.com</span>
  <span class="na">facebook  </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://www.facebook.com/user_name</span>
  <span class="na">twitter   </span><span class="pi">:</span> <span class="s">kitian616</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://twitter.com/user_name</span>
  <span class="na">weibo     </span><span class="pi">:</span> <span class="m">234695683</span> <span class="c1"># "user_id"   the last part of your profile url, e.g. https://www.weibo.com/user_id/profile?...</span>
  <span class="na">googleplus</span><span class="pi">:</span> <span class="m">101827554735084402671</span> <span class="c1"># "user_id"   the last part of your profile url, e.g. https://plus.google.com/u/0/user_id</span>
  <span class="na">telegram  </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://t.me/user_name</span>
  <span class="na">medium    </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://medium.com/user_name</span>
  <span class="na">zhihu     </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://www.zhihu.com/people/user_name</span>
  <span class="na">douban    </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://www.douban.com/people/user_name</span>
  <span class="na">linkedin  </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://www.linkedin.com/in/user_name</span>
  <span class="na">github    </span><span class="pi">:</span> <span class="s">kitian616</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://github.com/user_name</span>
  <span class="na">npm       </span><span class="pi">:</span> <span class="c1"># "user_name" the last part of your profile url, e.g. https://www.npmjs.com/~user_name</span>
</code></pre></div></div>

<p>将在 <em>authors.yml</em> 中定义的作者作为某篇文章或页面的作者以覆盖 <code class="language-plaintext highlighter-rouge">site.author</code> 全局作者。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>---
author: Tian Qi
---
</code></pre></div></div>
<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:gitHub_metadata" role="doc-endnote">
      <p><a href="https://github.com/jekyll/github-metadata#what-it-does">GitHub Metadata, a.k.a. site.github</a> <a href="#fnref:gitHub_metadata" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:gitHub_metadata:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:font_matter" role="doc-endnote">
      <p><a href="https://jekyllrb.com/docs/frontmatter/">Front Matter</a> <a href="#fnref:font_matter" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:font_matter:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a></p>
    </li>
    <li id="fn:jekyll_global_configuration" role="doc-endnote">
      <p><a href="https://jekyllrb.com/docs/configuration/#global-configuration">Configuration#Global Configuration</a> <a href="#fnref:jekyll_global_configuration" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:cdn" role="doc-endnote">
      <p><a href="https://en.wikipedia.org/wiki/Content_delivery_network">Content delivery network</a> <a href="#fnref:cdn" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="博客" /><category term="博客" /><category term="写作" /><summary type="html"><![CDATA[Jekyll 允许你很轻松的设计你的网站，这很大程度上归功于灵活强大的配置功能。既可以配置在网站根目录下的 _config.yml 文件，也可以作为命令行的标记来配置。 _config.yml 包括一些在运行时一次性读入的全局配置和变量定义， 在自动生成的过程中并不会重新加载，除非重新运行。注意 Data Files 包括在自动生成范围内，可以在更改后自动重新加载。]]></summary></entry><entry><title type="html">CNN入门</title><link href="http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/CNN-day-1.html" rel="alternate" type="text/html" title="CNN入门" /><published>2022-02-06T11:26:41+00:00</published><updated>2022-02-06T11:26:41+00:00</updated><id>http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/CNN-day-1</id><content type="html" xml:base="http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/CNN-day-1.html"><![CDATA[<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ -->

<h2 id="激活函数relu">激活函数Relu</h2>

<p>$f(x)=\max{(0,x)}$</p>

<!-- ![CNN示意图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN.PNG){:.height=50%,width=50%}

常用架构描述：`INPUT -> [[CONV]*N -> POOL?]*M -> [FC]*K`{:info}，CNN由**卷积层、池化层、全连接**层组成

描述为`INPUT -> CONV -> POOL -> CONV -> POOL -> FC -> FC`{:.info}，一个卷积层（含池化层），为N=1个卷积层叠加,重复了M=2次,最后共K=2个全连接层 -->

<h2 id="卷积输出">卷积输出</h2>

<p>通过图像的矩阵乘以filter矩阵得到特征图</p>

<!-- ![卷积输出](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_1.PNG){:.height=50%,width=50%} -->

\[\begin{equation*}
  \begin{split}
    Out_{0,0}
    &amp; = f(\sum_{m=0}^{2}\sum_{n=0}^{2}W_{m,n}X_{m+0,n+1}+W_{b}) \\
    &amp; = relu(W_{0,0}X_{0,0}+W_{0,1}X_{0,1}+W_{0,2}X_{0,2}+W_{1,0}X_{1,0}+W_{1,1}X_{1,1}+W_{1,2}X_{1,2}+W_{2,0}X_{2,0}+W_{2,1}X_{2,1}+W_{2,2}X_{2,2}+W_{b}) \\
    &amp; = relu(1+0+1+0+1+0+0+0+1+0) \\
    &amp; = relu(4) \\
    &amp; = 4
  \end{split}
\end{equation*}\]

<p>M为第M行，N为第N列</p>

<h3 id="卷积输出的计算过程">卷积输出的计算过程</h3>
<!-- ![步幅为1时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_2.GIF)

步幅为2时的卷积输出，看完二图就明白

![步幅为2时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_3.PNG)

![步幅为2时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_4.PNG)

宽度跳2下，高度也跳二个步骤，共输出

 4 | 4 
--|:--:|--
 2 | 4  -->

<h3 id="特征图大小的关系公式">特征图大小的关系公式</h3>

\[\begin{equation*}特征图宽度=\frac{输入矩阵宽度-filter矩阵宽度+2*补几圈的0}{步幅}+1\end{equation*}\]

\[\begin{equation*}特征图高度=\frac{输入矩阵高度-filter矩阵高度+2*补几圈的0}{步幅}+1\end{equation*}\]

<p>参考上面步幅为2的图，输入矩阵宽度5，filter矩阵宽度3，补圈0为0，步幅为2</p>

\[\begin{equation*}
  \begin{split}
    特征图宽度
    &amp; = \frac{5-3+2*0}{2}+1 \\
    &amp; = 2 
  \end{split}
\end{equation*}\]

<h3 id="多输入多filter的卷积计算过程">多输入多Filter的卷积计算过程</h3>

<p>输入矩阵若多个时，深度为D个矩阵，F为filter矩阵的大小（高和宽相同），$w_{d,m,n}$表示filter矩阵的第d层第m行第n列filter，$Out_{d,i,j}$为第d层第i行第j列的元素</p>

<blockquote>
  <p>filter矩阵为超参数<sup id="fnref:foot1" role="doc-noteref"><a href="#fn:foot1" class="footnote" rel="footnote">1</a></sup>，有<strong>几个</strong>filter矩阵就有<strong>几个</strong>输出矩阵（数量相同）</p>
</blockquote>

<blockquote>
  <p>补了一圈0，便于图像边缘的特征提取</p>
</blockquote>

<p>7<em>7</em>3的输入，经2个3<em>3</em>3filter矩阵的卷积计算（步幅为2），得到3<em>3</em>2的输出</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_5.GIF" alt="多个输入矩阵的计算过程" /></p>

<blockquote>
  <p>上面卷积的计算方法，体现了<strong>局部连接</strong>和<strong>权值共享</strong>，2个3<em>3</em>3的filter卷积层参数仅有(3<em>3</em>3+1)*2=56个，相比全连接大大减少了参数数量</p>
</blockquote>

\[\begin{equation*}
  \begin{split}
    Out_{i,j}
    &amp; = f(\displaystyle\sum\limits_{d=0}^{D-1}\displaystyle\sum\limits_{m=0}^{F-1}\displaystyle\sum\limits_{n=0}^{F-1}W_{d,m,n}X_{d,i+m,j+n}+W_{b}) \\
  \end{split}
\end{equation*}\]

<h2 id="pooling层的计算">Pooling层的计算</h2>

<h3 id="max-pooling">Max Pooling</h3>

<p>n*n的样本中取最大值</p>

<p><img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/Max_Pooling.png" alt="Max_Pooling" /></p>

<p>n*n的样本中取均值
类似上图，取4个元素的均值</p>

<h2 id="卷积层的训练">卷积层的训练</h2>

<p>训练算法原理：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。</p>

<ul>
  <li>前向计算每个神经元的输出值$a_{j}$(j表示网络的第j个神经元)</li>
  <li>反向计算每个神经元的误差项，是网络的损失函数对神经元<strong>加权</strong>输入\(net_{j}\)的偏导数，即\(\delta_{j}=\frac{\vartheta E_{d}}{\vartheta net_{j}}\)</li>
  <li>计算每个神经元的连接权重$W_{ji}$(神经元i到神经元j的权重)的<strong>梯度</strong>，公式为\(\frac{\vartheta E_{d}}{\vartheta W_{ji}}=a_{i}\delta_{j}\)</li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:foot1" role="doc-endnote">
      <p>人为设定的参数 <a href="#fnref:foot1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="sEMG识别" /><category term="深度学习" /><category term="CNN" /><summary type="html"><![CDATA[激活函数Relu $f(x)=\max{(0,x)}$ 卷积输出 通过图像的矩阵乘以filter矩阵得到特征图 \[\begin{equation*} \begin{split} Out_{0,0} &amp; = f(\sum_{m=0}^{2}\sum_{n=0}^{2}W_{m,n}X_{m+0,n+1}+W_{b}) \\ &amp; = relu(W_{0,0}X_{0,0}+W_{0,1}X_{0,1}+W_{0,2}X_{0,2}+W_{1,0}X_{1,0}+W_{1,1}X_{1,1}+W_{1,2}X_{1,2}+W_{2,0}X_{2,0}+W_{2,1}X_{2,1}+W_{2,2}X_{2,2}+W_{b}) \\ &amp; = relu(1+0+1+0+1+0+0+0+1+0) \\ &amp; = relu(4) \\ &amp; = 4 \end{split} \end{equation*}\] M为第M行，N为第N列 卷积输出的计算过程 特征图大小的关系公式 \[\begin{equation*}特征图宽度=\frac{输入矩阵宽度-filter矩阵宽度+2*补几圈的0}{步幅}+1\end{equation*}\] \[\begin{equation*}特征图高度=\frac{输入矩阵高度-filter矩阵高度+2*补几圈的0}{步幅}+1\end{equation*}\] 参考上面步幅为2的图，输入矩阵宽度5，filter矩阵宽度3，补圈0为0，步幅为2 \[\begin{equation*} \begin{split} 特征图宽度 &amp; = \frac{5-3+2*0}{2}+1 \\ &amp; = 2 \end{split} \end{equation*}\] 多输入多Filter的卷积计算过程 输入矩阵若多个时，深度为D个矩阵，F为filter矩阵的大小（高和宽相同），$w_{d,m,n}$表示filter矩阵的第d层第m行第n列filter，$Out_{d,i,j}$为第d层第i行第j列的元素 filter矩阵为超参数1，有几个filter矩阵就有几个输出矩阵（数量相同） 补了一圈0，便于图像边缘的特征提取 773的输入，经2个333filter矩阵的卷积计算（步幅为2），得到332的输出 上面卷积的计算方法，体现了局部连接和权值共享，2个333的filter卷积层参数仅有(333+1)*2=56个，相比全连接大大减少了参数数量 \[\begin{equation*} \begin{split} Out_{i,j} &amp; = f(\displaystyle\sum\limits_{d=0}^{D-1}\displaystyle\sum\limits_{m=0}^{F-1}\displaystyle\sum\limits_{n=0}^{F-1}W_{d,m,n}X_{d,i+m,j+n}+W_{b}) \\ \end{split} \end{equation*}\] Pooling层的计算 Max Pooling n*n的样本中取最大值 n*n的样本中取均值 类似上图，取4个元素的均值 卷积层的训练 训练算法原理：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。 前向计算每个神经元的输出值$a_{j}$(j表示网络的第j个神经元) 反向计算每个神经元的误差项，是网络的损失函数对神经元加权输入\(net_{j}\)的偏导数，即\(\delta_{j}=\frac{\vartheta E_{d}}{\vartheta net_{j}}\) 计算每个神经元的连接权重$W_{ji}$(神经元i到神经元j的权重)的梯度，公式为\(\frac{\vartheta E_{d}}{\vartheta W_{ji}}=a_{i}\delta_{j}\) 人为设定的参数 &#8617;]]></summary></entry><entry><title type="html">基于sEMG信号的手势识别系统</title><link href="http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/sEMG-proposal-report.html" rel="alternate" type="text/html" title="基于sEMG信号的手势识别系统" /><published>2022-02-06T11:26:41+00:00</published><updated>2022-02-06T11:26:41+00:00</updated><id>http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/sEMG-proposal-report</id><content type="html" xml:base="http://localhost:4000/Study-Blog/semg%E8%AF%86%E5%88%AB/2022/02/06/sEMG-proposal-report.html"><![CDATA[<p>（报告内容包括课题的意义、国内外发展状况、本课题的研究内容、研究方法、研究手段、研究步骤以及参考文献资料等。）</p>
<h2 id="课题的意义">课题的意义：</h2>
<p><code class="language-plaintext highlighter-rouge">sEMG</code>(Surface Electromyography)是指浅层肌肉纤维收缩时形成的综合电位差信号，肌电本质是大脑的中枢神经系统通过MUs(motor units)逐次兴奋和单位时间内的兴奋次数等控制肌肉收缩力的大小和变化速度。肌电信号是一维时间序列信号，来源于中枢神经脊髓中的运动神经元，是电极接触的众多运动单元发射与释放的APs（action potentials）总和。所以肌电信号包含肌肉收缩模式及强度的信息。
  因此sEMG客观反映了人体肌肉活动的情况，手臂驱使人手作出不同手势动作的肌肉层的电势变化，各种动作的不同肌电变化是本课题研究目的。手势语言作为语言障碍者的交际工具、AR交互输入方式、遥控机器人、义肢等，应用前景相当广泛，本选题目的是建立sEMG识别模型推测sEMG活动段所对应的含义。课题意义是研究人机交互的一种方式，sEMG客观反映了人体肌肉活动的情况，采用非入侵式肌电检测方法，检测皮层下的肌电活动信号，计算机从肌电信号提取某些特殊特征将其经拟合函数输出，输出为计算机理解对应的含义，以实现计算机解读手势动作，达成较好的人机交互效果。</p>

<h2 id="国内外发展状况">国内外发展状况：</h2>
<p>国内外90年代肌电研究历程，大多数研究采用三种方案：</p>
<ol>
  <li><strong>基于阈值识别方案</strong>
  sEMG信号经校正、滤波和调制后，肌肉每次收缩产生信号峰值，通过比较峰值与阈值，表示肌电状态处于握拳或伸展状态；</li>
  <li>
    <p><strong>基于幅值编码方案</strong>
  将将肌电信号的三态模式按照时序进行编码输出。具体的肌电控制语言由三位编码组成，（a1(t),a2(t),a3(t)） ai(t)属于{0，1，2}  每一位可以使用肌电信号幅值的三个态“1”“2”“0”表示（通过肌电收缩幅度的大小实现“1”和“2”，“0”代表无肌肉活动）它们组合起来对应输出各种模式。
  每次肌肉收缩的检测依靠对肌电信号的非高斯性评估（倾斜度、峭度），并随即进行特定时间窗口内的滤波（去除干扰噪声），以确定信号幅度峰值强度是“1”或“2”。编码语言的全部三位采集完毕后对应模式即作出规定输出（映射表，不同肌电活动段分别映射不同模式）；</p>
  </li>
  <li><strong>分层控制决策</strong>，由先进的信号调制及模式识别算法识别出手臂肌肉进行的输出模式。
  目前，现在已发展新的一种模式：基于<code class="language-plaintext highlighter-rouge">模式（NN）识别</code>的算法
  国外已有产品案例，Thalmic Labs公司的产品MYO手环2016年正式上市，MYO手环的8个非入侵式干电极用于捕获用户胳臂运动时肌肉产生的生物电活动段，加上九轴IMU（三轴加速计、三轴陀螺仪、三轴磁强计）判断胳膊的运动姿势，通过模式识别从而实现判别佩戴者的动作，亦可利用蓝牙远程控制设备。
  使用肌电信号的义肢，论文Shared human–robot proportional control of a dexterous myoelectric prosthesis提出的概念：通过截肢者残肢的肌肉活动来解码手指运动的意图，以达到控制单个手指的目的，这种神经工程学与机器人工程学交叉的概念已在截肢者获得试验成功，发表于《Nature Machine Intelligence》。其核心关键是NN算法学习如何解码用户的肌肉活动信号的意图，并转换为操控义肢控制装置的指令，来完成手指的动作。</li>
</ol>

<h2 id="本课题的研究内容">本课题的研究内容：</h2>
<p>肌电识别的定义：提取肌电信号有<code class="language-plaintext highlighter-rouge">价值</code>的信息，并解码生物电信号表达的手指<code class="language-plaintext highlighter-rouge">运动意图</code>。表层肌电信号是电极接触的皮肤表层皮下的经过皮脂、溶液传导的动作电位总和，由于表层肌肉活动信号存在三种特点：</p>

<ul>
  <li>肌电信号微弱；</li>
  <li>随机性强；</li>
  <li>信噪比低</li>
</ul>

<p>从模式识别的角度看，肌电信号的特征在特征空间允许呈现无穷维，所以我们用机器学习算法对有限的模式进行训练，一是NN算法，二是样本量，只要两者相当合适，可以保证肌电信号有用信息的提取与识别。
 本课题运用NN模式识别将某些样本量的特征提取出来（肌肉收缩的信号幅值、统计学特性的变化等），经一定训练组成合适的状态空间，等待下一次肌电活动段识别的时候去判断信号与状态空间的哪个状态最为接近（识别与回归）。</p>

<h3 id="主要实现功能如下">主要实现功能如下：</h3>
<ol>
  <li>模拟信号采集：根据香农采样定理，采样频率应不小于1000HZ；</li>
  <li>信号预处理：整流、过滤10～500hz有效范围的信号；</li>
  <li>无线传输信号：每个数据包带有0.1s内的数字信息，数据包抵达处理终端时自动打上时间戳；</li>
  <li>数字过滤：使用过滤算法去掉无用噪音，提取有价值的信息</li>
  <li>特征提取：系统分割活动段后，用如平均绝对值、简单平方积分、均方根值等；样本均值、方差、积分等；频率的平均频率、平均功率等特征，特征提取由NN模型决定；</li>
  <li>RNN、CNN、SVM向量机等神经网络模型，根据5.创建的数据集测量它们的识别精度，决定手势识别系统采用哪个模型；</li>
  <li>NN模型推测某段肌电活动段可能的最大概率的字符，从而实现NN模式识别
 
<img src="https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-08-sEMG-proposal-report/设计流程与实现.png" alt="图1 设计流程与实现" /></li>
</ol>

<h3 id="研究方法和手段">研究方法和手段：</h3>
<ul>
  <li>通过Arduino开发平台完成sEMG采集装置的设计与实现。</li>
  <li>借用NN开发工具实现分类算法设计、样本采集工具以及训练。</li>
  <li>手势映射表的设计与实现。</li>
</ul>

<h3 id="研究步骤">研究步骤：</h3>
<ul>
  <li>通过图书馆、互联网，充分查阅相关的文献资料，规划设计的需求。</li>
  <li>明确毕设方案，进行课题方案和进度设计，且能不断地研究和完善。</li>
  <li>系统的界面设计，要求简洁大方，资源库要保证准确性。</li>
  <li>认真测试和维护系统，保证系统的正常运行。</li>
  <li>毕业设计说明书的内容要对所开发系统的过程进行充分的论述说明，其格式要符合天津理工大学本科毕业设计说明书规范格式的要求。按时保质保量完成毕业设计。</li>
</ul>

<h2 id="参考文献">参考文献：</h2>
<p>[1] Andrés G. Jaramillo; Marco E. Benalcázar . Real-time hand gesture recognition with EMG using machine learning . 2017 IEEE Second Ecuador Technical Chapters
[2] 待添加</p>]]></content><author><name>李广琛</name><email>allrobots@163.com</email></author><category term="sEMG识别" /><category term="深度学习" /><category term="机器学习" /><category term="生理信号" /><category term="sEMG识别" /><summary type="html"><![CDATA[（报告内容包括课题的意义、国内外发展状况、本课题的研究内容、研究方法、研究手段、研究步骤以及参考文献资料等。） 课题的意义： sEMG(Surface Electromyography)是指浅层肌肉纤维收缩时形成的综合电位差信号，肌电本质是大脑的中枢神经系统通过MUs(motor units)逐次兴奋和单位时间内的兴奋次数等控制肌肉收缩力的大小和变化速度。肌电信号是一维时间序列信号，来源于中枢神经脊髓中的运动神经元，是电极接触的众多运动单元发射与释放的APs（action potentials）总和。所以肌电信号包含肌肉收缩模式及强度的信息。 因此sEMG客观反映了人体肌肉活动的情况，手臂驱使人手作出不同手势动作的肌肉层的电势变化，各种动作的不同肌电变化是本课题研究目的。手势语言作为语言障碍者的交际工具、AR交互输入方式、遥控机器人、义肢等，应用前景相当广泛，本选题目的是建立sEMG识别模型推测sEMG活动段所对应的含义。课题意义是研究人机交互的一种方式，sEMG客观反映了人体肌肉活动的情况，采用非入侵式肌电检测方法，检测皮层下的肌电活动信号，计算机从肌电信号提取某些特殊特征将其经拟合函数输出，输出为计算机理解对应的含义，以实现计算机解读手势动作，达成较好的人机交互效果。 国内外发展状况： 国内外90年代肌电研究历程，大多数研究采用三种方案： 基于阈值识别方案 sEMG信号经校正、滤波和调制后，肌肉每次收缩产生信号峰值，通过比较峰值与阈值，表示肌电状态处于握拳或伸展状态； 基于幅值编码方案 将将肌电信号的三态模式按照时序进行编码输出。具体的肌电控制语言由三位编码组成，（a1(t),a2(t),a3(t)） ai(t)属于{0，1，2}  每一位可以使用肌电信号幅值的三个态“1”“2”“0”表示（通过肌电收缩幅度的大小实现“1”和“2”，“0”代表无肌肉活动）它们组合起来对应输出各种模式。 每次肌肉收缩的检测依靠对肌电信号的非高斯性评估（倾斜度、峭度），并随即进行特定时间窗口内的滤波（去除干扰噪声），以确定信号幅度峰值强度是“1”或“2”。编码语言的全部三位采集完毕后对应模式即作出规定输出（映射表，不同肌电活动段分别映射不同模式）； 分层控制决策，由先进的信号调制及模式识别算法识别出手臂肌肉进行的输出模式。 目前，现在已发展新的一种模式：基于模式（NN）识别的算法 国外已有产品案例，Thalmic Labs公司的产品MYO手环2016年正式上市，MYO手环的8个非入侵式干电极用于捕获用户胳臂运动时肌肉产生的生物电活动段，加上九轴IMU（三轴加速计、三轴陀螺仪、三轴磁强计）判断胳膊的运动姿势，通过模式识别从而实现判别佩戴者的动作，亦可利用蓝牙远程控制设备。 使用肌电信号的义肢，论文Shared human–robot proportional control of a dexterous myoelectric prosthesis提出的概念：通过截肢者残肢的肌肉活动来解码手指运动的意图，以达到控制单个手指的目的，这种神经工程学与机器人工程学交叉的概念已在截肢者获得试验成功，发表于《Nature Machine Intelligence》。其核心关键是NN算法学习如何解码用户的肌肉活动信号的意图，并转换为操控义肢控制装置的指令，来完成手指的动作。 本课题的研究内容： 肌电识别的定义：提取肌电信号有价值的信息，并解码生物电信号表达的手指运动意图。表层肌电信号是电极接触的皮肤表层皮下的经过皮脂、溶液传导的动作电位总和，由于表层肌肉活动信号存在三种特点： 肌电信号微弱； 随机性强； 信噪比低 从模式识别的角度看，肌电信号的特征在特征空间允许呈现无穷维，所以我们用机器学习算法对有限的模式进行训练，一是NN算法，二是样本量，只要两者相当合适，可以保证肌电信号有用信息的提取与识别。 本课题运用NN模式识别将某些样本量的特征提取出来（肌肉收缩的信号幅值、统计学特性的变化等），经一定训练组成合适的状态空间，等待下一次肌电活动段识别的时候去判断信号与状态空间的哪个状态最为接近（识别与回归）。 主要实现功能如下： 模拟信号采集：根据香农采样定理，采样频率应不小于1000HZ； 信号预处理：整流、过滤10～500hz有效范围的信号； 无线传输信号：每个数据包带有0.1s内的数字信息，数据包抵达处理终端时自动打上时间戳； 数字过滤：使用过滤算法去掉无用噪音，提取有价值的信息 特征提取：系统分割活动段后，用如平均绝对值、简单平方积分、均方根值等；样本均值、方差、积分等；频率的平均频率、平均功率等特征，特征提取由NN模型决定； RNN、CNN、SVM向量机等神经网络模型，根据5.创建的数据集测量它们的识别精度，决定手势识别系统采用哪个模型； NN模型推测某段肌电活动段可能的最大概率的字符，从而实现NN模式识别   研究方法和手段： 通过Arduino开发平台完成sEMG采集装置的设计与实现。 借用NN开发工具实现分类算法设计、样本采集工具以及训练。 手势映射表的设计与实现。 研究步骤： 通过图书馆、互联网，充分查阅相关的文献资料，规划设计的需求。 明确毕设方案，进行课题方案和进度设计，且能不断地研究和完善。 系统的界面设计，要求简洁大方，资源库要保证准确性。 认真测试和维护系统，保证系统的正常运行。 毕业设计说明书的内容要对所开发系统的过程进行充分的论述说明，其格式要符合天津理工大学本科毕业设计说明书规范格式的要求。按时保质保量完成毕业设计。 参考文献： [1] Andrés G. Jaramillo; Marco E. Benalcázar . Real-time hand gesture recognition with EMG using machine learning . 2017 IEEE Second Ecuador Technical Chapters [2] 待添加]]></summary></entry></feed>