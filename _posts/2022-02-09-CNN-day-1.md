---
layout: article

title:  "CNN入门"

date:   2022-02-06 19:26:41 +0800

categories: sEMG识别

tags: [深度学习,CNN]
---
<!-- https://github.com/allrobot/Study-Blog/raw/main/assets/images/ -->

## 激活函数Relu

$f(x)=\max{\(0,x\)}$

<!-- ![CNN示意图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN.PNG){:.height=50%,width=50%}

常用架构描述：`INPUT -> [[CONV]*N -> POOL?]*M -> [FC]*K`{:info}，CNN由**卷积层、池化层、全连接**层组成

描述为`INPUT -> CONV -> POOL -> CONV -> POOL -> FC -> FC`{:.info}，一个卷积层（含池化层），为N=1个卷积层叠加,重复了M=2次,最后共K=2个全连接层 -->

## 卷积输出

通过图像的矩阵乘以filter矩阵得到特征图

<!-- ![卷积输出](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_1.PNG){:.height=50%,width=50%} -->

$$\begin{equation*}
  \begin{split}
    Out_{0,0}
    & = f(\sum_{m=0}^{2}\sum_{n=0}^{2}W_{m,n}X_{m+0,n+1}+W_{b}) \\
    & = relu(W_{0,0}X_{0,0}+W_{0,1}X_{0,1}+W_{0,2}X_{0,2}+W_{1,0}X_{1,0}+W_{1,1}X_{1,1}+W_{1,2}X_{1,2}+W_{2,0}X_{2,0}+W_{2,1}X_{2,1}+W_{2,2}X_{2,2}+W_{b}) \\
    & = relu(1+0+1+0+1+0+0+0+1+0) \\
    & = relu(4) \\
    & = 4
  \end{split}
\end{equation*}$$

M为第M行，N为第N列

### 卷积输出的计算过程
<!-- ![步幅为1时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_2.GIF)

步幅为2时的卷积输出，看完二图就明白

![步幅为2时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_3.PNG)

![步幅为2时的卷积输出特征图](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_4.PNG)

宽度跳2下，高度也跳二个步骤，共输出

 4 | 4 
--|:--:|--
 2 | 4  -->

### 特征图大小的关系公式

$$\begin{equation*}特征图宽度=\frac{输入矩阵宽度-filter矩阵宽度+2*补几圈的0}{步幅}+1\end{equation*}$$

$$\begin{equation*}特征图高度=\frac{输入矩阵高度-filter矩阵高度+2*补几圈的0}{步幅}+1\end{equation*}$$

参考上面步幅为2的图，输入矩阵宽度5，filter矩阵宽度3，补圈0为0，步幅为2

$$\begin{equation*}
  \begin{split}
    特征图宽度
    & = \frac{5-3+2*0}{2}+1 \\
    & = 2 
  \end{split}
\end{equation*}$$

### 多输入多Filter的卷积计算过程

输入矩阵若多个时，深度为D个矩阵，F为filter矩阵的大小（高和宽相同），$w_{d,m,n}$表示filter矩阵的第d层第m行第n列filter，$Out_{d,i,j}$为第d层第i行第j列的元素

>filter矩阵为超参数[^foot1]，有**几个**filter矩阵就有**几个**输出矩阵（数量相同）

>补了一圈0，便于图像边缘的特征提取

7*7*3的输入，经2个3*3*3filter矩阵的卷积计算（步幅为2），得到3*3*2的输出

![多个输入矩阵的计算过程](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/CNN_5.GIF)

>上面卷积的计算方法，体现了**局部连接**和**权值共享**，2个3*3*3的filter卷积层参数仅有(3*3*3+1)*2=56个，相比全连接大大减少了参数数量

$$\begin{equation*}
  \begin{split}
    Out_{i,j}
    & = f(\displaystyle\sum\limits_{d=0}^{D-1}\displaystyle\sum\limits_{m=0}^{F-1}\displaystyle\sum\limits_{n=0}^{F-1}W_{d,m,n}X_{d,i+m,j+n}+W_{b}) \\
  \end{split}
\end{equation*}$$


## Pooling层的计算

### Max Pooling

n*n的样本中取最大值

![Max_Pooling](https://github.com/allrobot/Study-Blog/raw/main/assets/images/2022-02-09-CNN/Max_Pooling.png)

n*n的样本中取均值
类似上图，取4个元素的均值

## 卷积层的训练

训练算法原理：利用链式求导计算损失函数对每个权重的偏导数（梯度），然后根据梯度下降公式更新权重。

- 前向计算每个神经元的输出值$a_{j}$(j表示网络的第j个神经元)
- 反向计算每个神经元的误差项，是网络的损失函数对神经元**加权**输入$$net_{j}$$的偏导数，即$$\delta_{j}=\frac{\vartheta E_{d}}{\vartheta net_{j}}$$
- 计算每个神经元的连接权重$W_{ji}$(神经元i到神经元j的权重)的**梯度**，公式为$$\frac{\vartheta E_{d}}{\vartheta W_{ji}}=a_{i}\delta_{j}$$


[^foot1]: 人为设定的参数